@Article"Shafique2015,
author="Shafique, Muhammad
and Labiche, Yvan",
title="A systematic review of state-based test tools",
journal="International Journal on Software Tools for Technology Transfer",
year="2015",
volume="17",
number="1",
pages="59--76",
abstract="Model-based testing (MBT) is about testing a software system using a model of its behaviour. To benefit fully from MBT, automation support is required. The goal of this systematic review is determining the current state of the art of prominent MBT tool support where we focus on tools that rely on state-based models. We automatically searched different source of information including digital libraries and mailing lists dedicated to the topic. Precisely defined criteria are used to compare selected tools and comprise support for test adequacy and coverage criteria, level of automation for various testing activities and support for the construction of test scaffolding. Simple adequacy criteria are supported but not advanced ones; data(-flow) criteria are seldom supported; support for creating test scaffolding varies a great deal. The results of this review should be of interest to a wide range of stakeholders: software companies interested in selecting the most appropriate MBT tool for their needs; organizations willing to invest into creating MBT tool support; researchers interested in setting research directions.",
issn="1433-2787",
doi="10.1007/s10009-013-0291-0",
url="http://dx.doi.org/10.1007/s10009-013-0291-0"
"


@Article"Santos2015,
author="Santos, Ismayle S.
and Andrade, Rossana MC
and Santos Neto, Pedro A.",
title="Templates for textual use cases of software product lines: results from a systematic mapping study and a controlled experiment",
journal="Journal of Software Engineering Research and Development",
year="2015",
volume="3",
number="1",
pages="5",
abstract="Use case templates can be used to describe functional requirements of a Software Product Line. However, to the best of our knowledge, no efforts have been made to collect and summarize these existing templates and no empirical evaluation of the use cases' comprehensibility provided by these templates has been addressed yet. The contributions of this paper are twofold. First, we present a systematic mapping study about the SPL variability description using textual use cases. From this mapping, we found twelve SPL use case templates and observed the need not only for the application of these templates in real SPL but also for supporting tools. Secondly, this work presents an evaluation of the comprehensibility of SPL use cases specified in these templates through a controlled experiment with 48 volunteers. The results of this experiment show that the specification of variabilities in the steps' numeric identifiers of the textual use cases is better to the use case understanding than the other approaches identified. We also found evidence that the specification of variabilities at the end of the use cases favors the comprehension of them and the use of questions associated to the variation points in the use cases improves the understanding of use cases. We conclude that each characteristic of the existing templates has an impact on the SPL use case understanding and this should be taken into account when choosing one.",
issn="2195-1721",
doi="10.1186/s40411-015-0020-3",
url="http://dx.doi.org/10.1186/s40411-015-0020-3"
"


@Article"Ouhbi2015,
author="Ouhbi, Sofia
and Idri, Ali
and Fern"\'a"ndez-Alem"\'a"n, Jos"\'e" Luis
and Toval, Ambrosio",
title="Requirements engineering education: a systematic mapping study",
journal="Requirements Engineering",
year="2015",
volume="20",
number="2",
pages="119--138",
abstract="Requirements engineering (RE) has attracted a great deal of attention from researchers and practitioners in recent years. Requirements engineering education (REE) is therefore an important undertaking if the field is to have professionals who are capable of successfully accomplishing software projects. This increasing interest demands that academia should provide software engineering students with a solid foundation in the subject matter. This paper aims to identify and to present the current research on REE that is available at present, and to select useful approaches and needs for future research. A systematic mapping study was therefore performed to classify the selected studies into five classification criteria: research type, empirical type, contribution type, RE activity, and curricula. A total of 79 papers were selected and classified according to these criteria. The results of this systematic mapping study are discussed, and a list of advice obtained from the REE literature for instructors is provided.",
issn="1432-010X",
doi="10.1007/s00766-013-0192-5",
url="http://dx.doi.org/10.1007/s00766-013-0192-5"
"


@Article"van�den�Berghe2015,
author="van�den�Berghe, Alexander
and Scandariato, Riccardo
and Yskout, Koen
and Joosen, Wouter",
title="Design notations for secure software: a systematic literature review",
journal="Software "\&" Systems Modeling",
year="2015",
pages="1--23",
abstract="In the past 10�years, the research community has produced a significant number of design notations to represent security properties and concepts in a design artifact. These notations are aimed at documenting and analyzing security in a software design model. The fragmentation of the research space, however, has resulted in a complex tangle of different techniques. Hence, practitioners are confronted with the challenging task of scouting the right approach from a multitude of proposals. Similarly, it is hard for researchers to keep track of the synergies among the existing notations, in order to identify the existing opportunities for original contributions. This paper presents a systematic literature review that inventorizes the existing notations and provides an in-depth, comparative analysis for each.",
issn="1619-1374",
doi="10.1007/s10270-015-0486-9",
url="http://dx.doi.org/10.1007/s10270-015-0486-9"
"


@Article"Yue2011,
author="Yue, Tao
and Briand, Lionel C.
and Labiche, Yvan",
title="A systematic review of transformation approaches between user requirements and analysis models",
journal="Requirements Engineering",
year="2011",
volume="16",
number="2",
pages="75--99",
abstract="Model transformation is one of the basic principles of Model Driven Architecture. To build a software system, a sequence of transformations is performed, starting from requirements and ending with implementation. However, requirements are mostly in the form of text, but not a model that can be easily understood by computers; therefore, automated transformations from requirements to analysis models are not easy to achieve. The overall objective of this systematic review is to examine existing literature works that transform textual requirements into analysis models, highlight open issues, and provide suggestions on potential directions of future research. The systematic review led to the analysis of 20 primary studies (16 approaches) obtained after a carefully designed procedure for selecting papers published in journals and conferences from 1996 to 2008 and Software Engineering textbooks. A conceptual framework is designed to provide common concepts and terminology and to define a unified transformation process. This facilitates the comparison and evaluation of the reviewed papers.",
issn="1432-010X",
doi="10.1007/s00766-010-0111-y",
url="http://dx.doi.org/10.1007/s00766-010-0111-y"
"


@Article"Ivarsson2009,
author="Ivarsson, Martin
and Gorschek, Tony",
title="Technology transfer decision support in requirements engineering research: a systematic review of REj",
journal="Requirements Engineering",
year="2009",
volume="14",
number="3",
pages="155--175",
abstract="One of the main goals of an applied research field such as requirements engineering is the transfer of research results to industrial use. To promote industrial adoption of technologies developed in academia, researchers need to provide tangible evidence of the advantages of using them. This can be done through industry validation, enabling researchers to test and validate technologies in a real setting with real users and applications. The evidence obtained, together with detailed information on how the validation was conducted, offers rich decision support material for industrial practitioners seeking to adopt new technologies. This paper presents a comprehensive systematic literature review of all papers published in the Requirements Engineering journal containing any type of technology evaluation. The aim is to gauge the support for technology transfer, i.e., to what degree industrial practitioners can use the reporting of technology evaluations in the journal as decision support for adopting the technologies in industrial practice. Findings show that very few evaluations offer full technology transfer support, i.e., have a realistic scale, application or subjects. The major improvement potential concerning support for technology transfer is found to be the subjects used in the evaluations. Attaining company support, including support for using practitioners as subjects, is vital for technology transfer and for researchers seeking to validate technologies.",
issn="1432-010X",
doi="10.1007/s00766-009-0080-1",
url="http://dx.doi.org/10.1007/s00766-009-0080-1"
"


@Article"Erdogan2014,
author="Erdogan, Gencer
and Li, Yan
and Runde, Ragnhild Kobro
and Seehusen, Fredrik
and St"\o"len, Ketil",
title="Approaches for the combined use of risk analysis and testing: a systematic literature review",
journal="International Journal on Software Tools for Technology Transfer",
year="2014",
volume="16",
number="5",
pages="627--642",
abstract="Risk analysis and testing are conducted for different purposes. Risk analysis and testing nevertheless involve processes that may be combined to the benefit of both. We may use testing to support risk analysis and risk analysis to support testing. This paper surveys literature on the combined use of risk analysis and testing. First, the existing approaches are identified through a systematic literature review. The identified approaches are then classified and discussed with respect to main goal, context of use and maturity level. The survey highlights the need for more structure and rigor in the definition and presentation of approaches. Evaluations are missing in most cases. The paper may serve as a basis for examining approaches for the combined use of risk analysis and testing, or as a resource for identifying the adequate approach to use.",
issn="1433-2787",
doi="10.1007/s10009-014-0330-5",
url="http://dx.doi.org/10.1007/s10009-014-0330-5"
"


@Article"Svahnberg2015,
author="Svahnberg, Mikael
and Gorschek, Tony
and Nguyen, Thi Than Loan
and Nguyen, Mai",
title="Uni-REPM: a framework for requirements engineering process assessment",
journal="Requirements Engineering",
year="2015",
volume="20",
number="1",
pages="91--118",
abstract="It has been shown that potential business benefits could be achieved by assessing and improving the requirements engineering (RE) process. However, process assessment models such as CMMI and ISO9000 only cover RE shallowly. Tailored models such as REGPG and REPM, on the other hand, do not cover market-driven requirements engineering. Other attempts such as MDREPM covers market-driven requirements engineering, but correspondingly neglects bespoke requirements engineering. Moreover, the area itself has evolved so practices that once were cutting edge are now commonplace. In this article, we develop and evaluate a unified requirements engineering process maturity model (Uni-REPM) that can be used in a market-driven as well as a bespoke context. This model is based on REPM, but has evolved to reflect contemporary requirements engineering practices. Uni-REPM is primarily created based on a systematic literature review of market-driven requirements engineering practices and a literature review of bespoke practices. Based on the results, Uni-REPM is formulated. The objective of Uni-REPM is twofold. Firstly, it is expected to be applicable for assessing the maturity of RE processes in various scenarios where an organisation would use different development approaches. Secondly, it instructs practitioners about which RE practices to perform and their expected benefits. As an assessment instrument, Uni-REPM provides a simple and low-cost solution for practitioners to identify the status of their RE process. As a guidance tool, Uni-REPM lessens the gap between theoretical and practical worlds by transferring the available RE technologies from research to industry practice.",
issn="1432-010X",
doi="10.1007/s00766-013-0188-1",
url="http://dx.doi.org/10.1007/s00766-013-0188-1"
"


@Article"Diirr2014,
author="Diirr, Tha"\'i"ssa
and Santos, Gleison",
title="Improvement of IT service processes: a study of critical success factors",
journal="Journal of Software Engineering Research and Development",
year="2014",
volume="2",
number="1",
pages="4",
abstract="Maturity models and Information Technology (IT) service management models guide the definition and improvement of service management processes. Known approaches include ITIL, COBIT, ISO/IEC 20000, CMMI-SVC, and MR-MPS-SV. The implementation of these models results in benefits such as: increased user and customer satisfaction with IT services; financial savings due to less rework and less time used, and improved resource management and usage; improved decision making and optimized risk; and better alignment based on the business focus. However, some organizations find it difficult to use the models. This paper presents a study in which we identified critical factors for success and failure of the improvement of IT service processes. By doing a systematic mapping study and by snowballing, we were able to identify factors such as: project implementation strategy; support, commitment, and involvement; processes; and internal and external resources. Also, we analyzed our results using grounded theory procedures in order to facilitate their understanding.",
issn="2195-1721",
doi="10.1186/2195-1721-2-4",
url="http://dx.doi.org/10.1186/2195-1721-2-4"
"


@Article"Eichelberger2015,
author="Eichelberger, Holger
and Schmid, Klaus",
title="Mapping the design-space of textual variability modeling languages: a refined analysis",
journal="International Journal on Software Tools for Technology Transfer",
year="2015",
volume="17",
number="5",
pages="559--584",
abstract="Variability modeling is a major part of modern product line engineering. Graphical or table-based approaches to variability modeling are focused around abstract models and specialized tools to interact with these models. However, more recently textual variability modeling languages, comparable to some extent to programming languages, were introduced. We consider the recent trend in product line engineering towards textual variability modeling languages as a phenomenon, which deserves deeper analysis. In this article, we report on the results and approach of a literature survey combined with an expert study. In the literature survey, we identified 11 languages, which enable the textual specification of product line variability and which are sufficiently described for an in-depth analysis. We provide a classification scheme, useful to describe the range of capabilities of such languages. Initially, we identified the relevant capabilities of these languages from a literature survey. The result of this has been refined, validated and partially improved by the expert survey. A second recent phenomenon in product line variability modeling is the increasing scale of variability models. Some authors of textual variability modeling languages argue that these languages are more appropriate for large-scale models. As a consequence, we would expect specific capabilities addressing scalability in the languages. Thus, we compare the capabilities of textual variability modeling techniques, if compared to graphical variability modeling approaches and in particular to analyze their specialized capabilities for large-scale models.",
issn="1433-2787",
doi="10.1007/s10009-014-0362-x",
url="http://dx.doi.org/10.1007/s10009-014-0362-x"
"


@Article"Assun��o2014,
author="Assun"\c"c"""\~a"o, Wesley KG
and Barros, M"\'a"rcio de O.
and Colanzi, Thelma E.
and Dias-Neto, Arilo C.
and Paix"\~a"o, Matheus HE
and de Souza, Jerffeson T.
and Vergilio, Silvia R.",
title="A mapping study of the Brazilian SBSE community",
journal="Journal of Software Engineering Research and Development",
year="2014",
volume="2",
number="1",
pages="3",
abstract="Research communities evolve over time, changing their interests for specific problems or research areas. Mapping the evolution of a research community, including the most frequently addressed problems, the strategies selected to propose solution for them, the venues on which results observed from applying these strategies are published, and the collaboration among distinct groups may provide lessons on actions that can positively influence the growth of research in a given field. To this end, this paper presents an analysis of the Brazilian SBSE research community. We present our major research groups focusing on the field, the software engineering problems most addressed by them, the search techniques most frequently used to solve these problems, and an analysis of our publications and collaboration. We could conclude that the Brazilian community is still expanding, both geographically and in terms of publications, and that the creation of a national workshop focusing on the research field was a keystone to allow this growth.",
issn="2195-1721",
doi="10.1186/2195-1721-2-3",
url="http://dx.doi.org/10.1186/2195-1721-2-3"
"


@Article"Li2014,
author="Li, Zhi
and Hall, Jon G.
and Rapanotti, Lucia",
title="On the systematic transformation of requirements to specifications",
journal="Requirements Engineering",
year="2014",
volume="19",
number="4",
pages="397--419",
abstract="Formal approaches to development are widely acknowledged to have difficulty in the validation of real-world requirements; in contrast, non-formal approaches find it difficult to identify the formal structures in requirements that are useful in a solution. That the problems that computing treats are embedded in the real world with solutions being an essentially formal machine means this dichotomy will always exist, with some new approach to the development needed which can draw a boundary between what is formalised and what can be left informal. In this paper, we show how the natural cause-and-effect structures that can be found in non-formal requirements descriptions can be used systematically to arrive at a software specification. The theoretical contribution of the work is the formalisation of Jackson's idea of problem progression in his Problem Frames framework through the use of a graph grammar to capture problem models as graphs and their manipulation as transformations. The approach is illustrated through a substantial benchmark example---Swartout's and Balzer's package router. We also report on the results of an initial empirical evaluation of the approach based on a prototype problem progression tool we have constructed.",
issn="1432-010X",
doi="10.1007/s00766-013-0173-8",
url="http://dx.doi.org/10.1007/s00766-013-0173-8"
"


@Article"Lisboa2011,
author="Lisboa, Liana Barachisio
and Garcia, Vinicius Cardoso
and de Almeida, Eduardo Santana
and Meira, Silvio Romero de Lemos",
title="ToolDAy: a tool for domain analysis",
journal="International Journal on Software Tools for Technology Transfer",
year="2011",
volume="13",
number="4",
pages="337--353",
abstract="Domain analysis is the process of identifying and documenting common and variable characteristics of systems in a specific domain. This process is a large and complex one, involving many interrelated activities, making it essential to have a tool support for aiding the process. We present a domain analysis tool called ToolDAy that has the purpose of making the process semi-automatic. The requirements definition presented were based on the results of a systematic review that analyzed several existing tools. Furthermore, this article describes the tool architecture, implementation and its evaluations (two as a controlled experiment and one as an industrial case study) with three different domains. The results of these evaluations indicate that the tool can aid the domain analyst to achieve systematic reuse in an effective way.",
issn="1433-2787",
doi="10.1007/s10009-010-0174-6",
url="http://dx.doi.org/10.1007/s10009-010-0174-6"
"


@Article"Zschaler2010,
author="Zschaler, Steffen",
title="Formal specification of non-functional properties of component-based software systems",
journal="Software "\&" Systems Modeling",
year="2010",
volume="9",
number="2",
pages="161--201",
abstract="Component-based software engineering (CBSE) is viewed as an opportunity to deal with the increasing complexity of modern-day software. Along with CBSE comes the notion of component markets, where more or less generic pieces of software are traded, to be combined into applications by third-party application developers. For such a component market to work successfully, all relevant properties of components must be precisely and formally described. This is especially true for non-functional properties, such as performance, memory foot print, or security. While the specification of functional properties is well understood, non-functional properties are only beginning to become a research focus. This paper discusses semantic concepts for the specification of non-functional properties, taking into account the specific needs of a component market. Based on these semantic concepts, we present a new specification language QML/CS that can be used to model non-functional product properties of components and component-based software systems.",
issn="1619-1374",
doi="10.1007/s10270-009-0115-6",
url="http://dx.doi.org/10.1007/s10270-009-0115-6"
"


@Article"Derakhshanmanesh2014,
author="Derakhshanmanesh, Mahdi
and Fox, Joachim
and Ebert, J"\"u"rgen",
title="Requirements-driven incremental adoption of variability management techniques and tools: an industrial experience report",
journal="Requirements Engineering",
year="2014",
volume="19",
number="4",
pages="333--354",
abstract="In theory, software product line engineering has reached a mature state. In practice though, implementing a variability management approach remains a tough case-by-case challenge for any organization. To tame the complexity of this undertaking, it is inevitable to handle variability from multiple perspectives and to manage variability consistently across artifacts, tools, and workflows. Especially, a solid understanding and management of the requirements to be met by the products is an inevitable prerequisite. In this article, we share experiences from the ongoing incremental adoption of explicit variability management at TRW Automotive's department for automotive slip control systems---located in Koblenz, Germany. On the technical side, the three key drivers of this adoption effort are (a) domain modeling and scoping, (b) handling of variability in requirements and (c) tighter integration of software engineering focus areas (e.g., domain modeling, requirements engineering, architectural modeling) to make use of variability-related data. In addition to implementation challenges with using and integrating concrete third-party tools, social and workflow-related issues are covered as well. The lessons learned are presented, discussed, and thoroughly compared with the state of the art in research.",
issn="1432-010X",
doi="10.1007/s00766-013-0185-4",
url="http://dx.doi.org/10.1007/s00766-013-0185-4"
"


@Article"Haber2015,
author="Haber, Arne
and H"\"o"lldobler, Katrin
and Kolassa, Carsten
and Look, Markus
and M"\"u"ller, Klaus
and Rumpe, Bernhard
and Schaefer, Ina
and Schulze, Christoph",
title="Systematic synthesis of delta modeling languages",
journal="International Journal on Software Tools for Technology Transfer",
year="2015",
volume="17",
number="5",
pages="601--626",
abstract="Delta modeling is a modular, yet flexible approach to capture variability by explicitly representing differences between system variants or versions. The conceptual idea of delta modeling is language-independent. But, to apply delta modeling to a concrete language, either a generic transformation language has to be used or the corresponding delta language has to be manually developed for each considered base language. Generic languages and their tool support often lack readability and specific context condition checking, since they are unrelated to the base language. In this paper, we present a process that allows synthesizing a delta language from the grammar of a given base language. Our method relies on an automatically generated language extension that can be manually adapted to meet domain-specific needs. We illustrate our method using delta modeling on a textual variant of architecture diagrams. Furthermore, we evaluate our method using a comparative case study. This case study covers an architectural, a structural, and a behavioral language and compares the preexisting handwritten grammars to the generated grammars as well as the manually tailored grammars. This paper is an extension of Haber et al. (Proceedings of the 17th international software product line conference (SPLC'13), pp 22--31, 2013).",
issn="1433-2787",
doi="10.1007/s10009-015-0387-9",
url="http://dx.doi.org/10.1007/s10009-015-0387-9"
"


@Article"Brace2014,
author="Brace, William
and Ekman, Kalevi",
title="CORAMOD: a checklist-oriented model-based requirements analysis approach",
journal="Requirements Engineering",
year="2014",
volume="19",
number="1",
pages="1--26",
abstract="Requirement development activities such as requirements analysis and modelling are well defined in software engineering. A model-based requirement development may result in significant improvements in engineering design. In current product development activities in this domain, not all requirements are consciously identified and modelled. This paper presents the checklist-oriented requirements analysis modelling (CORAMOD) approach. CORAMOD is a methodology for the use of model-based systems engineering for requirements analysis of complex products utilizing checklists, the simplest kind of rational design method. The model-based focuses the requirements analysis process on requirement modelling, whereas the checklist encourages a conscious and systematic approach to identify requirements. We illustrate the utility of CORAMOD artefacts by a comprehensive case study example and modelling with system modelling language (SysML). We suggest that visual accessibility of the SysML views facilitates the full participation of all stakeholders and enables the necessary dialogue and negotiation. The approach promotes tracing derived requirements to the customer need statement and enhances validation by model execution and simulation.",
issn="1432-010X",
doi="10.1007/s00766-012-0154-3",
url="http://dx.doi.org/10.1007/s00766-012-0154-3"
"


@Article"Bagheri2014,
author="Bagheri, Ebrahim
and Ensan, Faezeh",
title="Dynamic decision models for staged software product line configuration",
journal="Requirements Engineering",
year="2014",
volume="19",
number="2",
pages="187--212",
abstract="Software product line engineering practices offer desirable characteristics such as rapid product development, reduced time-to-market, and more affordable development costs as a result of systematic representation of the variabilities of a domain of discourse that leads to methodical reuse of software assets. The development lifecycle of a product line consists of two main phases: domain engineering, which deals with the understanding and formally modeling of the target domain, and application engineering that is concerned with the configuration of a product line into one concrete product based on the preferences and requirements of the stakeholders. The work presented in this paper focuses on the application engineering phase and builds both the theoretical and technological tools to assist the stakeholders in (a) understanding the complex interactions of the features of a product line; (b) eliciting the utility of each feature for the stakeholders and hence exposing the stakeholders' otherwise implicit preferences in a way that they can more easily make decisions; and (c) dynamically building a decision model through interaction with the stakeholders and by considering the structural characteristics of software product line feature models, which will guide the stakeholders through the product configuration process. Initial exploratory empirical experiments that we have performed show that our proposed approach for helping stakeholders understand their feature preferences and its associated staged feature model configuration process is able to positively impact the quality of the end results of the application engineering process within the context of the limited number of participants. In addition, it has been observed that the offered tooling support is able to ease the staged feature model configuration process.",
issn="1432-010X",
doi="10.1007/s00766-013-0165-8",
url="http://dx.doi.org/10.1007/s00766-013-0165-8"
"


@Article"Guerra2013,
author="Guerra, Esther
and de Lara, Juan
and Kolovos, Dimitrios S.
and Paige, Richard F.
and dos Santos, Osmar Marchi",
title="Engineering model transformations with transML",
journal="Software "\&" Systems Modeling",
year="2013",
volume="12",
number="3",
pages="555--577",
abstract="Model transformation is one of the pillars of model-driven engineering (MDE). The increasing complexity of systems and modelling languages has dramatically raised the complexity and size of model transformations as well. Even though many transformation languages and tools have been proposed in the last few years, most of them are directed to the implementation phase of transformation development. In this way, even though transformations should be built using sound engineering principles---just like any other kind of software---there is currently a lack of cohesive support for the other phases of the transformation development, like requirements, analysis, design and testing. In this paper, we propose a unified family of languages to cover the life cycle of transformation development enabling the engineering of transformations. Moreover, following an MDE approach, we provide tools to partially automate the progressive refinement of models between the different phases and the generation of code for several transformation implementation languages.",
issn="1619-1374",
doi="10.1007/s10270-011-0211-2",
url="http://dx.doi.org/10.1007/s10270-011-0211-2"
"


@Article"Gholami2014,
author="Gholami, Mahdi Fahmideh
and Sharifi, Mohsen
and Jamshidi, Pooyan",
title="Enhancing the OPEN Process Framework with service-oriented method fragments",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="1",
pages="361--390",
abstract="Service orientation is a promising paradigm that enables the engineering of large-scale distributed software systems using rigorous software development processes. The existing problem is that every service-oriented software development project often requires a customized development process that provides specific service-oriented software engineering tasks in support of requirements unique to that project. To resolve this problem and allow situational method engineering, we have defined a set of method fragments in support of the engineering of the project-specific service-oriented software development processes. We have derived the proposed method fragments from the recurring features of 11 prominent service-oriented software development methodologies using a systematic mining approach. We have added these new fragments to the repository of OPEN Process Framework to make them available to software engineers as reusable fragments using this well-known method repository.",
issn="1619-1374",
doi="10.1007/s10270-011-0222-z",
url="http://dx.doi.org/10.1007/s10270-011-0222-z"
"


@Article"deMello2015,
author="de Mello, Rafael Maiani
and da Silva, Pedro Corr"\^e"a
and Travassos, Guilherme Horta",
title="Investigating probabilistic sampling approaches for large-scale surveys in software engineering",
journal="Journal of Software Engineering Research and Development",
year="2015",
volume="3",
number="1",
pages="8",
abstract="Establishing representative samples for Software Engineering surveys is still considered a challenge. Specialized literature often presents limitations on interpreting surveys' results, mainly due to the use of sampling frames established by convenience and non-probabilistic criteria for sampling from them. In this sense, we argue that a strategy to support the systematic establishment of sampling frames from an adequate source of sampling can contribute to improve this scenario.",
issn="2195-1721",
doi="10.1186/s40411-015-0023-0",
url="http://dx.doi.org/10.1186/s40411-015-0023-0"
"


@Article"Seffah2012,
author="Seffah, Ahmed
and Taleb, Mohamed",
title="Tracing the evolution of HCI patterns as an interaction design tool",
journal="Innovations in Systems and Software Engineering",
year="2012",
volume="8",
number="2",
pages="93--109",
abstract="Design patterns have been introduced as a medium to capture and disseminate the best design knowledge and practices. In the field of human--computer interaction, practitioners and researchers have explored different avenues to use patterns and pattern languages as design tools. This paper surveys these avenues---from individual pattern use for solving a specific design problem, to pattern-oriented design, which guides designers in building a conceptual design by leveraging relationships between patterns. One of our underlying goals is to investigate how patterns can be used, not only to foster the reuse of proven and valid design solutions, but also as a central artefact in the process of deriving a design from user experiences and requirements. We will present our investigations on pattern-based design, and discuss how user experiences can be incorporated in the pattern selection process through the use of user variables, pattern attributes and associated relationships.",
issn="1614-5054",
doi="10.1007/s11334-011-0178-8",
url="http://dx.doi.org/10.1007/s11334-011-0178-8"
"


@Article"Acher2014,
author="Acher, Mathieu
and Cleve, Anthony
and Collet, Philippe
and Merle, Philippe
and Duchien, Laurence
and Lahire, Philippe",
title="Extraction and evolution of architectural variability models in plugin-based systems",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="4",
pages="1367--1394",
abstract="Variability management is a key issue when building and evolving software-intensive systems, making it possible to extend, configure, customize and adapt such systems to customers' needs and specific deployment contexts. A wide form of variability can be found in extensible software systems, typically built on top of plugin-based architectures that offer a (large) number of configuration options through plugins. In an ideal world, a software architect should be able to generate a system variant on-demand, corresponding to a particular assembly of plugins. To this end, the variation points and constraints between architectural elements should be properly modeled and maintained over time (i.e., for each version of an architecture). A crucial, yet error-prone and time-consuming, task for a software architect is to build an accurate representation of the variability of an architecture, in order to prevent unsafe architectural variants and reach the highest possible level of flexibility. In this article, we propose a reverse engineering process for producing a variability model (i.e., a feature model) of a plugin-based architecture. We develop automated techniques to extract and combine different variability descriptions, including a hierarchical software architecture model, a plugin dependency model and the software architect knowledge. By computing and reasoning about differences between versions of architectural feature models, software architect can control both the variability extraction and evolution processes. The proposed approach has been applied to a representative, large-scale plugin-based system (FraSCAti), considering different versions of its architecture. We report on our experience in this context.",
issn="1619-1374",
doi="10.1007/s10270-013-0364-2",
url="http://dx.doi.org/10.1007/s10270-013-0364-2"
"


@Article"Mammar2009,
author="Mammar, Amel",
title="A systematic approach to generate B preconditions: application to the database domain",
journal="Software "\&" Systems Modeling",
year="2009",
volume="8",
number="3",
pages="385--401",
abstract="Maintaining integrity constraints in information systems is a real issue. In our previous work, we have defined a formal approach that derives B formal specifications from a UML description of the system. Basically, the generated B specification is composed of a set of variables modeling data and a set of operations representing transactions. The integrity constraints are directly specified as B invariant properties. So far, the operations we generate establish only a reduced class of constraints. In this paper, we describe a systematic approach to identify preconditions that take a larger class of invariants into account. The key idea is the definition of rewriting and simplification rules that we apply to the B invariants.",
issn="1619-1374",
doi="10.1007/s10270-008-0098-8",
url="http://dx.doi.org/10.1007/s10270-008-0098-8"
"


@Article"Buchmann2014,
author="Buchmann, Thomas
and Westfechtel, Bernhard",
title="Mapping feature models onto domain models: ensuring consistency of configured domain models",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="4",
pages="1495--1527",
abstract="We present an approach to model-driven software product line engineering which is based on feature models and domain models. A feature model describes both common and varying properties of the instances of a software product line. The domain model is composed of a structural model (package and class diagrams) and a behavioral model (story diagrams). Features are mapped onto the domain model by annotating elements of the domain model with features. An element of a domain model is specific to the features included in its feature annotation. An instance of the product line is defined by a set of selected features (a feature configuration). A configuration of the domain model is built by excluding all elements whose feature set is not included in the feature configuration. To ensure consistency of the configured domain model, we define constraints on the annotations of inter-dependent domain model elements. These constraints guarantee that a model element may be selected only when the model elements are also included on which it depends. Violations of dependency constraints may be removed automatically with the help of an error repair tool which propagates features to dependent model elements.",
issn="1619-1374",
doi="10.1007/s10270-012-0305-5",
url="http://dx.doi.org/10.1007/s10270-012-0305-5"
"


@Article"Winkler2010,
author="Winkler, Stefan
and von Pilgrim, Jens",
title="A survey of traceability in requirements engineering and model-driven development",
journal="Software "\&" Systems Modeling",
year="2010",
volume="9",
number="4",
pages="529--565",
abstract="Traceability---the ability to follow the life of software artifacts---is a topic of great interest to software developers in general, and to requirements engineers and model-driven developers in particular. This article aims to bring those stakeholders together by providing an overview of the current state of traceability research and practice in both areas. As part of an extensive literature survey, we identify commonalities and differences in these areas and uncover several unresolved challenges which affect both domains. A good common foundation for further advances regarding these challenges appears to be a combination of the formal basis and the automated recording opportunities of MDD on the one hand, and the more holistic view of traceability in the requirements engineering domain on the other hand.",
issn="1619-1374",
doi="10.1007/s10270-009-0145-0",
url="http://dx.doi.org/10.1007/s10270-009-0145-0"
"


@Article"Biehl2014,
author="Biehl, Matthias
and El-Khoury, Jad
and Loiret, Fr"\'e"d"\'e"ric
and T"\"o"rngren, Martin",
title="On the modeling and generation of service-oriented tool chains",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="2",
pages="461--480",
abstract="Tool chains have grown from ad-hoc solutions to complex software systems, which often have a service-oriented architecture. With service-oriented tool integration, development tools are made available as services, which can be orchestrated to form tool chains. Due to the increasing sophistication and size of tool chains, there is a need for a systematic development approach for service-oriented tool chains. We propose a domain-specific modeling language (DSML) that allows us to describe the tool chain on an appropriate level of abstraction. We present how this language supports three activities when developing service-oriented tool chains: communication, design and realization. A generative approach supports the realization of the tool chain using the service component architecture. We present experiences from an industrial case study, which applies the DSML to support the creation of a service-oriented tool chain. We evaluate the approach both qualitatively and quantitatively by comparing it with a traditional development approach.",
issn="1619-1374",
doi="10.1007/s10270-012-0275-7",
url="http://dx.doi.org/10.1007/s10270-012-0275-7"
"


@Article"El-Attar2008,
author="El-Attar, Mohamed
and Miller, James",
title="Producing robust use case diagrams via reverse engineering of use case descriptions",
journal="Software "\&" Systems Modeling",
year="2008",
volume="7",
number="1",
pages="67--83",
abstract="In a use case driven development process, a use case model is utilized by a development team to construct an object-oriented software system. The large degree of informality in use case models, coupled with the fact that use case models directly affect the quality of all aspects of the development process, is a very dangerous combination. Naturally, informal use case models are prone to contain problems, which lead to the injection of defects at a very early stage in the development cycle. In this paper, we propose a structure that will aid the detection and elimination of potential defects caused by inconsistencies present in use case models. The structure contains a small set of formal constructs that will allow use case models to be machine readable while retaining their readability by retaining a large degree of unstructured natural language. In this paper we also propose a process which utilizes the structured use cases to systematically generate their corresponding use case diagrams and vice versa. Finally a tool provides support for the new structure and the new process. To demonstrate the feasibility of this approach, a simple study is conducted using a mock online hockey store system.",
issn="1619-1374",
doi="10.1007/s10270-006-0039-3",
url="http://dx.doi.org/10.1007/s10270-006-0039-3"
"


@Article"G�rses2013,
author="G"\"u"rses, Seda
and Seguran, Magali
and Zannone, Nicola",
title="Requirements engineering within a large-scale security-oriented research project: lessons learned",
journal="Requirements Engineering",
year="2013",
volume="18",
number="1",
pages="43--66",
abstract="Requirements engineering has been recognized as a fundamental phase of the software engineering process. Nevertheless, the elicitation and analysis of requirements are often left aside in favor of architecture-driven software development. This tendency, however, can lead to issues that may affect the success of a project. This paper presents our experience gained in the elicitation and analysis of requirements in a large-scale security-oriented European research project, which was originally conceived as an architecture-driven project. In particular, we illustrate the challenges that can be faced in large-scale research projects and consider the applicability of existing best practices and off-the-shelf methodologies with respect to the needs of such projects. We then discuss how those practices and methods can be integrated into the requirements engineering process and possibly improved to address the identified challenges. Finally, we summarize the lessons learned from our experience and the benefits that a proper requirements analysis can bring to a project.",
issn="1432-010X",
doi="10.1007/s00766-011-0139-7",
url="http://dx.doi.org/10.1007/s00766-011-0139-7"
"


@Article"Elaasar2015,
author="Elaasar, Maged
and Briand, Lionel C.
and Labiche, Yvan",
title="VPML: an approach to detect design patterns of MOF-based modeling languages",
journal="Software "\&" Systems Modeling",
year="2015",
volume="14",
number="2",
pages="735--764",
abstract="A design pattern is a recurring and well-understood design fragment. In a model-driven engineering methodology, detecting occurrences of design patterns supports the activities of model comprehension and maintenance. With the recent explosion of domain-specific modeling languages, each with its own syntax and semantics, there has been a corresponding explosion in approaches to detecting design patterns that are so much tailored to those many languages that they are difficult to reuse. This makes developing generic analysis tools extremely hard. Such a generic tool is however desirable to reduce the learning curve for pattern designers as they specify patterns for different languages used to model different aspects of a system. In this paper, we propose a unified approach to detecting design patterns of MOF-based modeling languages. MOF is increasingly used to define modeling languages, including UML and BPMN. In our approach, a pattern is modeled with a Visual Pattern Modeling Language and mapped to a corresponding QVT-Relations transformation. Such a transformation runs over an input model where pattern occurrences are to be detected and reports those occurrences in a result model. The approach is prototyped on Eclipse and validated in two large case studies that involve detecting design patterns---specifically a subset of GoF patterns in a UML model and a subset of Control Flow patterns in a BPMN model. Results show that the approach is adequate for modeling complex design patterns for MOF-based modeling languages and detecting their occurrences with high accuracy and performance.",
issn="1619-1374",
doi="10.1007/s10270-013-0325-9",
url="http://dx.doi.org/10.1007/s10270-013-0325-9"
"


@Article"Schaefer2012,
author="Schaefer, Ina
and Rabiser, Rick
and Clarke, Dave
and Bettini, Lorenzo
and Benavides, David
and Botterweck, Goetz
and Pathak, Animesh
and Trujillo, Salvador
and Villela, Karina",
title="Software diversity: state of the art and perspectives",
journal="International Journal on Software Tools for Technology Transfer",
year="2012",
volume="14",
number="5",
pages="477--495",
abstract="Diversity is prevalent in modern software systems to facilitate adapting the software to customer requirements or the execution environment. Diversity has an impact on all phases of the software development process. Appropriate means and organizational structures are required to deal with the additional complexity introduced by software variability. This introductory article to the special section ``Software Diversity---Modeling, Analysis and Evolution'' provides an overview of the current state of the art in diverse systems development and discusses challenges and potential solutions. The article covers requirements analysis, design, implementation, verification and validation, maintenance and evolution as well as organizational aspects. It also provides an overview of the articles which are part of this special section and addresses particular issues of diverse systems development.",
issn="1433-2787",
doi="10.1007/s10009-012-0253-y",
url="http://dx.doi.org/10.1007/s10009-012-0253-y"
"


@Article"Sikora2012,
author="Sikora, Ernst
and Tenbergen, Bastian
and Pohl, Klaus",
title="Industry needs and research directions in requirements engineering for embedded systems",
journal="Requirements Engineering",
year="2012",
volume="17",
number="1",
pages="57--78",
abstract="The industry has a strong demand for sophisticated requirements engineering (RE) methods in order to manage the high complexity of requirements specifications for software-intensive embedded systems and ensure a high requirements quality. RE methods and techniques proposed by research are only slowly adopted by the industry. An important step to improve the adoption of novel RE approaches is to gain a detailed understanding of the needs, expectations, and constraints that RE approaches must satisfy. We have conducted an industrial study to gain an in-depth understanding of practitioners' needs concerning RE research and method development. The study involved qualitative interviews as well as quantitative data collection by means of questionnaires. We report on the main results of our study related to five aspects of RE approaches: the use of requirements models, the support for high system complexity, quality assurance for requirements, the transition between RE and architecture design, and the interrelation of RE and safety engineering. Based on the results of the study, we draw conclusions for future RE research.",
issn="1432-010X",
doi="10.1007/s00766-011-0144-x",
url="http://dx.doi.org/10.1007/s00766-011-0144-x"
"


@Article"Kornecki2009,
author="Kornecki, Andrew
and Zalewski, Janusz",
title="Certification of software for real-time safety-critical systems: state of the art",
journal="Innovations in Systems and Software Engineering",
year="2009",
volume="5",
number="2",
pages="149--161",
abstract="This paper presents an overview and discusses the role of certification in safety-critical computer systems focusing on software, and partially hardware, used in the civil aviation domain. It discusses certification activities according to RTCA DO-178B ``Software Considerations in Airborne Systems and Equipment Certification'' and touches on tool qualification according to RTCA DO-254 ``Design Assurance Guidance for Airborne Electronic Hardware.'' Specifically, certification issues as related to real-time operating systems and programming languages are reviewed, as well as software development tools and complex electronic hardware tool qualification processes are discussed. Results of an independent industry survey done by the authors are also presented.",
issn="1614-5054",
doi="10.1007/s11334-009-0088-1",
url="http://dx.doi.org/10.1007/s11334-009-0088-1"
"


@Article"El-Attar2014,
author="El-Attar, Mohamed",
title="From misuse cases to mal-activity diagrams: bridging the gap between functional security analysis and design",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="1",
pages="173--190",
abstract="Secure software engineering is concerned with developing software systems that will continue delivering its intended functionality despite a multitude of harmful software technologies that can attack these systems from anywhere and at anytime. Misuse cases and mal-activity diagrams are two techniques to model functional security requirements address security concerns early in the development life cycle. This allows system designers to equip their systems with security mechanisms built within system design rather than relying on external defensive mechanisms. In a model-driven engineering process, misuse cases are expected to drive the construction of mal-activity diagrams. However, a systematic approach to transform misuse cases into mal-activity diagrams is missing. Therefore, this process remains dependent on human skill and judgment, which raises the risk of developing mal-activity diagrams that are inconsistent with the security requirements described in misuse cases, leading to the development of an insecure system. This paper presents an authoring structure for misuse cases and a transformation technique to systematically perform this desired model transformation. A study was conducted to evaluate the proposed technique using 46 attack stories outlined in a book by a former well-known hacker (Mitnick and Simon in The art of deception: controlling the human element of security, Wiley, Indianapolis, 2002). The results indicate that applying the proposed technique produces correct mal-activity diagrams from misuse cases.",
issn="1619-1374",
doi="10.1007/s10270-012-0240-5",
url="http://dx.doi.org/10.1007/s10270-012-0240-5"
"


@Article"Reddivari2014,
author="Reddivari, Sandeep
and Rad, Shirin
and Bhowmik, Tanmay
and Cain, Nisreen
and Niu, Nan",
title="Visual requirements analytics: a framework and case study",
journal="Requirements Engineering",
year="2014",
volume="19",
number="3",
pages="257--279",
abstract="For many software projects, keeping requirements on track needs an effective and efficient path from data to decision. Visual analytics creates such a path that enables the human to extract insights by interacting with the relevant information. While various requirements visualization techniques exist, few have produced end-to-end value to practitioners. In this paper, we advance the literature on visual requirements analytics by characterizing its key components and relationships in a framework. We follow the goal--question--metric paradigm to define the framework by teasing out five conceptual goals (user, data, model, visualization, and knowledge), their specific operationalizations, and their interconnections. The framework allows us to not only assess existing approaches, but also create tool enhancements in a principled manner. We evaluate our enhanced tool support through a case study where massive, heterogeneous, and dynamic requirements are processed, visualized, and analyzed. Working together with practitioners on a contemporary software project within its real-life context leads to the main finding that visual analytics can help tackle both open-ended visual exploration tasks and well-structured visual exploitation tasks in requirements engineering. In addition, the study helps the practitioners to reach actionable decisions in a wide range of areas relating to their project, ranging from theme and outlier identification, over requirements tracing, to risk assessment. Overall, our work illuminates how the data-to-decision analytical capabilities could be improved by the increased interactivity of requirements visualization.",
issn="1432-010X",
doi="10.1007/s00766-013-0194-3",
url="http://dx.doi.org/10.1007/s00766-013-0194-3"
"


@Article"Bohner2007,
author="Bohner, Shawn
and Ravichandar, Ramya
and Arthur, James",
title="Model-based engineering for change-tolerant systems",
journal="Innovations in Systems and Software Engineering",
year="2007",
volume="3",
number="4",
pages="237--257",
abstract="Developing and evolving today's systems are often stymied by the sheer size and complexity of the capabilities being developed and integrated. At one end of the spectrum, we have sophisticated agent-based software with hundreds of thousands of collaborating nodes. These require modeling abstractions relevant to their complex workflow tasks as well as predictable transforms and mappings for the requisite elaborations and refinements that must be accomplished in composing these systems. At the other end of the spectrum, we have ever-increasing capabilities of reconfigurable hardware devices such as field-programmable gate arrays to support the emerging adaptability and flexibility needs of these systems. From a model-based engineering perspective, these challenges are very similar; both must move their abstraction and reuse levels up to meet growing productivity and quality objectives. Model-based engineering and software system variants such as the model-driven architecture (MDA) are increasingly being applied to systems development as the engineering community recognizes the benefits of managing complexity, separating key concerns, and automating transformations from high-level abstract requirements down through the implementation. However, there are challenges when it comes to establishing the correct boundaries for change-tolerant parts of the system. Capabilities engineering (CE) is a promising approach for defining long-lived components of a system to ensure some sense of change tolerance. For innovative initiatives such as the National Aeronautics and Space Administration (NASA)'s autonomous nanotechology swarms (ANTS), the development and subsequent evolution of such systems are of considerable importance as their missions involve complex, collaborative behaviors across distributed, reconfigurable satellites. In this paper, we investigate the intersection of these two technologies as they support the development of complex, change-tolerant systems. We present an effective approach for bounding computationally independent models so that, as they transition to the architecture, capabilities-based groupings of components are relevant to the change-tolerant properties that must convey in the design solution space. The model-based engineering approach is validated via a fully functional prototype and verified by generating nontrivial multiagent systems and reusing components in subsequent systems. We build off of this research completed on the collaborative agent architecture, discuss the CE approach for the transition to architecture, and then examine how this will be applied in the reconfigurable computing community with the new National Science Foundation Center for High-Performance Reconfigurable Computing. Based on this work and extrapolating from similar efforts, the model-based approach shows promise to reduce the complexities of software evolution and increase productivity---particularly as the model libraries are populated with canonical components.",
issn="1614-5054",
doi="10.1007/s11334-007-0038-8",
url="http://dx.doi.org/10.1007/s11334-007-0038-8"
"


@Article"Selim2015,
author="Selim, Gehan M. K.
and Wang, Shige
and Cordy, James R.
and Dingel, Juergen",
title="Model transformations for migrating legacy deployment models in the automotive industry",
journal="Software "\&" Systems Modeling",
year="2015",
volume="14",
number="1",
pages="365--381",
abstract="Many companies in the automotive industry have adopted model-driven development in their vehicle software development. As a major automotive company, General Motors (GM) has been using a custom-built, domain-specific modeling language, implemented as an internal proprietary metamodel, to meet the modeling needs in its control software development. Since AUTomotive Open System ARchitecture (AUTOSAR) has been developed as a standard to ease the process of integrating components provided by different suppliers and manufacturers, there has been a growing demand to migrate these GM-specific, legacy models to AUTOSAR models. Given that AUTOSAR defines its own metamodel for various system artifacts in automotive software development, we explore applying model transformations to address the challenges in migrating GM-specific, legacy models to their AUTOSAR equivalents. As a case study, we have built and validated a model transformation using the MDWorkbench tool, the Atlas Transformation Language, and the Metamodel Coverage Checker tool. This paper reports on the case study, makes observations based on our experience to assist in the development of similar types of transformations, and provides recommendations for further research.",
issn="1619-1374",
doi="10.1007/s10270-013-0365-1",
url="http://dx.doi.org/10.1007/s10270-013-0365-1"
"


@Article"Bider2015,
author="Bider, Ilia
and Perjons, Erik",
title="Design science in action: developing a modeling technique for eliciting requirements on business process management (BPM) tools",
journal="Software "\&" Systems Modeling",
year="2015",
volume="14",
number="3",
pages="1159--1188",
abstract="Selecting a suitable business process management (BPM) tool to build a business process support system for a particular business process is difficult. There are a number of BPM tools on the market that are available as systems to install locally and as services in the cloud. These tools are based on different BPM paradigms (e.g., workflow or case management) and provide different capabilities (e.g., enforcement of the control flow, shared spaces, or a collaborative environment). This makes it difficult for an organization to select a tool that would fit the business processes at hand. The paper suggests a solution for this problem. The core of the solution is a modeling technique for business processes for eliciting their requirements for a suitable BPM tool. It produces a high-level, business process model, called a ``step-relationship'' model that depicts the essential characteristics of a process in a paradigm-independent way. The solution presented in this paper has been developed based on the paradigm of design science research, and the paper discusses the research project from the design science perspective. The solution has been applied in two case studies in order to demonstrate its feasibility.",
issn="1619-1374",
doi="10.1007/s10270-014-0412-6",
url="http://dx.doi.org/10.1007/s10270-014-0412-6"
"


@Article"Singh2009,
author="Singh, Sase Narine
and Woo, Carson",
title="Investigating business-IT alignment through multi-disciplinary goal concepts",
journal="Requirements Engineering",
year="2009",
volume="14",
number="3",
pages="177--207",
abstract="The alignment of information technology (IT) with business strategies is optimal when harmony exists between organizational and system goals. Empirical evidence reveals that effective strategic alignment leads to superior financial performance for organizations. This observation has spurred extensive research into business-IT alignment. Yet, the issue of alignment remains a top concern for CIOs. We argue that the parochial view undertaken by past research into business-IT alignment is a probable cause for continuing system failures. Furthermore, strategic alignment research is limited and devoid within the requirements engineering discipline. In this paper, we highlight existing shortfalls of research in business-IT alignment, and bring to light insights that may be offered by other disciplines to augment this field. Subscribing to a multi-disciplinary perspective, we develop a goal-based framework that incorporates goals from various literatures in investigating business-IT alignment. One of the novelties of our proposed framework lies in its delineation between goals that have been originally assigned to stakeholders versus those interpreted by stakeholders. Additionally, the framework includes constructs at the strategic level for supporting the rationale of strategic level goals. We tested the usefulness and usability of this methodology in an organization with a newly developed information system founded on strategic business goals and reported lessons learnt from both researchers' and practitioners' perspectives.",
issn="1432-010X",
doi="10.1007/s00766-009-0081-0",
url="http://dx.doi.org/10.1007/s00766-009-0081-0"
"


@Article"Lung2007,
author="Lung, Chung-Horng
and Urban, Joseph E.
and Mackulak, Gerald T.",
title="Analogy-based domain analysis approach to software reuse",
journal="Requirements Engineering",
year="2007",
volume="12",
number="1",
pages="1--22",
abstract="Domain analysis is an expansion of conventional requirements analysis. Domain analysis can support effective software reuse. However, domain analysis is time consuming and is limited to a particular application area. Analogical approaches to software reuse, on the other hand, often occur across domains. Analogical problem solving is a process of transferring knowledge from a well-understood base domain to a new target problem area. Analogy can facilitate software reuse for poorly understood problems or new application areas. Analogy shares similar concepts with reuse and some analogy theories have been applied to software reuse. However, current research on software analogy often overlooks the importance of analysis for the base domain and does not consider some critical aspects of analogy concepts. Reuse must be based on high quality artifacts, especially reuse across domains. This paper presents an approach to integrate domain analysis and analogy methods. In our view, domain analysis and software analogy have complementary roles. Domain analysis is regarded as a process to identify and supply necessary information for analogical transfer. Software analogy can provide the analyst with similar problems and solutions to reuse previous domain analysis knowledge or artifacts for a new domain. This paper presents case studies to demonstrate the increase of efficiency in applying the approach. Evaluation of the approach from various perspectives is also reported.",
issn="1432-010X",
doi="10.1007/s00766-006-0035-8",
url="http://dx.doi.org/10.1007/s00766-006-0035-8"
"


@Article"Nascimento2014,
author="Nascimento, Amanda S.
and Rubira, Cec"\'i"lia MF
and Burrows, Rachel
and Castor, Fernando
and Brito, Patrick HS",
title="Designing fault-tolerant SOA based on design diversity",
journal="Journal of Software Engineering Research and Development",
year="2014",
volume="2",
number="1",
pages="13",
abstract="Over recent years, software developers have been evaluating the benefits of both Service-Oriented Architecture (SOA) and software fault tolerance techniques based on design diversity. This is achieved by creating fault-tolerant composite services that leverage functionally-equivalent services. Three major design issues need to be considered while building software fault-tolerant architectures based on design diversity: (i) selection of variants; (ii) selection of an adjudication algorithm to choose one of the results; and (iii) execution of variants. In addition, applications based on SOA need to function effectively in a dynamic environment where it is necessary to postpone decisions until runtime. In this scenario, control is highly distributed and involves conflicting user requirements. We aim to support the software architect in the design of fault-tolerant compositions.",
issn="2195-1721",
doi="10.1186/s40411-014-0013-7",
url="http://dx.doi.org/10.1186/s40411-014-0013-7"
"


@Article"Corral2015,
author="Corral, Luis
and Sillitti, Alberto
and Succi, Giancarlo",
title="Software assurance practices for mobile applications",
journal="Computing",
year="2015",
volume="97",
number="10",
pages="1001--1022",
abstract="Mobile software applications have to cope with a particular environment that involves small size, limited resources, high autonomy requirements, competitive business models and many other challenges. To provide development guidelines that respond to these needs, several practices have been introduced; however, it is not clear how these guidelines may contribute to solve the issues present in the mobile domain. Furthermore, the rapid evolution of the mobile ecosystem challenges many of the premises upon which the proposed practices were designed. In this paper, we present a survey of the literature on software assurance practices for mobile applications, with the objective of describing them and assessing their contribution and success. We identified, organized and reviewed a body of research that spans in three levels: software development processes, software product assurance practices, and software implementation practices. By carrying out this literature survey, we reviewed the different approaches that researchers on Software Engineering have provided to address the needs that raise in the mobile software development arena. Moreover, we review the evolution of these practices, identifying how the constant changes and modernization of the mobile execution environment has impacted the methods proposed in the literature. Finally, we introduced discussion on the application of these practices in a real productive setting, opening an area for further research that may determine if practitioners have followed the proposed assurance paradigms.",
issn="1436-5057",
doi="10.1007/s00607-014-0395-8",
url="http://dx.doi.org/10.1007/s00607-014-0395-8"
"


@Article"Orrego2007,
author="Orrego, Andres S.
and Mundy, Gregory E.",
title="A study of software reuse in NASA legacy systems",
journal="Innovations in Systems and Software Engineering",
year="2007",
volume="3",
number="3",
pages="167--180",
abstract="Software reuse is regarded as a highly important factor in reducing development overheads for new software projects; however, much of the literature is concerned with cost and labor savings that reuse brings to industrial software development and little is known about the inherent risks associated with reuse, particularly in the case of mission and safety-critical software systems. We present the preliminary findings of a research project geared toward assessing the impact of risk in National Aeronautics and Space Administration (NASA) legacy software in flight control systems. We introduce the concept of context variables and the impact they have on reuse within these legacy systems as well as the genealogy classification models, which provide a simple, concise method of mapping reuse between families of software projects.",
issn="1614-5054",
doi="10.1007/s11334-007-0027-y",
url="http://dx.doi.org/10.1007/s11334-007-0027-y"
"


@Article"Islam2011,
author="Islam, Shareeful
and Mouratidis, Haralambos
and J"\"u"rjens, Jan",
title="A framework to support alignment of secure software engineering with legal regulations",
journal="Software "\&" Systems Modeling",
year="2011",
volume="10",
number="3",
pages="369--394",
abstract="Regulation compliance is getting more and more important for software systems that process and manage sensitive information. Therefore, identifying and analysing relevant legal regulations and aligning them with security requirements become necessary for the effective development of secure software systems. Nevertheless, Secure Software Engineering Modelling Languages (SSEML) use different concepts and terminology from those used in the legal domain for the description of legal regulations. This situation, together with the lack of appropriate background and knowledge of laws and regulations, introduces a challenge for software developers. In particular, it makes difficult to perform (i) the elicitation of appropriate security requirements from the relevant laws and regulations; and (ii) the correct tracing of the security requirements throughout the development stages. This paper presents a framework to support the consideration of laws and regulations during the development of secure software systems. In particular, the framework enables software developers (i) to correctly elicit security requirements from the appropriate laws and regulations; and (ii) to trace these requirements throughout the development stages in order to ensure that the design indeed supports the required laws and regulations. Our framework is based on existing work from the area of secure software engineering, and it complements this work with a novel and structured process and a well-defined method. A practical case study is employed to demonstrate the applicability of our work.",
issn="1619-1374",
doi="10.1007/s10270-010-0154-z",
url="http://dx.doi.org/10.1007/s10270-010-0154-z"
"


@Article"Ali2012,
author="Ali, Shaukat
and Briand, Lionel C.
and Hemmati, Hadi",
title="Modeling robustness behavior using aspect-oriented modeling to support robustness testing of industrial systems",
journal="Software "\&" Systems Modeling",
year="2012",
volume="11",
number="4",
pages="633--670",
abstract="Model-based robustness testing requires precise and complete behavioral, robustness modeling. For example, state machines can be used to model software behavior when hardware (e.g., sensors) breaks down and be fed to a tool to automate test case generation. But robustness behavior is a crosscutting behavior and, if modeled directly, often results in large, complex state machines. These in practice tend to be error prone and difficult to read and understand. As a result, modeling robustness behavior in this way is not scalable for complex industrial systems. To overcome these problems, aspect-oriented modeling (AOM) can be employed to model robustness behavior as aspects in the form of state machines specifically designed to model robustness behavior. In this paper, we present a RobUstness Modeling Methodology (RUMM) that allows modeling robustness behavior as aspects. Our goal is to have a complete and practical methodology that covers all features of state machines and aspect concepts necessary for model-based robustness testing. At the core of RUMM is a UML profile (AspectSM) that allows modeling UML state machine aspects as UML state machines (aspect state machines). Such an approach, relying on a standard and using the target notation as the basis to model the aspects themselves, is expected to make the practical adoption of aspect modeling easier in industrial contexts. We have used AspectSM to model the crosscutting robustness behavior of a videoconferencing system and discuss the benefits of doing so in terms of reduced modeling effort and improved readability.",
issn="1619-1374",
doi="10.1007/s10270-011-0206-z",
url="http://dx.doi.org/10.1007/s10270-011-0206-z"
"


@Article"Yang2013,
author="Yang, Qi-Liang
and Lv, Jian
and Tao, Xian-Ping
and Ma, Xiao-Xing
and Xing, Jian-Chun
and Song, Wei",
title="Fuzzy Self-Adaptation of Mission-Critical Software Under Uncertainty",
journal="Journal of Computer Science and Technology",
year="2013",
volume="28",
number="1",
pages="165--187",
abstract="Mission-critical software (MCS) must provide continuous, online services to ensure the successful accomplishment of critical missions. Self-adaptation is particularly desirable for assuring the quality of service (QoS) and availability of MCS under uncertainty. Few techniques have insofar addressed the issue of MCS self-adaptation, and most existing approaches to software self-adaptation fail to take into account uncertainty in the self-adaptation loop. To tackle this problem, we propose a fuzzy control based approach, i.e., Software Fuzzy Self-Adaptation (SFSA), with a view to deal with the challenge of MCS self-adaptation under uncertainty. First, we present the SFSA conceptual framework, consisting of sensing, deciding and acting stages, and establish the formal model of SFSA to lay a rigorous and mathematical foundation of our approach. Second, we develop a novel SFSA implementation technology as well as its supporting tool, i.e., the SFSA toolkit, to automate the realization process of SFSA. Finally, we demonstrate the effectiveness of our approach through the development of an adaptive MCS application in process control systems. Validation experiments show that the fuzzy control based approach proposed in this work is effective and with low overheads.",
issn="1860-4749",
doi="10.1007/s11390-013-1321-9",
url="http://dx.doi.org/10.1007/s11390-013-1321-9"
"


@Article"Deng2011,
author="Deng, Mina
and Wuyts, Kim
and Scandariato, Riccardo
and Preneel, Bart
and Joosen, Wouter",
title="A privacy threat analysis framework: supporting the elicitation and fulfillment of privacy requirements",
journal="Requirements Engineering",
year="2011",
volume="16",
number="1",
pages="3--32",
abstract="Ready or not, the digitalization of information has come, and privacy is standing out there, possibly at stake. Although digital privacy is an identified priority in our society, few systematic, effective methodologies exist that deal with privacy threats thoroughly. This paper presents a comprehensive framework to model privacy threats in software-based systems. First, this work provides a systematic methodology to model privacy-specific threats. Analogous to STRIDE, an information flow--oriented model of the system is leveraged to guide the analysis and to provide broad coverage. The methodology instructs the analyst on what issues should be investigated, and where in the model those issues could emerge. This is achieved by (i) defining a list of privacy threat types and (ii) providing the mappings between threat types and the elements in the system model. Second, this work provides an extensive catalog of privacy-specific threat tree patterns that can be used to detail the threat analysis outlined above. Finally, this work provides the means to map the existing privacy-enhancing technologies (PETs) to the identified privacy threats. Therefore, the selection of sound privacy countermeasures is simplified.",
issn="1432-010X",
doi="10.1007/s00766-010-0115-7",
url="http://dx.doi.org/10.1007/s00766-010-0115-7"
"


@Article"Gudmundsson2015,
author="Gudmundsson, Vignir
and Schulze, Christoph
and Ganesan, Dharmalingam
and Lindvall, Mikael
and Wiegand, Robert",
title="Model-based testing of NASA's GMSEC, a reusable framework for ground system software",
journal="Innovations in Systems and Software Engineering",
year="2015",
volume="11",
number="3",
pages="217--232",
abstract="We present an empirical study in which model-based testing (MBT) was applied to the software bus of NASA's Goddard Mission Service Evolution Center (GMSEC), a reusable software framework. The goal was to study the feasibility of using MBT on a real-world software system that was designed to be flexible. GMSEC has three levels of flexibility: 1) loose application coupling through a software bus based on the publish--subscribe architectural style, 2) language independence by providing APIs to the bus in several programming languages, 3) middleware independence by providing wrappers for several middlewares that are supported by the software bus. The novelty brought forward in this paper is that one model and one set of generated test cases were used as the basis to test the software bus for behavioral consistency across multiple programming languages and middleware wrappers. The comparison of costs and benefits from using finite state machines (FSM) vs. extended FSMs (EFSM) when used for MBT on a real-world system is also novel. The case study shows that it was feasible, even for a programmer who neither knew MBT nor the system under test, to successfully apply MBT to a flexible system such as GMSEC and that MBT could within reasonable effort detect non-trivial defects in a fielded system.",
issn="1614-5054",
doi="10.1007/s11334-015-0254-6",
url="http://dx.doi.org/10.1007/s11334-015-0254-6"
"


@Article"Autili2014,
author="Autili, Marco
and Caporuscio, Mauro
and Issarny, Val"\'e"rie
and Berardinelli, Luca",
title="Model-driven engineering of middleware-based ubiquitous services",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="2",
pages="481--511",
abstract="Supporting the execution of service-oriented applications over ubiquitous networks specifically calls for a service-oriented middleware (SOM), which effectively enables ubiquitous networking while benefiting from the diversity and richness of the networking infrastructure. However, developing ubiquitous applications that exploit the specific features offered by a SOM might be a time-consuming task, which demands a deep knowledge spanning from the application domain concepts down to the underlying middleware technicalities. In this paper, first we present the model-driven development process underpinning ubiSOAP, a SOM for the ubiquitous networking domain. Then, based on the domain concepts defined by the conceptual model of ubiSOAP, its architecture and its technicalities, we propose a domain-specific environment, called ubiDSE, that aids the development of applications that exploits the ubiSOAP features, from design to implementation. ubiDSE allows developers to focus on the main behavior of the modeled systems, rather than on complex details inherent to ubiquitous environments. As part of ubiDSE, specific tools are provided to automatically generate skeleton code for service-oriented applications to be executed on ubiSOAP-enabled devices, hence facilitating the exploitation of ubiSOAP by developers.",
issn="1619-1374",
doi="10.1007/s10270-013-0344-6",
url="http://dx.doi.org/10.1007/s10270-013-0344-6"
"


@Article"Hall2012,
author="Hall, Jon G.
and Rapanotti, Lucia",
title="Software engineering as the design theoretic transformation of software problems",
journal="Innovations in Systems and Software Engineering",
year="2012",
volume="8",
number="3",
pages="175--193",
abstract="Rogers characterises engineering as:                  [...] the practice of organising the design and construction of any artifice which transforms the physical world around us to meet some recognised need.                              ",
issn="1614-5054",
doi="10.1007/s11334-011-0171-2",
url="http://dx.doi.org/10.1007/s11334-011-0171-2"
"


@Article"Jiang2008,
author="Jiang, Li
and Eberlein, Armin
and Far, Behrouz H.
and Mousavi, Majid",
title="A methodology for the selection of requirements engineering techniques",
journal="Software "\&" Systems Modeling",
year="2008",
volume="7",
number="3",
pages="303--328",
abstract="The complexity of software projects as well as the multidisciplinary nature of requirements engineering (RE) requires developers to carefully select RE techniques and practices during software development. Nevertheless, the selection of RE techniques is usually based on personal preference or existing company practice rather than on characteristics of the project at hand. Furthermore, there is a lack of guidance on which techniques are suitable for a certain project context. So far, only a limited amount of research has been done regarding the selection of RE techniques based on the attributes of the project under development. The few approaches that currently exist for the selection of RE techniques provide only little guidance for the actual selection process. We believe that the evaluation of RE techniques in the context of an application domain and a specific project is of great importance. This paper describes a Methodology for Requirements Engineering Techniques Selection (MRETS) as an approach that helps requirements engineers select suitable RE techniques for the project at hand. The MRETS has three aspects: Firstly, it aids requirements engineers in establishing a link between the attributes of the project and the attributes of RE techniques. Secondly, based on the evaluation schema proposed in our research, MRETS provides an opportunity to analyze RE techniques in detail using clustering. Thirdly, the objective function used in our approach provides an effective decision support mechanism for the selection of RE techniques. This paper makes contributions to RE techniques analysis, the application of RE techniques in practice, RE research, and software engineering in general. The application of the proposed methodology to an industrial project provides preliminary information on the effectiveness of MRETS for the selection of RE techniques.",
issn="1619-1374",
doi="10.1007/s10270-007-0055-y",
url="http://dx.doi.org/10.1007/s10270-007-0055-y"
"


@Article"Abrah�o2006,
author="Abrah"\~a"o, Silvia
and Poels, Geert
and Pastor, Oscar",
title="A functional size measurement method for object-oriented conceptual schemas: design and evaluation issues",
journal="Software "\&" Systems Modeling",
year="2006",
volume="5",
number="1",
pages="48--71",
abstract="Functional Size Measurement (FSM) methods are intended to measure the size of software by quantifying the functional user requirements of the software. The capability to accurately quantify the size of software in an early stage of the development lifecycle is critical to software project managers for evaluating risks, developing project estimates and having early project indicators. In this paper, we present OO-Method Function Points (OOmFP), which is a new FSM method for object-oriented systems that is based on measuring conceptual schemas. OOmFP is presented following the steps of a process model for software measurement. Using this process model, we present the design of the measurement method, its application in a case study, and the analysis of different evaluation types that can be carried out to validate the method and to verify its application and results.",
issn="1619-1374",
doi="10.1007/s10270-005-0098-x",
url="http://dx.doi.org/10.1007/s10270-005-0098-x"
"


@Article"Pitula2011,
author="Pitula, Kristina
and Radhakrishnan, Thiruvengadam",
title="On eliciting requirements from end-users in the ICT4D domain",
journal="Requirements Engineering",
year="2011",
volume="16",
number="4",
pages="323",
abstract="Currently, there is much interest in harnessing the potential of new and affordable Information and Communication Technologies (ICT) such as mobile phones, to assist in reducing disparities in socioeconomic conditions throughout the world. Such efforts have come to be known as ICT for Development or ICT4D. Although this field of research holds much promise, few projects have managed to achieve long-term sustained success. Among the many reasons for this, from a software engineering perspective, in many cases, it can be attributed to inadequacies in gathering and defining software requirements. Incomplete software requirements and the consequent failures in creating sustainable systems arise because of inadequate consideration of the high-level social development goals, neglect of environmental constraints and/or a lack of adequate input from end-users regarding their specific needs and socio-cultural context. We propose enhancements to the requirements elicitation methodology specifically adapted to address these shortcomings. Our approach incorporates the novel technique of Structured Digital Storytelling to elicit input from end-users who have limited literacy and applies a conceptual model derived from Communications Theory to analyse the constraints that arise from their socio-cultural context. The needs, goals and constraints thus identified are integrated using a goal-based analysis to produce a more informed understanding of the potential areas of technology intervention and the needed software requirements. We illustrate our approach and validate its effectiveness with a field study.",
issn="1432-010X",
doi="10.1007/s00766-011-0127-y",
url="http://dx.doi.org/10.1007/s00766-011-0127-y"
"


@Article"Fabian2010,
author="Fabian, Benjamin
and G"\"u"rses, Seda
and Heisel, Maritta
and Santen, Thomas
and Schmidt, Holger",
title="A comparison of security requirements engineering methods",
journal="Requirements Engineering",
year="2010",
volume="15",
number="1",
pages="7--40",
abstract="This paper presents a conceptual framework for security engineering, with a strong focus on security requirements elicitation and analysis. This conceptual framework establishes a clear-cut vocabulary and makes explicit the interrelations between the different concepts and notions used in security engineering. Further, we apply our conceptual framework to compare and evaluate current security requirements engineering approaches, such as the Common Criteria, Secure Tropos, SREP, MSRA, as well as methods based on UML and problem frames. We review these methods and assess them according to different criteria, such as the general approach and scope of the method, its validation, and quality assurance capabilities. Finally, we discuss how these methods are related to the conceptual framework and to one another.",
issn="1432-010X",
doi="10.1007/s00766-009-0092-x",
url="http://dx.doi.org/10.1007/s00766-009-0092-x"
"


@Article"Jiang2008,
author="Jiang, Li
and Eberlein, Armin
and Far, Behrouz H.",
title="A case study validation of a knowledge-based approach for the selection of requirements engineering techniques",
journal="Requirements Engineering",
year="2008",
volume="13",
number="2",
pages="117--146",
abstract="Requirements engineering (RE) is a critical phase in the software engineering process and plays a vital role in ensuring the overall quality of a software product. Recent research has shown that industry increasingly recognizes the importance of good RE practices and the use of appropriate RE techniques. However, due to the large number of RE techniques, requirements engineers find it challenging to select suitable techniques for a particular project. Unfortunately, technique selection based on personal experience has limitations with regards to the scope, effectiveness and suitability of the RE techniques for the project at hand. In this paper, a Knowledge-based Approach for the Selection of Requirements Engineering Techniques (KASRET) is proposed that helps during RE techniques selection. This approach has three major features. First, a library of requirements techniques was developed which includes detailed knowledge about RE techniques. Second, KASRET integrates advantages of different knowledge representation schemata and reasoning mechanisms. Thus, KASRET provides mechanisms for the management of knowledge about requirements techniques and support for RE process development. Third, as a major decision support mechanism, an objective function evaluates the overall ability and cost of RE techniques, which is helpful for the selection of RE techniques. This paper makes not only a contribution to RE but also to research and application of knowledge management and decision support in process development. A case study using an industrial project shows the support of KASRET for RE techniques selection.",
issn="1432-010X",
doi="10.1007/s00766-007-0060-2",
url="http://dx.doi.org/10.1007/s00766-007-0060-2"
"


@Article"Ellis2013,
author="Ellis, Keith
and Berry, Daniel M.",
title="Quantifying the impact of requirements definition and management process maturity on project outcome in large business application development",
journal="Requirements Engineering",
year="2013",
volume="18",
number="3",
pages="223--249",
abstract="Using data from two surveys of people knowledgeable about requirements for, and the success of the development of, large commercial applications (CAs) in hundreds of large organizations from around the world, this paper reports a high positive correlation between an organization's requirements definition and management (RDM) maturity and that organization's successful performance on CA development projects. Among the organizations that responded with a filled survey, an organization that is assessed at a high RDM maturity is significantly more successful in its CA development projects than is an organization that is assessed at a low RDM maturity, when success in CA development projects is measured as (1) delivering CAs on-time, on-budget, and on-function, (2) meeting the business objectives of these projects, and (3) the perceived success of these projects. This paper presents a comprehensive framework for RDM, describes a quality RDM process, and describes RDM maturity and how to measure it. It describes the two surveys, the first of which ended up being a pilot for the second, which was designed taking into account what was learned from the first survey. The paper concludes with advice to practitioners on the application of the RDM maturity framework in any organization that wishes to improve its RDM and its performance in the development of large CAs.",
issn="1432-010X",
doi="10.1007/s00766-012-0146-3",
url="http://dx.doi.org/10.1007/s00766-012-0146-3"
"


@Article"Bernardi2011,
author="Bernardi, Simona
and Merseguer, Jos"\'e"
and Petriu, Dorina C.",
title="A dependability profile within MARTE",
journal="Software "\&" Systems Modeling",
year="2011",
volume="10",
number="3",
pages="313--336",
abstract="The importance of assessing software non-functional properties (NFP) beside the functional ones is well accepted in the software engineering community. In particular, dependability is a NFP that should be assessed early in the software life-cycle by evaluating the system behaviour under different fault assumptions. Dependability-specific modeling and analysis techniques include for example Failure Mode and Effect Analysis for qualitative evaluation, stochastic Petri nets for quantitative evaluation, and fault trees for both forms of evaluation. Unified Modeling Language (UML) may be specialized for different domains by using the profile mechanism. For example, the MARTE profile extends UML with concepts for modeling and quantitative analysis of real-time and embedded systems (more specifically, for schedulability and performance analysis). This paper proposes to add to MARTE a profile for dependability analysis and modeling (DAM). A case study of an intrusion-tolerant message service will offer insight on how the MARTE-DAM profile can be used to derive a stochastic Petri net model for performance and dependability assessment.",
issn="1619-1374",
doi="10.1007/s10270-009-0128-1",
url="http://dx.doi.org/10.1007/s10270-009-0128-1"
"


@Article"Kahraman2015,
author="Kahraman, G"\"o"khan
and Bilgen, Semih",
title="A framework for qualitative assessment of domain-specific languages",
journal="Software "\&" Systems Modeling",
year="2015",
volume="14",
number="4",
pages="1505--1526",
abstract="Domain-specific languages (DSLs) are used for improving many facets of software development, but whether and to what extent this aim is achieved is an important issue that must be addressed. This paper presents a proposal for a Framework for Qualitative Assessment of DSLs (FQAD). FQAD is used for determining the perspective of the evaluator, understanding the goal of the assessment and selecting fundamental DSL quality characteristics to guide the evaluator in the process. This framework adapts and integrates the ISO/IEC 25010:2011 standard, CMMI maturity level evaluation approach and the scaling approach used in DESMET into a perspective-based assessment. A detailed list of domain-specific language quality characteristics is elaborated, and a novel assessment method is proposed. Two case studies through which FQAD is matured and evaluated are reported. The case studies have shown that stakeholders find the FQAD process beneficial.",
issn="1619-1374",
doi="10.1007/s10270-013-0387-8",
url="http://dx.doi.org/10.1007/s10270-013-0387-8"
"


@Article"Perrouin2012,
author="Perrouin, Gilles
and Vanwormhoudt, Gilles
and Morin, Brice
and Lahire, Philippe
and Barais, Olivier
and J"\'e"z"\'e"quel, Jean-Marc",
title="Weaving variability into domain metamodels",
journal="Software "\&" Systems Modeling",
year="2012",
volume="11",
number="3",
pages="361--383",
abstract="Domain-specific modeling languages (DSMLs) are the essence of MDE. A DSML describes the concepts of a particular domain in a metamodel, as well as their relationships. Using a DSML, it is possible to describe a wide range of different models that often share a common base and vary on some parts. On the one hand, some current approaches tend to distinguish the variability language from the DSMLs themselves, implying greater learning curve for DSMLs stakeholders and a significant overhead in product line engineering. On the other hand, approaches integrating variability in DSMLs lack generality and tool support. We argue that aspect-oriented modeling techniques enabling flexible metamodel composition and results obtained by the software product line community to manage and resolve variability form the pillars for a solution for integrating variability into DSMLs. In this article, we consider variability as an independent and generic aspect to be woven into the DSML. In particular, we detail how variability is woven and how to perform product line derivation. We validate our approach through the weaving of variability into two different metamodels: Ecore---widely used for DSML definition---and SmartAdapters, our aspect model weaver. These results emphasize how new abilities of the language can be provided by this means.",
issn="1619-1374",
doi="10.1007/s10270-010-0186-4",
url="http://dx.doi.org/10.1007/s10270-010-0186-4"
"


@Article"Beckers2013,
author="Beckers, Kristian
and C"\^o"t"\'e", Isabelle
and Fa"\ss"bender, Stephan
and Heisel, Maritta
and Hofbauer, Stefan",
title="A pattern-based method for establishing a cloud-specific information security management system",
journal="Requirements Engineering",
year="2013",
volume="18",
number="4",
pages="343--395",
abstract="Assembling an information security management system (ISMS) according to the ISO 27001 standard is difficult, because the standard provides only very sparse support for system development and documentation. Assembling an ISMS consists of several difficult tasks, e.g., asset identification, threat and risk analysis and security reasoning. Moreover, the standard demands consideration of laws and regulations, as well as privacy concerns. These demands present multi-disciplinary challenges for security engineers. Cloud computing provides scalable IT resources and the challenges of establishing an ISMS increases, because of the significant number of stakeholders and technologies involved and the distribution of clouds among many countries. We analyzed the ISO 27001 demands for these multi-disciplinary challenges and cloud computing systems. Based on these insights, we provide a method that relies upon existing requirements engineering methods and patterns for several security tasks, e.g., context descriptions, threat analysis and policy definition. These can ease the effort of establishing an ISMS and can produce the necessary documentation for an ISO 27001 compliant ISMS. We illustrate our approach using the example of an online bank.",
issn="1432-010X",
doi="10.1007/s00766-013-0174-7",
url="http://dx.doi.org/10.1007/s00766-013-0174-7"
"


@Article"Patr�cio2009,
author="Patr"\'i"cio, Lia
and Falc"\~a"o e Cunha, Jo"\~a"o
and Fisk, Raymond P.",
title="Requirements engineering for multi-channel services: the SEB method and its application to a multi-channel bank",
journal="Requirements Engineering",
year="2009",
volume="14",
number="3",
pages="209--227",
abstract="The widespread usage of technology for service provision to customers has created a new and challenging environment for the design of interactive systems, with the emergence of technology enabled multi-channel services. Requirements engineers involved in the design of such service systems must actively work together with interaction designers and service managers to better integrate customer service experience and technology components, requiring unifying methods and tools within the emerging field of service science management and engineering. This paper proposes the service experience blueprint (SEB), a multidisciplinary method for the design of technology enabled multi-channel service systems and illustrates its application in two examples of redesign of banking services that involved an extensive study with more than 4,000 bank customers. The SEB method is based on concepts and tools from RE and interaction design, such as goal-oriented analysis and conceptual modeling, but also uses methods developed in the service and marketing fields, such as service blueprinting. SEB brings marketing research methods to the requirements process, as they can provide a useful contribution for the elicitation of customer experience requirements in service environments. By bringing together goal-oriented modeling and use case modeling from requirements engineering, with service blueprinting from service design, the SEB method contributes to creating a shared understanding and a unifying language to better support the design of new technology enabled multi-channel service systems, where technology and service issues are deeply intertwined.",
issn="1432-010X",
doi="10.1007/s00766-009-0082-z",
url="http://dx.doi.org/10.1007/s00766-009-0082-z"
"


@Article"R�ttger2007,
author="R"\"o"ttger, Simone
and Zschaler, Steffen",
title="Tool Support for Refinement of Non-functional Specifications",
journal="Software "\&" Systems Modeling",
year="2007",
volume="6",
number="2",
pages="185--204",
abstract="Model driven architecture (MDA) views application development as a continuous transformation of models of the target system. We propose a methodology which extends this view to non-functional properties. In previous publications we have shown how we can use so-called context models to make the specification of non-functional measurements independent of their application in concrete system specifications. We have also shown how this allows us to distinguish two roles in the development process: the measurement designer and the application designer.",
issn="1619-1374",
doi="10.1007/s10270-006-0024-x",
url="http://dx.doi.org/10.1007/s10270-006-0024-x"
"


@Article"Sutcliffe2011,
author="Sutcliffe, Alistair
and Thew, Sarah
and Jarvis, Paul",
title="Experience with user-centred requirements engineering",
journal="Requirements Engineering",
year="2011",
volume="16",
number="4",
pages="267--280",
abstract="This paper describes the application of human--computer interaction (HCI) principles and methods to requirements engineering in a case study development of a visualisation tool, ADVISES, to support epidemiological research. The development approach consisted of scenario-based design and analysis of the users' tasks and mental model of the domain. Prototyping and storyboarding techniques were used to explore design options with users as well as specifying functionality for two versions of the software to meet the needs of novice and expert users. Application of HCI functional allocation heuristics to guide system requirements decisions is explained. An evaluation of the prototype was carried out to assess the extent to which the expert model would support public health professionals in their analysis activities. The results of the design exploration requirements analysis study are reported. The implications of scenario-based design exploration, functional allocation and software architecture are discussed.",
issn="1432-010X",
doi="10.1007/s00766-011-0118-z",
url="http://dx.doi.org/10.1007/s00766-011-0118-z"
"


@Article"deKinderen2014,
author="de Kinderen, Sybren
and Gaaloul, Khaled
and Proper, Henderik A.",
title="Bridging value modelling to ArchiMate via transaction modelling",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="3",
pages="1043--1057",
abstract="The ArchiMate modelling language provides a coherent and a holistic view of an enterprise in terms of its products, services, business processes, actors, business units, software applications and more. Yet, ArchiMate currently lacks (1) expressivity in modelling an enterprise from a value exchange perspective, and (2) rigour and guidelines in modelling business processes that realize the transactions relevant from a value perspective. To address these issues, we show how to connect e                                                                                          "\$""\$"^"\""3"\"""\$""\$"                                value, a technique for value modelling, to ArchiMate via transaction patterns from the DEMO methodology. Using ontology alignment techniques, we show a transformation between the meta models underlying e                                                                                          "\$""\$"^"\""3"\"""\$""\$"                                value, DEMO and ArchiMate. Furthermore, we present a step-wise approach that shows how this model transformation is achieved and, in doing so, we also show the of such a transformation. We exemplify the transformation of DEMO and e                                                                                          "\$""\$"^"\""3"\"""\$""\$"                                value into ArchiMate by means of a case study in the insurance industry. As a proof of concept, we present a software tool supporting our transformation approach. Finally, we discuss the functionalities and limitations of our approach; thereby, we analyze its and practical applicability.",
issn="1619-1374",
doi="10.1007/s10270-012-0299-z",
url="http://dx.doi.org/10.1007/s10270-012-0299-z"
"


@Article"Alves2014,
author="Alves, Everton L. G.
and Machado, Patricia D. L.
and Ramalho, Franklin",
title="Automatic generation of built-in contract test drivers",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="3",
pages="1141--1165",
abstract="Automatic generation of platform-independent and -dependent built-in contract test drivers that check pairwise interactions between client and server components is presented, focusing on the built-in contract testing (BIT) method and the model-driven testing approach. Components are specified by UML diagrams that define the contract between client and server, independent of a specific platform. MDA approaches are applied to formalize and perform automatic transformations from a platform-independent model to a platform-independent test architecture according to a BIT profile. The test architecture is mapped to Java platform models and then to test code. All these transformations are specified by a set of transformation rules written in the Atlas Transformation Language (ATL) that are automatically performed by the ATL engine. The solution named the MoBIT tool is applied to case studies in order to investigate the expected benefits and challenges to be faced.",
issn="1619-1374",
doi="10.1007/s10270-012-0282-8",
url="http://dx.doi.org/10.1007/s10270-012-0282-8"
"


@Article"K�ster2006,
author="K"\"u"ster, Jochen M.",
title="Definition and validation of model transformations",
journal="Software "\&" Systems Modeling",
year="2006",
volume="5",
number="3",
pages="233--259",
abstract="With model transformations becoming more widely used, there is an increasing need for approaches focussing on a systematic development of model transformations. Although a number of approaches for specifying model transformations exist, none of them focusses on systematically validating model transformations with respect to termination and confluence. Termination and confluence ensure that a model transformation always produces a unique result. Also called functionality, these properties are important requirements for practical applications of model transformations. In this paper, we introduce our approach to model transformation. Using and extending results from the theory of graph transformation, we investigate termination and confluence properties of model transformations specified in our approach. We establish a set of criteria for termination and confluence to be checked at design time by static analysis of the transformation rules and the underlying metamodels. Moreover, the criteria are formulated in such a way that they require less experience with the theory of graph transformation. Our concepts are illustrated by a running example of a model tranformation from statecharts to the process algebra Communicating Sequential Processes.",
issn="1619-1374",
doi="10.1007/s10270-006-0018-8",
url="http://dx.doi.org/10.1007/s10270-006-0018-8"
"


@Article"Leitner2014,
author="Leitner, Andrea
and Preschern, Christopher
and Kreiner, Christian",
title="Effective development of automation systems through domain-specific modeling in a small enterprise context",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="1",
pages="35--54",
abstract="High development and maintenance costs and a high error rate are the major problems in the development of automation systems, which are mainly caused by bad communication and inefficient reuse methods. To overcome these problems, we propose a more systematic reuse approach. Though systematic reuse approaches such as software product lines are appealing, they tend to involve rather burdensome development and management processes. This paper focuses on small enterprises. Since such companies are often unable to perform a ``big bang'' adoption of the software product line, we suggest an incremental, more lightweight process to transition from single-system development to software product line development. Besides the components of the transition process, this paper discusses tool selection, DSL technology, stakeholder communication support, and business considerations. Although based on problems from the automation system domain, we believe the approach may be general enough to be applicable in other domains as well. The approach has proven successful in two case studies. First, we applied it to a research project for the automation of a logistics lab model, and in the second case (a real-life industry case), we investigated the approaches suitability for fish farm automation systems. Several metrics were collected throughout the evolution of each case, and this paper presents the data for single system development, clone"\&"own and software product line development. The results and observable effects are compared, discussed, and finally summarized in a list of lessons learned.",
issn="1619-1374",
doi="10.1007/s10270-012-0289-1",
url="http://dx.doi.org/10.1007/s10270-012-0289-1"
"


@Article"Iqbal2015,
author="Iqbal, Muhammad Zohaib
and Arcuri, Andrea
and Briand, Lionel",
title="Environment modeling and simulation for automated testing of soft real-time embedded software",
journal="Software "\&" Systems Modeling",
year="2015",
volume="14",
number="1",
pages="483--524",
abstract="Given the challenges of testing at the system level, only a fully automated approach can really scale up to industrial real-time embedded systems (RTES). Our goal is to provide a practical approach to the model-based testing of RTES by allowing system testers, who are often not familiar with the system's design but are application domain experts, to model the system environment in such a way as to enable its black-box test automation. Environment models can support the automation of three tasks: the code generation of an environment simulator to enable testing on the development platform or without involving actual hardware, the selection of test cases, and the evaluation of their expected results (oracles). From a practical standpoint---and such considerations are crucial for industrial adoption---environment modeling should be based on modeling standards (1) that are at an adequate level of abstraction, (2) that software engineers are familiar with, and (3) that are well supported by commercial or open source tools. In this paper, we propose a precise environment modeling methodology fitting these requirements and discuss how these models can be used to generate environment simulators. The environment models are expressed using UML/MARTE and OCL, which are international standards for real-time systems and constraint modeling. The presented techniques are evaluated on a set of three artificial problems and on two industrial RTES.",
issn="1619-1374",
doi="10.1007/s10270-013-0328-6",
url="http://dx.doi.org/10.1007/s10270-013-0328-6"
"


@Article"Garousi2012,
author="Garousi, Vahid",
title="Classification and trend analysis of UML books (1997--2009)",
journal="Software "\&" Systems Modeling",
year="2012",
volume="11",
number="2",
pages="273--285",
abstract="Technical books of each subject area denote the level of maturity and knowledge demand in that area. According to the Google Books database, about 208 Unified Modeling Language (UML) books have been published from its inception in 1997 until 2009. While various book reviews are frequently published in various sources (e.g., IEEE Software Bookshelf), there are no studies to classify UML books into meaningful categories. Such a classification can help researchers in the area to identify trends and also reveal the level of activity in each sub-area of UML. The statistical survey reported in this article intends to be a first step in classification and trend analysis of the UML books published from 1997 to 2009. The study also sheds light on the quantity of books published in different focus areas (e.g., UML's core concepts, patterns, tool support, Object Constraint Language and Model-Driven Architecture) and also on different application domains (e.g., database modeling, web applications, and real-time systems). The trends of book publications in each sub-area of UML are also used to track the level of maturity, to identify possible Hype cycles and also to measure knowledge demand in each area.",
issn="1619-1374",
doi="10.1007/s10270-011-0189-9",
url="http://dx.doi.org/10.1007/s10270-011-0189-9"
"


@Article"Jureta2007,
author="Jureta, Ivan J.
and Faulkner, St"\'e"phane
and Schobbens, Pierre-Yves",
title="Clear justification of modeling decisions for goal-oriented requirements engineering",
journal="Requirements Engineering",
year="2007",
volume="13",
number="2",
pages="87",
abstract="Representation and reasoning about goals of an information system unavoidably involve the transformation of unclear stakeholder requirements into an instance of a goal model. If the requirements engineer does not justify why one clear form of requirements is chosen over others, the subsequent modeling decisions cannot be justified either. If arguments for clarification and modeling decisions are instead explicit, justifiably appropriate instances of goal models can be constructed and additional analyses applied to discover richer sets of requirements. The paper proposes the ``Goal Argumentation Method (GAM)'' to fulfil three roles: (i) GAM guides argumentation and justification of modeling choices during the construction or critique of goal model instances; (ii) it enables the detection of deficient argumentation within goal model instances; and (iii) it provides practical techniques for the engineer to ensure that requirements appearing both in arguments and in model instance elements are clear.",
issn="1432-010X",
doi="10.1007/s00766-007-0056-y",
url="http://dx.doi.org/10.1007/s00766-007-0056-y"
"


@Article"Zeni2015,
author="Zeni, Nicola
and Kiyavitskaya, Nadzeya
and Mich, Luisa
and Cordy, James R.
and Mylopoulos, John",
title="GaiusT: supporting the extraction of rights and obligations for regulatory compliance",
journal="Requirements Engineering",
year="2015",
volume="20",
number="1",
pages="1--22",
abstract="Ensuring compliance of software systems with government regulations, policies, and laws is a complex problem. Generally speaking, solutions to the problem first identify rights and obligations defined in the law and then treat these as requirements for the system under design. This work examines the challenge of developing tool support for extracting such requirements from legal documents. To address this challenge, we have developed a tool called GaiusT. The tool is founded on a framework for textual semantic annotation. It semiautomatically generates elements of requirements models, including actors, rights, and obligations. We present the complexities of annotating prescriptive text, the architecture of GaiusT, and the process by which annotation is accomplished. We also present experimental results from two case studies to illustrate the application of the tool and its effectiveness relative to manual efforts. The first case study is based on the US Health Insurance Portability and Accountability Act, while the second analyzes the Italian accessibility law for information technology instruments.",
issn="1432-010X",
doi="10.1007/s00766-013-0181-8",
url="http://dx.doi.org/10.1007/s00766-013-0181-8"
"


@Article"Jirapanthong2009,
author="Jirapanthong, Waraporn
and Zisman, Andrea",
title="XTraQue: traceability for product line systems",
journal="Software "\&" Systems Modeling",
year="2009",
volume="8",
number="1",
pages="117--144",
abstract="Product line engineering has been increasingly used to support the development and deployment of software systems that share a common set of features and are developed based on the reuse of core assets. The large number and heterogeneity of documents generated during the development of product line systems may cause difficulties to identify common and variable aspects among applications, and to reuse core assets that are available under the product line. In this paper, we present a traceability approach for product line systems. Traceability has been recognised as an important task in software system development. Traceability relations can improve the quality of the product being developed and reduce development time and cost. We present a rule-based approach to support automatic generation of traceability relations between feature-based object-oriented documents. We define a traceability reference model with nine different types of traceability relations for eight types of documents. The traceability rules used in our work are classified into two groups namely (a) direct rules, which support the creation of traceability relations that do not depend on the existence of other relations, and (b) indirect rules, which require the existence of previously generated relations. The documents are represented in XML and the rules are represented in an extension of XQuery. A prototype tool called XTraQue has been implemented. This tool, together with a mobile phone product line case study, has been used to demonstrate and evaluate our work in various experiments. The results of these experiments are encouraging and comparable with other approaches that support automatic generation of traceability relations.",
issn="1619-1374",
doi="10.1007/s10270-007-0066-8",
url="http://dx.doi.org/10.1007/s10270-007-0066-8"
"


@Article"Horkoff2013,
author="Horkoff, Jennifer
and Yu, Eric",
title="Comparison and evaluation of goal-oriented satisfaction analysis techniques",
journal="Requirements Engineering",
year="2013",
volume="18",
number="3",
pages="199--222",
abstract="Goal-oriented requirements engineering (GORE) has been introduced as a means of modeling and understanding the motivations for system requirements. Using models to make goals explicit helps to avoid system failures due to implementing the wrong requirements or ignoring certain stakeholder needs. These models are unique when compared to other models used in system analysis in that their structure naturally lends itself to an analysis of goal satisfaction. Existing work claims that analysis using goal models can facilitate decision making over functional or design alternatives, using criteria in the model. Many different approaches to the analysis of goal-oriented requirements models have been proposed, including several procedures that analyze the satisfaction or denial of goals. These procedures make different choices in their interpretation of the goal model syntax, the methods to resolve conflicting or partial evidence, and in the way they represent satisfaction. This work uses three available tools implementing seven similar goal satisfaction analysis procedures to analyze three sample goal models. Results are reported and compared. The purpose of this comparison is to understand the ways in which procedural design choices affect analysis results, and how differences in analysis results could lead to different recommendations over alternatives in the model. Our comparison shows that different satisfaction analysis techniques for goal models can produce variable results, depending on the structure of the model. Comparison findings lead us to recommend the use of satisfaction analysis techniques for goal models as only heuristics for decision making. Our results emphasize investigation into the benefits of satisfaction analysis beyond decision making, namely improving model quality, increasing domain knowledge, and facilitating communication.",
issn="1432-010X",
doi="10.1007/s00766-011-0143-y",
url="http://dx.doi.org/10.1007/s00766-011-0143-y"
"


@Article"Heymans2012,
author="Heymans, Patrick
and Boucher, Quentin
and Classen, Andreas
and Bourdoux, Arnaud
and Demonceau, Laurent",
title="A code tagging approach to software product line development",
journal="International Journal on Software Tools for Technology Transfer",
year="2012",
volume="14",
number="5",
pages="553--566",
abstract="Software product line engineering seeks to systematise reuse when developing families of similar software systems so as to minimise development time, cost and defects. To realise variability at the code level, product line methods classically advocate usage of inheritance, components, frameworks, aspects or generative techniques. However, these might require unaffordable paradigm shifts for developers if the software was not thought at the outset as a product line. Furthermore, these techniques can be conflicting with a company's coding practices or external regulations. These concerns were the motivation for the industry--university collaboration described in this paper in which we developed a minimally intrusive coding technique based on tags. The approach was complemented with traceability from code to feature diagrams which were exploited for automated configuration. It is supported by a toolchain and is now in use in the partner company for the development of flight-grade satellite communication software libraries.",
issn="1433-2787",
doi="10.1007/s10009-012-0242-1",
url="http://dx.doi.org/10.1007/s10009-012-0242-1"
"


@Article"Mahmoud2014,
author="Mahmoud, Anas
and Niu, Nan",
title="Supporting requirements to code traceability through refactoring",
journal="Requirements Engineering",
year="2014",
volume="19",
number="3",
pages="309--329",
abstract="In this paper, we hypothesize that the distorted traceability tracks of a software system can be systematically re-established through refactoring, a set of behavior-preserving transformations for keeping the system quality under control during evolution. To test our hypothesis, we conduct an experimental analysis using three requirements-to-code datasets from various application domains. Our objective is to assess the impact of various refactoring methods on the performance of automated tracing tools based on information retrieval. Results show that renaming inconsistently named code identifiers, using Rename Identifier refactoring, often leads to improvements in traceability. In contrast, removing code clones, using eXtract Method (XM) refactoring, is found to be detrimental. In addition, results show that moving misplaced code fragments, using Move Method refactoring, has no significant impact on trace link retrieval. We further evaluate Rename Identifier refactoring by comparing its performance with other strategies often used to overcome the vocabulary mismatch problem in software artifacts. In addition, we propose and evaluate various techniques to mitigate the negative impact of XM refactoring. An effective traceability sign analysis is also conducted to quantify the effect of these refactoring methods on the vocabulary structure of software systems.",
issn="1432-010X",
doi="10.1007/s00766-013-0197-0",
url="http://dx.doi.org/10.1007/s00766-013-0197-0"
"


@Article"Alenljung2008,
author="Alenljung, Beatrice
and Persson, Anne",
title="Portraying the practice of decision-making in requirements engineering: a case of large scale bespoke development",
journal="Requirements Engineering",
year="2008",
volume="13",
number="4",
pages="257--279",
abstract="Complex decision-making is a prominent aspect of requirements engineering (RE) and the need for improved decision support for RE decision-makers has been identified by a number of authors in the research literature. A first step toward better decision support in requirements engineering is to understand multifaceted decision situations of decision-makers. In this paper, the focus is on RE decision-making in large scale bespoke development. The decision situation of RE decision-makers on a subsystem level has been studied at a systems engineering company and is depicted in this paper. These situations are described in terms of, e.g., RE decision matters, RE decision-making activities, and RE decision processes. Factors that affect RE decision-makers are also identified.",
issn="1432-010X",
doi="10.1007/s00766-008-0068-2",
url="http://dx.doi.org/10.1007/s00766-008-0068-2"
"


@Article"Kogalovsky2009,
author="Kogalovsky, M. R.
and Kalinichenko, L. A.",
title="Conceptual and ontological modeling in information systems",
journal="Programming and Computer Software",
year="2009",
volume="35",
number="5",
pages="241--256",
abstract="Conceptual modeling of a subject domain, which produces its conceptual model, is an important stage in designing information systems. In recent years, much attention in the development of such systems has been given to reusing information resources and to providing access to them at the semantic level. Methods and technologies of ontological modeling have lately been under intensive development. In this paper, problems and preconditions of conceptual modeling of the subject domain in database technologies and information systems are discussed. Various approaches to conceptual modeling, conceptual modeling languages, and the respective tools are considered, various interpretations of the role of the conceptual model of the subject domain are discussed, and the current state of conceptual modeling tools produced by software industry is assessed. The relationships between the conceptual schemas of the subject domain and ontologies are analyzed and their similarities and differences are described. Terminological issues and the directions of research in the field of conceptual and ontological modeling are considered. An extensive list of references is given.",
issn="1608-3261",
doi="10.1134/S0361768809050016",
url="http://dx.doi.org/10.1134/S0361768809050016"
"


@Article"Kogalovsky2013,
author="Kogalovsky, M. R.",
title="Metadata in computer systems",
journal="Programming and Computer Software",
year="2013",
volume="39",
number="4",
pages="182--193",
abstract="An important role in modern computer systems is played by a special kind of information resources known as metadata. Numerous publications are available on metadata. Most of them deal with the standards of metadata used in various areas, while publications discussing the properties and functions of this kind of information resources are rather rare. Probably, for this reason, there is no sustained treatment of the term metadata in the literature. Rather frequently, metadata-concerning publications involve obvious errors or no necessary comments are made in discussions of special kinds of metadata. As a result, their properties and functions are incorrectly extended to the general case. This paper offers a systematic view of metadata as information resources of special sort. The definition of this term is discussed, and examples of metadata used in various areas of information technologies are given. The general (domain-independent) properties and functions of metadata are examined. Facilities for metadata representation, available generalized classifications of metadata, and activities related to metadata standardization are discussed.",
issn="1608-3261",
doi="10.1134/S0361768813040038",
url="http://dx.doi.org/10.1134/S0361768813040038"
"


@Article"M�hlberg2014,
author="M"\"u"hlberg, Jan Tobias
and L"\"u"ttgen, Gerald",
title="Symbolic object code analysis",
journal="International Journal on Software Tools for Technology Transfer",
year="2014",
volume="16",
number="1",
pages="81--102",
abstract="Software model checkers quickly reach their limits when being applied to verifying pointer safety properties in source code that includes function pointers and inlined assembly. This article introduces a novel technique for checking pointer safety violations, called symbolic object code analysis (SOCA), which is based on bounded symbolic execution, incorporates path-sensitive slicing, and employs the SMT solver Yices as its execution and verification engine. Extensive experimental results of a prototypic SOCA Verifier, using the Verisec suite and almost 10,000 Linux device driver functions as benchmarks, show that SOCA performs competitively to modern source-code model checkers, scales well when applied to real operating systems code and pointer safety issues, and effectively explores niches of pointer-complex software that current software verifiers do not reach.",
issn="1433-2787",
doi="10.1007/s10009-012-0256-8",
url="http://dx.doi.org/10.1007/s10009-012-0256-8"
"


@Article"Walraven2014,
author="Walraven, Stefan
and Truyen, Eddy
and Joosen, Wouter",
title="Comparing PaaS offerings in light of SaaS development",
journal="Computing",
year="2014",
volume="96",
number="8",
pages="669--724",
abstract="Software vendors increasingly aim to apply the Software-as-a-Service (SaaS) delivery model instead of the traditional on-premise model. Platforms-as-a-Service (PaaS), such as Google App Engine and Windows Azure, deliver a computing platform and solution stack as a service, but they also aim to facilitate the development of cloud applications (SaaS). Such PaaS offerings should enable third parties to build and deliver multi-tenant SaaS applications while shielding the complexity of the underpinning middleware and infrastructure. This paper compares, on the basis of a practical case study, three different and representative PaaS platforms with respect to their support for SaaS application development. We have reengineered an on-premise enterprise application into a SaaS application and we have subsequently deployed it in three PaaS-based cloud environments. We have investigated the following qualities of the PaaS platforms from the perspective of SaaS development: portability of the application code base, available support for creating and managing multi-tenant-aware applications, and quality of the tool support.",
issn="1436-5057",
doi="10.1007/s00607-013-0346-9",
url="http://dx.doi.org/10.1007/s00607-013-0346-9"
"


@Article"Ricca2009,
author="Ricca, Filippo
and Chao, Liu",
title="Special section on Web Systems Evolution",
journal="International Journal on Software Tools for Technology Transfer",
year="2009",
volume="11",
number="6",
pages="419",
abstract="Evolution of Web applications is high for several reasons. Among the others, the need of being constantly updated with the emerging technologies is maybe the most important. Web Systems Evolution is a multifaceted and broad field studying techniques, approaches and tools able to restructure, re-engineer and in general modify a Web application with the intention of renovating or improving some quality aspects of it. This special section is devoted to a selection of papers that have been originally published in the proceedings of the International Symposium on Web Site Evolution, held in Beijing, China in October 2008. The selected papers investigate different issues concerning the evolution of Web Systems, ranging from migration towards SOA to more classic re-engineering and maintenance tasks (e.g., improving the navigational structure). The main contribution of this special section consist of translating some interesting research ideas about Web systems evolution into solutions and tools able to transfer knowledge to the industry.",
issn="1433-2787",
doi="10.1007/s10009-009-0127-0",
url="http://dx.doi.org/10.1007/s10009-009-0127-0"
"


@Article"Zambon2011,
author="Zambon, Emmanuele
and Etalle, Sandro
and Wieringa, Roel J.
and Hartel, Pieter",
title="Model-based qualitative risk assessment for availability of IT infrastructures",
journal="Software "\&" Systems Modeling ",
year="2011",
volume="10",
number="4",
pages="553--580",
abstract="For today's organisations, having a reliable information system is crucial to safeguard enterprise revenues (think of on-line banking, reservations for e-tickets etc.). Such a system must often offer high guarantees in terms of its availability; in other words, to guarantee business continuity, IT systems can afford very little downtime. Unfortunately, making an assessment of IT availability risks is difficult: incidents affecting the availability of a marginal component of the system may propagate in unexpected ways to other more essential components that functionally depend on them. General-purpose risk assessment (RA) methods do not provide technical solutions to deal with this problem. In this paper we present the qualitative time dependency (QualTD) model and technique, which is meant to be employed together with standard RA methods for the qualitative assessment of availability risks based on the propagation of availability incidents in an IT architecture. The QualTD model is based on our previous quantitative time dependency (TD) model (Zambon et al. in BDIM '07: Second IEEE/IFIP international workshop on business-driven IT management. IEEE Computer Society Press, pp 75--83, 2007), but provides more flexible modelling capabilities for the target of assessment. Furthermore, the previous model required quantitative data which is often too costly to acquire, whereas QualTD applies only qualitative scales, making it more applicable to industrial practice. We validate our model and technique in a real-world case by performing a risk assessment on the authentication and authorisation system of a large multinational company and by evaluating the results with respect to the goals of the stakeholders of the system. We also perform a review of the most popular standard RA methods and discuss which type of method can be combined with our technique.",
issn="1619-1374",
doi="10.1007/s10270-010-0166-8",
url="http://dx.doi.org/10.1007/s10270-010-0166-8"
"


@Article"Garousi2009,
author="Garousi, Vahid
and Briand, Lionel C.
and Labiche, Yvan",
title="A UML-based quantitative framework for early prediction of resource usage and load in distributed real-time systems",
journal="Software "\&" Systems Modeling",
year="2009",
volume="8",
number="2",
pages="275--302",
abstract="This paper presents a quantitative framework for early prediction of resource usage and load in distributed real-time systems (DRTS). The prediction is based on an analysis of UML 2.0 sequence diagrams, augmented with timing information, to extract timed-control flow information. It is aimed at improving the early predictability of a DRTS by offering a systematic approach to predict, at the design phase, system behavior in each time instant during its execution. Since behavioral models such as sequence diagrams are available in early design phases of the software life cycle, the framework enables resource analysis at a stage when design decisions are still easy to change. Though we provide a general framework, we use network traffic as an example resource type to illustrate how the approach is applied. We also indicate how usage and load analysis of other types of resources (e.g., CPU and memory) can be performed in a similar fashion. A case study illustrates the feasibility of the approach.",
issn="1619-1374",
doi="10.1007/s10270-008-0099-7",
url="http://dx.doi.org/10.1007/s10270-008-0099-7"
"


@Article"Agouridas2008,
author="Agouridas, Vassilis
and McKay, Alison
and Winand, Henri
and de Pennington, Alan",
title="Advanced product planning: a comprehensive process for systemic definition of new product requirements",
journal="Requirements Engineering",
year="2008",
volume="13",
number="1",
pages="19--48",
abstract="This paper reports results of research into the definition of requirements for new consumer products----specifically, electro-mechanical products. The research dealt with the derivation of design requirements that are demonstrably aligned with stakeholder needs. The paper describes a comprehensive process that can enable product development teams to deal with statements of product requirements, as originally collected through market research activities, in a systematic and traceable manner from the early, fuzzy front end, stages of the design process. The process described has been based on principles of systems engineering. A case study from its application and evaluation drawn from the power sector is described in this paper. The case study demonstrates how the process can significantly improve product quality planning practices through revision of captured product requirements, analysis of stakeholder requirements and derivation of design requirements. The paper discusses benefits and issues from the use of the process by product development teams, and identifies areas for further research. Finally, the conclusions drawn from the reported research are presented.",
issn="1432-010X",
doi="10.1007/s00766-007-0055-z",
url="http://dx.doi.org/10.1007/s00766-007-0055-z"
"


@Article"Peng2009,
author="Peng, Xin
and Lee, Seok-Won
and Zhao, Wen-Yun",
title="Feature-Oriented Nonfunctional Requirement Analysis for Software Product Line",
journal="Journal of Computer Science and Technology",
year="2009",
volume="24",
number="2",
pages="319--338",
abstract="Domain analysis in software product line (SPL) development provides a basis for core assets design and implementation by a systematic and comprehensive commonality/variability analysis. In feature-oriented SPL methods, products of the domain analysis are domain feature models and corresponding feature decision models to facilitate application-oriented customization. As in requirement analysis for a single system, the domain analysis in the SPL development should consider both functional and nonfunctional domain requirements. However, the nonfunctional requirements (NFRs) are often neglected in the existing domain analysis methods. In this paper, we propose a context-based method of the NFR analysis for the SPL development. In the method, NFRs are materialized by connecting nonfunctional goals with real-world context, thus NFR elicitation and variability analysis can be performed by context analysis for the whole domain with the assistance of NFR templates and NFR graphs. After the variability analysis, our method integrates both functional and nonfunctional perspectives by incorporating the nonfunctional goals and operationalizations into an initial functional feature model. NFR-related constraints are also elicited and integrated. Finally, a decision model with both functional and nonfunctional perspectives is constructed to facilitate application-oriented feature model customization. A computer-aided grading system (CAGS) product line is employed to demonstrate the method throughout the paper.",
issn="1860-4749",
doi="10.1007/s11390-009-9227-2",
url="http://dx.doi.org/10.1007/s11390-009-9227-2"
"


@Article"Moody2010,
author="Moody, Daniel L.
and Heymans, Patrick
and Matulevi"\v"c""ius, Raimundas",
title="Visual syntax does matter: improving the cognitive effectiveness of the i* visual notation",
journal="Requirements Engineering",
year="2010",
volume="15",
number="2",
pages="141--175",
abstract="Goal-oriented modelling is one of the most important research developments in the requirements engineering (RE) field. This paper conducts a systematic analysis of the visual syntax of i*, one of the leading goal-oriented languages. Like most RE notations, i* is highly visual. Yet surprisingly, there has been little debate about or modification to its graphical conventions since it was proposed more than a decade ago. We evaluate the i* visual notation using a set of principles for designing cognitively effective visual notations (the Physics of Notations). The analysis reveals some serious flaws in the notation together with some practical recommendations for improvement. The results can be used to improve its effectiveness in practice, particularly for communicating with end users. A broader goal of the paper is to raise awareness about the importance of visual representation in RE research, which has historically received little attention.",
issn="1432-010X",
doi="10.1007/s00766-010-0100-1",
url="http://dx.doi.org/10.1007/s00766-010-0100-1"
"


@Article"Svahnberg2013,
author="Svahnberg, Mikael
and Gorschek, Tony
and Nguyen, Thi Than Loan
and Nguyen, Mai",
title="Uni-REPM: validated and improved",
journal="Requirements Engineering",
year="2013",
volume="18",
number="1",
pages="85--103",
abstract="Software products are usually developed for either a specific customer (bespoke) or a broader market (market-driven). Due to their characteristic, bespoke and market-driven development face different challenges, especially concerning requirements engineering. Many challenges are caused by an inadequate requirements engineering process, and hence there is a need for process improvement frameworks based on empirical research and industry needs. In a previous article we introduced Uni-REPM, a lightweight requirements engineering process assessment framework based on a review of empirically motivated practices in market-driven and bespoke requirements engineering literature. In this article, we validate this framework in academia as well as industry, in order to prepare Uni-REPM for widespread industry use. We conduct two validations; a static validation based on interviews with seven academic experts and a dynamic validation where Uni-REPM is applied in four industrial organisations. Uni-REPM is refined according to the feedback obtained in the validations. The study shows that Uni-REPM is a quick, simple, and cost-effective solution to assess the maturity level of the requirements engineering process of projects. Moreover, the assessment method using checklists is highly usable and applicable in various international development environments.",
issn="1432-010X",
doi="10.1007/s00766-012-0148-1",
url="http://dx.doi.org/10.1007/s00766-012-0148-1"
"


@Article"Stevens2013,
author="Stevens, Perdita",
title="A simple game-theoretic approach to checkonly QVT Relations",
journal="Software "\&" Systems Modeling",
year="2013",
volume="12",
number="1",
pages="175--199",
abstract="The QVT Relations (QVT-R) transformation language allows the definition of bidirectional model transformations, which are required in cases where two (or more) models must be kept consistent in the face of changes to either or both. A QVT-R transformation can be used either in checkonly mode, to determine whether a target model is consistent with a given source model, or in enforce mode, to change the target model. A precise understanding of checkonly mode transformations is prerequisite to a precise understanding of enforce mode transformations, and this is the focus of this paper. In order to give semantics to checkonly QVT-R transformations, we need to consider the overall structure of the transformation as given by when and where clauses, and the role of trace classes. In the standard, the semantics of QVT-R are given both directly, and by means of a translation to QVT Core, a language which is intended to be simpler. In this paper, we argue that there are irreconcilable differences between the intended semantics of QVT-R and those of QVT Core, so that no translation from QVT-R to QVT Core can be semantics-preserving, and hence no such translation can be helpful in defining the semantics of QVT-R. Treating QVT-R directly, we propose a simple game-theoretic semantics. We demonstrate its behaviour on examples and show how it can be used to prove an example result comparing two QVT-R transformations. We demonstrate that consistent models may not possess a single trace model whose objects can be read as traceability links in either direction. We briefly discuss the effect of variations in the rules of the game, to elucidate some design choices available to the designers of the QVT-R language.",
issn="1619-1374",
doi="10.1007/s10270-011-0198-8",
url="http://dx.doi.org/10.1007/s10270-011-0198-8"
"


@Article"Rubin2015,
author="Rubin, Julia
and Czarnecki, Krzysztof
and Chechik, Marsha",
title="Cloned product variants: from ad-hoc to managed software product lines",
journal="International Journal on Software Tools for Technology Transfer",
year="2015",
volume="17",
number="5",
pages="627--646",
abstract="We focus on the problem of managing a collection of related software product variants realized via cloning. By analyzing three industrial case studies of organizations with cloned product lines, we conclude that an efficient management of clones relies on both refactoring cloned variants into a single-copy product line representation and improving development experience when maintaining existing clones. We propose a framework that consists of seven conceptual operators for cloned product line management and show that these operators are adequate to realize development activities we observed in the analyzed case studies. We discuss options for implementing the operators and benefits of the operator-based view.",
issn="1433-2787",
doi="10.1007/s10009-014-0347-9",
url="http://dx.doi.org/10.1007/s10009-014-0347-9"
"


@Article"Barnes2014,
author="Barnes, Jeffrey M.
and Garlan, David
and Schmerl, Bradley",
title="Evolution styles: foundations and models for software architecture evolution",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="2",
pages="649--678",
abstract="As new market opportunities, technologies, platforms, and frameworks become available, systems�require large-scale and systematic architectural restructuring to accommodate them. Today's architects have few techniques to help them plan this architecture evolution. In particular, they have little assistance in planning alternative evolution paths, trading off various aspects of the different paths, or knowing best practices for particular domains. In this paper, we describe an approach for planning and reasoning about architecture evolution. Our approach focuses on providing architects with the means to model prospective evolution paths and supporting analysis to select among these candidate paths. To demonstrate the usefulness of our approach, we show how it can be applied to an actual architecture evolution. In addition, we present some theoretical results about our evolution path constraint specification language.",
issn="1619-1374",
doi="10.1007/s10270-012-0301-9",
url="http://dx.doi.org/10.1007/s10270-012-0301-9"
"


@Article"Buyens2013,
author="Buyens, Koen
and Scandariato, Riccardo
and Joosen, Wouter",
title="Least privilege analysis in software architectures",
journal="Software "\&" Systems Modeling",
year="2013",
volume="12",
number="2",
pages="331--348",
abstract="Due to the lack of both precise definitions and effective software engineering methodologies, security design principles are often neglected by software architects, resulting in potentially high-risk threats to systems. This work lays the formal foundations for understanding the security design principle of least privilege in software architectures and provides a technique to identify violations against this principle. The technique can also be leveraged to analyze violations against the security design principle of separation of duties. The proposed approach is supported by tools and has been validated in four case studies, two of which are presented in detail in this paper.",
issn="1619-1374",
doi="10.1007/s10270-011-0218-8",
url="http://dx.doi.org/10.1007/s10270-011-0218-8"
"


@Article"Pleuss2012,
author="Pleuss, Andreas
and Botterweck, Goetz",
title="Visualization of variability and configuration options",
journal="International Journal on Software Tools for Technology Transfer",
year="2012",
volume="14",
number="5",
pages="497--510",
abstract="When designing, constructing, and maintaining diverse and variable software systems, a key challenge is the complexity of systems. A potential approach to tackle this challenge are techniques from variability management and product line engineering to handle the diversity and variability. A key asset in variability management is a variability model, which explicitly specifies the commonalities and variability of a system and the constraints between variants. However, handling variability and configurations remains a challenge due to the complexity on a cognitive level as human engineers reach their limits in identifying, understanding, and using all relevant details. In this paper we address this issue by providing concepts for interactive visual tool support for the configuration of systems with the help of feature models. We discuss relevant principles from the area of information visualization and their application to the domain of feature model configuration. We discuss techniques for interactive configuration support based on a reasoning engine, which, e.g., ensures the validity of configurations. We illustrate our findings by a concrete tool solution called S2T2 Configurator.",
issn="1433-2787",
doi="10.1007/s10009-012-0252-z",
url="http://dx.doi.org/10.1007/s10009-012-0252-z"
"


@Article"Rivero2013,
author="Rivero, Luis
and Conte, Tayana",
title="Using an empirical study to evaluate the feasibility of a new usability inspection technique for paper based prototypes of web applications",
journal="Journal of Software Engineering Research and Development",
year="2013",
volume="1",
number="1",
pages="2",
abstract="Usability is one of the most important factors that determine the quality of Web applications, which can be verified performing usability inspection. This paper presents the Web Design Usability Evaluation (Web DUE) technique, which allows the identification of usability problems in low-fidelity prototypes (or mockups) of Web applications during the design phases of the development. We have also proposed the Mockup Design Usability Evaluation (Mockup DUE) tool which is able to assist inspectors using the Web DUE technique.",
issn="2195-1721",
doi="10.1186/2195-1721-1-2",
url="http://dx.doi.org/10.1186/2195-1721-1-2"
"


@Article"Garousi2008,
author="Garousi, Vahid",
title="Incorporating message weights in UML-based analysis of behavioral dependencies in distributed systems",
journal="Software "\&" Systems Modeling",
year="2008",
volume="9",
number="1",
pages="113",
abstract="Behavioral dependency analysis (BDA) and the visualization of dependency information have been identified as a high priority in industrial software systems (in specific, distributed systems). BDA determines the extent to which the functionality of one system entity (e.g., an object or a node) depends on other entities. Among many uses, a BDA is used to perform risk analysis and assessment, load planning, fault tolerance and redundancy provisions in distributed systems. Traditionally, most BDA techniques are based on source code or execution traces of a system. However, as model-driven development is gaining more popularity, there is a need for model-based BDA techniques. To address this need, we proposed in a previous work a metric, referred to as dependency index (DI), for the BDA of distributed objects and nodes based on UML behavioral models (sequence diagrams). However, in our previous BDA work, for simplicity, it was assumed that all messages are equivalent in terms of the dependencies they entail. However, to perform a more realistic BDA on real-world systems, messages must be weighted, e.g., certain messages may be more critical (or important) than others, and thus entail more intensive dependency. To address the above need, we define in this article a family of new BDA metrics, as extensions to our basic DI metric, based on different weighting mechanisms. Through an example application of the proposed metrics, we show that they can be used to predict more realistic dependency information. Furthermore, we derive interesting observations from our dependency analysis that would influence, in practice, practical decisions, which could not have been easily derived without it, e.g., we come up with a suggestion to install more reliable data-transmission network links between two nodes to ensure a reliable communication on links with intensive dependencies.",
issn="1619-1374",
doi="10.1007/s10270-008-0111-2",
url="http://dx.doi.org/10.1007/s10270-008-0111-2"
"


@Article"Liu2010,
author="Liu, Shaoying",
title="An approach to applying SOFL for agile process and its application in developing a test support tool",
journal="Innovations in Systems and Software Engineering",
year="2010",
volume="6",
number="1",
pages="137--143",
abstract="Structured Object-Oriented Formal Language (SOFL) is a representative formal engineering method for software development. It offers a three-step specification approach to constructing formal specifications, and specification-based inspection and testing for verification and validation. In this paper, we describe a novel approach to applying the SOFL method to achieve agile development process. This approach results from our experience in several collaboration projects with industry, and aims to strike a balance between the fast delivery of software product and the assurance of its quality. We have tested the approach in developing a prototype test support tool.",
issn="1614-5054",
doi="10.1007/s11334-009-0114-3",
url="http://dx.doi.org/10.1007/s11334-009-0114-3"
"


@Article"Felderer2015,
author="Felderer, Michael
and Katt, Basel",
title="A process for mastering security evolution in the development lifecycle",
journal="International Journal on Software Tools for Technology Transfer",
year="2015",
volume="17",
number="3",
pages="245--250",
abstract="Continuous system evolution makes it challenging to keep software systems permanently secure as changes either in the system itself or its environment may cause new threats and vulnerabilities. Therefore, suitable activities aligned with the software development process are required to master security evolution. This introduction to the special section on eternal security evolution presents a process for handling security evolution throughout the software development lifecycle and uses this process to position the individual contributions. We first present the underlying security development process comprising the phases initialization, security analysis, security design, security implementation, security testing, and security deployment. On this basis, we define the security evolution process comprising the activities security requirements review, adaptation of design models, code fixing and patch development, regression testing as well as re-deployment. Finally, the defined security evolution activities are discussed in context of the four articles on eternal security evolution presented in this special section of the International Journal on Software Tools for Technology Transfer.",
issn="1433-2787",
doi="10.1007/s10009-015-0371-4",
url="http://dx.doi.org/10.1007/s10009-015-0371-4"
"


@Article"Almeida2010,
author="Almeida, Jos"\'e" Bacelar
and Barbosa, Manuel
and Pinto, Jorge Sousa
and Vieira, B"\'a"rbara",
title="Deductive verification of cryptographic software",
journal="Innovations in Systems and Software Engineering",
year="2010",
volume="6",
number="3",
pages="203--218",
abstract="We apply state-of-the art deductive verification tools to check security-relevant properties of cryptographic software, including safety, absence of error propagation, and correctness with respect to reference implementations. We also develop techniques to help us in our task, focusing on methods oriented towards increased levels of automation, in scenarios where there are clear obvious limits to such automation. These techniques allow us to integrate automatic proof tools with an interactive proof assistant, where the latter is used off-line to prove once-and-for-all fundamental lemmas about properties of programs. The techniques developed have independent interest for practical deductive verification in general.",
issn="1614-5054",
doi="10.1007/s11334-010-0127-y",
url="http://dx.doi.org/10.1007/s11334-010-0127-y"
"


@Article"Odeh2010,
author="Odeh, Salaheddin",
title="Building Reusable Remote Labs with Adaptable Client User-Interfaces",
journal="Journal of Computer Science and Technology",
year="2010",
volume="25",
number="5",
pages="999--1015",
abstract="Nowadays remote laboratories suffer the absence of reusability. In addition, their construction and maintenance require time, money and skills. The system implementation of a specific remote lab is neither generic nor reusable. In this paper, a solution for a reusable remote lab dedicated for disparate types of scientific and engineering experiments is presented. The experiment designer needs only to connect the experiment components and equipment such as capacitors, resistors, transistors, function generators with a switch system of a lab server, then, she/he has to map this connection structure in a configuration data structure. Once a student starts the Web-based client user-interface and logs-in into the lab server, the menu structure of the graphical user-interface builds and initializes itself automatically, using information stored in a configuration data structure. This contribution discusses some hitherto used lab servers, some of their drawbacks, the desirable requirements on a universal remote lab, which simplify the building process of newer lab experiments consisting of experiment components and equipment as well as a client user-interface that could enable students to remotely access the experiment.",
issn="1860-4749",
doi="10.1007/s11390-010-9383-4",
url="http://dx.doi.org/10.1007/s11390-010-9383-4"
"


@Article"Taentzer2014,
author="Taentzer, Gabriele
and Ermel, Claudia
and Langer, Philip
and Wimmer, Manuel",
title="A fundamental approach to model versioning based on graph modifications: from theory to implementation",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="1",
pages="239--272",
abstract="In model-driven engineering, models are primary artifacts that can evolve heavily during their life cycle. Therefore, versioning of models is a key technique to be offered by integrated development environments for model-driven engineering. In contrast to text-based versioning systems, we present an approach that takes model structures and their changes over time into account. Considering model structures as graphs, we define a fundamental approach where model revisions are considered as graph modifications consisting of delete and insert actions. Two different kinds of conflict detection are presented: (1) the check for operation-based conflicts between different graph modifications, and (2) the check for state-based conflicts on merged graph modifications. For the merging of graph modifications, a two-phase approach is proposed: First, operational conflicts are temporarily resolved by always giving insertion priority over deletion to keep as much information as possible. Thereafter, this tentative merge result is the basis for manual conflict resolution as well as for the application of repair actions that resolve state-based conflicts. If preferred by the user, giving deletion priority over insertion might be one solution. The fundamental concepts are illustrated by versioning scenarios for simplified statecharts. Furthermore, we show an implementation of this fundamental approach to model versioning based on the Eclipse Modeling Framework as technical space.",
issn="1619-1374",
doi="10.1007/s10270-012-0248-x",
url="http://dx.doi.org/10.1007/s10270-012-0248-x"
"


@Article"Abrial2010,
author="Abrial, Jean-Raymond
and Butler, Michael
and Hallerstede, Stefan
and Hoang, Thai Son
and Mehta, Farhad
and Voisin, Laurent",
title="Rodin: an open toolset for modelling and reasoning in Event-B",
journal="International Journal on Software Tools for Technology Transfer",
year="2010",
volume="12",
number="6",
pages="447--466",
abstract="Event-B is a formal method for system-level modelling and analysis. Key features of Event-B are the use of set theory as a modelling notation, the use of refinement to represent systems at different abstraction levels and the use of mathematical proof to verify consistency between refinement levels. In this article we present the Rodin modelling tool that seamlessly integrates modelling and proving. We outline how the Event-B language was designed to facilitate proof and how the tool has been designed to support changes to models while minimising the impact of changes on existing proofs. We outline the important features of the prover architecture and explain how well-definedness is treated. The tool is extensible and configurable so that it can be adapted more easily to different application domains and development methods.",
issn="1433-2787",
doi="10.1007/s10009-010-0145-y",
url="http://dx.doi.org/10.1007/s10009-010-0145-y"
"


@Article"Ali2015,
author="Ali, Naveed
and Lai, Richard",
title="A method of software requirements specification and validation for global software development",
journal="Requirements Engineering",
year="2015",
pages="1--24",
abstract="Global software development (GSD), where software teams are located in different parts of the world, has become increasingly popular. To devise a high-quality software requirements specification (SRS), effective communication and collaboration between stakeholders are necessary for GSD. However, geographical distance, cultural diversity, differences in time zones and language barriers create difficulties for stakeholders in engaging in effective collaboration. Taking into consideration the factors involved in GSD, previous research showed that the ways by which requirements are documented and validated for collocated software development projects cannot be used effectively for GSD. In this paper, we present a method of GSD requirements specification and validation. Our method begins with generating a requirements graph to understand details of the software requirements with respect to different GSD sites. The information obtained from a requirements graph is to be contained in a requirements specification document, and then be circulated between different GSD sites for reviewing, updating and finalizing its content. Finally, the requirements contained in the specification document are to be validated by generating and comparing validation matrices at different GSD sites. Past researchers used student groups in a university environment to play the roles of stakeholders in experiments in GSD studies. We therefore validate our method by applying it to a case study of an online shopping system, where the roles of stakeholders were played by a group of students.",
issn="1432-010X",
doi="10.1007/s00766-015-0240-4",
url="http://dx.doi.org/10.1007/s00766-015-0240-4"
"


@Article"Petre2014,
author="Petre, Marian",
title="``No shit'' or ``Oh, shit!'': responses to observations on the use of UML in professional practice",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="4",
pages="1225--1235",
abstract="This paper follows a paper, ``UML in Practice'' presented at ICSE 2013. It summarizes and reflects on the discussion and additional investigation that arose from ``UML in Practice.'' The paper provides a condensed recap of ``UML in Practice'' findings, explains what data were collected from which sources to inform this paper, and describes how the data were analyzed. It reports on the discussion that has arisen, summarizing responses from industry practitioners, academics teaching software engineering, and the UML community, and considers how those responses reflect on the original observations. The responses to ``UML in Practice'' divide (crudely) between two perspectives: (1) the observations made are familiar and unsurprizing, and match personal experience (``No shit''); or (2) the observations threaten long-held beliefs about UML use, and in particular about the status of UML as the de facto standard of software engineering, implying a need to change personal practice (``Oh, shit!'').",
issn="1619-1374",
doi="10.1007/s10270-014-0430-4",
url="http://dx.doi.org/10.1007/s10270-014-0430-4"
"


@Article"Sachs2013,
author="Sachs, Kai
and Kounev, Samuel
and Buchmann, Alejandro",
title="Performance modeling and analysis of message-oriented event-driven systems",
journal="Software "\&" Systems Modeling",
year="2013",
volume="12",
number="4",
pages="705--729",
abstract="Message-oriented event-driven systems are becoming increasingly ubiquitous in many industry domains including telecommunications, transportation and supply chain management. Applications in these areas typically have stringent requirements for performance and scalability. To guarantee adequate quality-of-service, systems must be subjected to a rigorous performance and scalability analysis before they are put into production. In this paper, we present a comprehensive modeling methodology for message-oriented event-driven systems in the context of a case study of a representative application in the supply chain management domain. The methodology, which is based on queueing Petri nets, provides a basis for performance analysis and capacity planning. We study a deployment of the SPECjms2007 standard benchmark on a leading commercial middleware platform. A detailed system model is built in a step-by-step fashion and then used to predict the system performance under various workload and configuration scenarios. After the case study, we present a set of generic performance modeling patterns that can be used as building blocks when modeling message-oriented event-driven systems. The results demonstrate the effectiveness, practicality and accuracy of the proposed modeling and prediction approach.",
issn="1619-1374",
doi="10.1007/s10270-012-0228-1",
url="http://dx.doi.org/10.1007/s10270-012-0228-1"
"


@Article"Nayak2011,
author="Nayak, Ashalatha
and Samanta, Debasis",
title="Synthesis of test scenarios using UML activity diagrams",
journal="Software "\&" Systems Modeling",
year="2011",
volume="10",
number="1",
pages="63--89",
abstract="Often system developers follow Unified Modeling Language (UML) activity diagrams to depict all possible flows of controls commonly known as scenarios of use cases. Hence, an activity diagram is treated as a useful design artifact to identify all possible scenarios and then check faults in scenarios of a use case. However, identification of all possible scenarios and then testing with activity diagrams is a challenging task because several control flow constructs and their nested combinations make path identification difficult. In this paper, we address this problem and propose an approach to identify all scenarios from activity diagrams and use them to test use cases. The proposed approach is based on the classification of control constructs followed by a transformation approach which takes into account any combination of nested structures and transforms an activity diagram into a model called Intermediate Testable Model (ITM). We use ITM to generate test scenarios. With our approach it is possible to generate more scenarios than the existing work. Further, the proposed approach can be directly carried out using design models without any addition of testability information unlike the existing approaches.",
issn="1619-1374",
doi="10.1007/s10270-009-0133-4",
url="http://dx.doi.org/10.1007/s10270-009-0133-4"
"


@Article"Hubaux2013,
author="Hubaux, Arnaud
and Heymans, Patrick
and Schobbens, Pierre-Yves
and Deridder, Dirk
and Abbasi, Ebrahim Khalil",
title="Supporting multiple perspectives in feature-based configuration",
journal="Software "\&" Systems Modeling",
year="2013",
volume="12",
number="3",
pages="641--663",
abstract="Feature diagrams have become commonplace in software product line engineering as a means to document variability early in the life cycle. Over the years, their application has also been extended to assist stakeholders in the configuration of software products. However, existing feature-based configuration techniques offer little support for tailoring configuration views to the profiles of the various stakeholders. In this paper, we propose a lightweight, yet formal and flexible, mechanism to leverage multidimensional separation of concerns in feature-based configuration. We propose a technique to specify concerns in feature diagrams and to generate automatically concern-specific configuration views. Three alternative visualisations are proposed. Our contributions are motivated and illustrated through excerpts from a real web-based meeting management application which was also used for a preliminary evaluation. We also report on the progress made in the development of a tool supporting multi-view feature-based configuration.",
issn="1619-1374",
doi="10.1007/s10270-011-0220-1",
url="http://dx.doi.org/10.1007/s10270-011-0220-1"
"


@Article"Lang2007,
author="Lang, Michael
and Fitzgerald, Brian",
title="Web-based systems design: a study of contemporary practices and an explanatory framework based on ``method-in-action''",
journal="Requirements Engineering",
year="2007",
volume="12",
number="4",
pages="203--220",
abstract="This paper reports the findings of a detailed study of Web-based systems design (WBSD) practices in Ireland based on data collected over a 3-year period (2002--2005), the objectives of which were to (1) contribute towards a richer understanding of the current ``real-world'' context of WBSD by characterising the profile of a typical project (team size, timeframe, nature of requirements, etc.) and identifying the key challenges, constraints, and imperatives (i.e. ``mediating factors'') faced by Web-based system designers, and (2) understand how those contextual parameters and mediating factors influence the activity of WBSD as regards the selection and enactment of whatever design practices are therefore engaged (i.e. the use of methods, procedures, etc.). Data was gathered through a survey which yielded 165 usable responses, and later through a series of semi-structured qualitative interviews. Using grounded theory, an explanatory conceptual framework is derived, based on an extension of the ``method-in-action'' model, the application of which to WBSD has not been previously investigated in depth. It is proposed that this framework of WBSD issues is valuable in a number of ways to educators, researchers, practitioners, and method engineers.",
issn="1432-010X",
doi="10.1007/s00766-007-0052-2",
url="http://dx.doi.org/10.1007/s00766-007-0052-2"
"


@Article"Lee2014,
author="Lee, Jaejoon
and Kang, Kyo C.
and Sawyer, Pete
and Lee, Hyesun",
title="A holistic approach to feature modeling for product line requirements engineering",
journal="Requirements Engineering",
year="2014",
volume="19",
number="4",
pages="377--395",
abstract="Requirements engineering (RE) offers the means to discover, model, and manage the requirements of the products that comprise a product line, while software product line engineering (SPLE) offers the means of realizing the products' requirements from a common base of software assets. In practice, however, RE and SPLE have proven to be less complementary than they should. While some RE techniques, particularly goal modeling, support the exploration of alternative solutions, the appropriate solution is typically conditional on context and a large product line may have many product-defining contexts. Thus, scalability and traceability through into product line features are key challenges for RE. Feature modeling, by contrast, has been widely accepted as a way of modeling commonality and variability of products of a product line that may be very complex. In this paper, we propose a goal-driven feature modeling approach that separates a feature space in terms of problem space and solution space features, and establish explicit mappings between them. This approach contributes to reducing the inherent complexity of a mixed-view feature model, deriving key engineering drivers for developing core assets of a product line, and facilitating the quality-based product configuration.",
issn="1432-010X",
doi="10.1007/s00766-013-0183-6",
url="http://dx.doi.org/10.1007/s00766-013-0183-6"
"


@Article"Lehtola2009,
author="Lehtola, Laura
and Kauppinen, Marjo
and V"\"a"h"\"a"niitty, Jarno
and Komssi, Marko",
title="Linking business and requirements engineering: is solution planning a missing activity in software product companies?",
journal="Requirements Engineering",
year="2009",
volume="14",
number="2",
pages="113--128",
abstract="A strong link between strategy and product development is important, since companies need to select requirements for forthcoming releases. However, in practice, connecting requirements engineering (RE) and business planning is far from trivial. This paper describes the lessons learned from four software product companies that have recognized the need for more business-oriented long-term planning. The study was conducted using the action research approach. We identified five practices that seem to strengthen the link between business decisions and RE. These are (1) explicating the planning levels and time horizons; (2) separating the planning of products' business goals from R"\&"D resource allocation; (3) planning open-endedly with a pre-defined rhythm; (4) emphasizing whole-product thinking; and (5) making solution planning visible. To support whole-product thinking and solution planning, we suggest that companies create solution concepts. The purpose of the solution concept is to provide a big picture of the solution and guide RE activities.",
issn="1432-010X",
doi="10.1007/s00766-009-0078-8",
url="http://dx.doi.org/10.1007/s00766-009-0078-8"
"


@Article"Bryl2009,
author="Bryl, Volha
and Giorgini, Paolo
and Mylopoulos, John",
title="Designing socio-technical systems: from stakeholder goals to social networks",
journal="Requirements Engineering",
year="2009",
volume="14",
number="1",
pages="47--70",
abstract="Software systems are becoming an integral part of everyday life influencing organizational and social activities. This aggravates the need for a socio-technical perspective for requirements engineering, which allows for modelling and analyzing the composition and interaction of hardware and software components with human and organizational actors. In this setting, alternative requirements models have to be evaluated and selected finding a right trade-off between the technical and social dimensions. To address this problem, we propose a tool-supported process of requirements analysis for socio-technical systems, which adopts planning techniques for exploring the space of requirements alternatives and a number of social criteria for their evaluation. We illustrate the proposed approach with the help of a case study, conducted within the context of an EU project.",
issn="1432-010X",
doi="10.1007/s00766-008-0073-5",
url="http://dx.doi.org/10.1007/s00766-008-0073-5"
"


@Article"Lew2012,
author="Lew, Philip
and Olsina, Luis
and Becker, Pablo
and Zhang, Li",
title="An integrated strategy to systematically understand and manage quality in use for web applications",
journal="Requirements Engineering",
year="2012",
volume="17",
number="4",
pages="299--330",
abstract="The main goal in evaluating software quality is to ultimately improve its quality. In this work, we discuss SIQinU (Strategy for Improving Quality in Use), a six-phased evaluation-driven strategy for understanding and improving software quality requirements in a systematic way. Starting with quality in use (QinU), we design specific user tasks and context of use, and through identifying problems in QinU, we determine external quality (EQ) attributes that could be related to these QinU weakly performing indicators. Then, after deriving EQ attributes related to the QinU problems, we evaluate EQ and derive a benchmark to be used as a basis to make improvements. Once improvement recommendations are made based on poorly performing EQ indicators, a new version of the software application is completed and evaluated again for its EQ to establish a delta from the initial benchmark. Then, we re-evaluate QinU to determine the improvements resulting in QinU from the improvements made at the EQ level, thus leading to a cyclic strategy for improvement and development of relationships. SIQinU is a repeatable and consistent strategy which relies on: a conceptual framework (with ontological base), a process, and specific methods. In order to illustrate SIQinU, a real case study is conducted.",
issn="1432-010X",
doi="10.1007/s00766-011-0128-x",
url="http://dx.doi.org/10.1007/s00766-011-0128-x"
"


@Article"Sadraei2007,
author="Sadraei, Emila
and Aurum, Ayb"\"u"ke
and Beydoun, Ghassan
and Paech, Barbara",
title="A field study of the requirements engineering practice in Australian software industry",
journal="Requirements Engineering",
year="2007",
volume="12",
number="3",
pages="145--162",
abstract="Empirical studies have demonstrated that requirements errors introduced during software development are most numerous in the software life-cycle, making software requirements critical determinants of software quality. This article reports an exploratory study which provides insight into industrial practices with respect to requirements engineering (RE). A combination of both qualitative and quantitative data is collected, using semi-structured interviews and a detailed questionnaire from 28 software projects in 16 Australian companies. The contribution of this RE study is threefold: Firstly, it includes a detailed examination of the characteristics of the RE activities involved in the projects. Secondly, it reconstructs the underlying practiced process models. Thirdly, it compares these models to one another and with a number of well-known process models from RE literature to give insight into the gap between RE theory and practice.",
issn="1432-010X",
doi="10.1007/s00766-007-0042-4",
url="http://dx.doi.org/10.1007/s00766-007-0042-4"
"


@Article"Merriam2013,
author="Merriam, Nicholas
and Gliwa, Peter
and Broster, Ian",
title="Measurement and tracing methods for timing analysis",
journal="International Journal on Software Tools for Technology Transfer",
year="2013",
volume="15",
number="1",
pages="9--28",
abstract="Observing the timing properties of actual software provides information not derivable from pure modelling of the hardware, software and test data. Equally, modelling provides worst-case timing values that cannot be realistically determined from only testing and measurement. From this observation, we develop a combined methodology, where measured and modelled results are used in turn to build a complete, unified approach to software timing analysis. Beyond this, we develop toward a powerful and symbiotic process, where that data can be freely and reliably exchanged, and that is greater than the obvious sum of the parts.",
issn="1433-2787",
doi="10.1007/s10009-012-0266-6",
url="http://dx.doi.org/10.1007/s10009-012-0266-6"
"


@Article"Katasonov2006,
author="Katasonov, Artem
and Sakkinen, Markku",
title="Requirements quality control: a unifying framework",
journal="Requirements Engineering",
year="2006",
volume="11",
number="1",
pages="42--57",
abstract="Literature tends to discuss software (and system) requirements quality control, which includes validation and verification, as a heterogeneous process using a great variety of relatively independent techniques. Also, process-oriented thinking prevails. In this paper, we attempt to promote the point that this important activity must be studied as a coherent entity. It cannot be seen as a rather mechanical process of checking documents either. Validation, especially, is more an issue of communicating requirements, as constructed by the analysts, back to the stakeholders whose goals those requirements are supposed to meet, and to all those other stakeholders, with whose goals those requirements may conflict. The main problem, therefore, is that of achieving a sufficient level of understanding of the stated requirements by a particular stakeholder, which may be hindered by, for example, lack of technical expertise. In this paper, we develop a unifying framework for requirements quality control. We reorganize the existing knowledge around the issue of communicating requirements to all the different stakeholders, instead of just focusing on some techniques and processes. We hope that this framework could clarify thinking in the area, and make future research a little more focused.",
issn="1432-010X",
doi="10.1007/s00766-005-0018-1",
url="http://dx.doi.org/10.1007/s00766-005-0018-1"
"


@Article"Wahler2010,
author="Wahler, Michael
and Basin, David
and Brucker, Achim D.
and Koehler, Jana",
title="Efficient analysis of pattern-based constraint specifications",
journal="Software "\&" Systems Modeling",
year="2010",
volume="9",
number="2",
pages="225--255",
abstract="Precision and consistency are important prerequisites for class models to conform to their intended domain semantics. Precision can be achieved by augmenting models with design constraints and consistency can be achieved by avoiding contradictory constraints. However, there are different views of what constitutes a contradiction for design constraints. Moreover, state-of-the-art analysis approaches for proving constrained models consistent either scale poorly or require the use of interactive theorem proving. In this paper, we present a heuristic approach for efficiently analyzing constraint specifications built from constraint patterns. This analysis is based on precise notions of consistency for constrained class models and exploits the semantic properties of constraint patterns, thereby enabling syntax-based consistency checking in polynomial-time. We introduce a consistency checker implementing these ideas and we report on case studies in applying our approach to analyze industrial-scale models. These studies show that pattern-based constraint development supports the creation of concise specifications and provides immediate feedback on model consistency.",
issn="1619-1374",
doi="10.1007/s10270-009-0123-6",
url="http://dx.doi.org/10.1007/s10270-009-0123-6"
"


@Article"Immonen2007,
author="Immonen, Anne
and Niemel"\"a", Eila",
title="Survey of reliability and availability prediction methods from the viewpoint of software architecture",
journal="Software "\&" Systems Modeling",
year="2007",
volume="7",
number="1",
pages="49",
abstract="Many future software systems will be distributed across a network, extensively providing different kinds of services for their users. These systems must be highly reliable and provide services when required. Reliability and availability must be engineered into software from the onset of its development, and potential problems must be detected in the early stages, when it is easier and less expensive to implement modifications. The software architecture design phase is the first stage of software development in which it is possible to evaluate how well the quality requirements are being met. For this reason, a method is needed for analyzing software architecture with respect to reliability and availability. In this paper, we define a framework for comparing reliability and availability analysis methods from the viewpoint of software architecture. Our contribution is the comparison of the existing analysis methods and techniques that can be used for reliability and availability prediction at the architectural level. The objective is to discover which methods are suitable for the reliability and availability prediction of today's complex systems, what are the shortcomings of the methods, and which research activities need to be conducted in order to overcome these identified shortcomings. The comparison reveals that none of the existing methods entirely fulfill the requirements that are defined in the framework. The comparison framework also defines the characteristics required of new reliability and availability analysis methods. Additionally, the framework is a valuable tool for selecting the best suitable method for architecture analysis. Furthermore, the framework can be extended and used for other evaluation methods as well.",
issn="1619-1374",
doi="10.1007/s10270-006-0040-x",
url="http://dx.doi.org/10.1007/s10270-006-0040-x"
"


@Article"Groenewegen2013,
author="Groenewegen, Danny M.
and Visser, Eelco",
title="Integration of data validation and user interface concerns in a DSL for web applications",
journal="Software "\&" Systems Modeling",
year="2013",
volume="12",
number="1",
pages="35--52",
abstract="Data validation rules constitute the constraints that data input and processing must adhere to in addition to the structural constraints imposed by a data model. Web modeling tools do not make all types of data validation explicit in their models, hampering full code generation and model expressivity. Web application frameworks do not offer a consistent interface for data validation. In this paper, we present a solution for the integration of declarative data validation rules with user interface models in the domain of web applications, unifying syntax, mechanisms for error handling, and semantics of validation checks, and covering value well-formedness, data invariants, input assertions, and action assertions. We have implemented the approach in WebDSL, a domain-specific language for the definition of web applications.",
issn="1619-1374",
doi="10.1007/s10270-010-0173-9",
url="http://dx.doi.org/10.1007/s10270-010-0173-9"
"


@Article"Scanniello2009,
author="Scanniello, Giuseppe
and Distante, Damiano
and Risi, Michele",
title="An approach and an Eclipse-based environment for enhancing the navigation structure of Web sites",
journal="International Journal on Software Tools for Technology Transfer",
year="2009",
volume="11",
number="6",
pages="469",
abstract="This paper presents an approach based on information retrieval and clustering techniques for automatically enhancing the navigation structure of a Web site for improving navigability. The approach increments the set of navigation links provided in each page of the site with a semantic navigation map, i.e., a set of links enabling navigating from a given page to other pages of the site showing similar or related content. The approach uses Latent Semantic Indexing to compute a dissimilarity measure between the pages of the site and a graph-theoretic clustering algorithm to group pages showing similar or related content according to the calculated dissimilarity measure. AJAX code is finally used to extend each Web page with an associated semantic navigation map. The paper also presents a prototype of a tool developed to support the approach and the results from a case study conducted to assess the validity and feasibility of the proposal.",
issn="1433-2787",
doi="10.1007/s10009-009-0125-2",
url="http://dx.doi.org/10.1007/s10009-009-0125-2"
"


@Article"Mahmoud2015,
author="Mahmoud, Anas
and Niu, Nan",
title="On the role of semantics in automated requirements tracing",
journal="Requirements Engineering",
year="2015",
volume="20",
number="3",
pages="281--300",
abstract="In this paper, we investigate the potential benefits of utilizing natural language semantics in automated traceability link retrieval. In particular, we evaluate the performance of a wide spectrum of semantically enabled information retrieval methods in capturing and presenting requirements traceability links in software systems. Our objectives are to gain more operational insights into these methods and to provide practical guidelines for the design and development of effective requirements tracing and management tools. To achieve our research objectives, we conduct an experimental analysis using three datasets from various application domains. Results show that considering more semantic relations in traceability link retrieval does not necessarily lead to higher quality results. Instead, a more focused semantic support, that targets specific semantic relations, is expected to have a greater impact on the overall performance of tracing tools. In addition, our analysis shows that explicit semantic methods, that exploit local or domain-specific sources of knowledge, often achieve a more satisfactory performance than latent methods, or methods that derive semantics from external or general-purpose knowledge sources.",
issn="1432-010X",
doi="10.1007/s00766-013-0199-y",
url="http://dx.doi.org/10.1007/s00766-013-0199-y"
"


@Article"Jarraya2014,
author="Jarraya, Yosr
and Debbabi, Mourad",
title="Quantitative and qualitative analysis of SysML activity diagrams",
journal="International Journal on Software Tools for Technology Transfer",
year="2014",
volume="16",
number="4",
pages="399--419",
abstract="Model-driven engineering refers to a range of approaches that uses models throughout systems and software development life cycle. Towards sustaining such a successful approach in practice, we present a model-based verification framework that supports the quantitative and qualitative analysis of SysML activity diagrams. To this end, we propose an algorithm that maps SysML activity diagrams into Markov decision processes expressed using the language of the probabilistic symbolic model checker PRISM. Furthermore, we elaborate on the correctness of our translation algorithm by proving its soundness with respect to a SysML activity diagrams operational semantics that we also present in this work. The generated models can be verified against a set of properties expressed in the probabilistic computation tree logic. To automate our approach, we developed a prototype tool that interfaces a modeling environment and the probabilistic model checker. We also show how to leverage adversary generation to provide the developer with a useful counterexample/witness as a feedback on the verified properties. Finally, the established theoretical foundations are complemented with an illustrative case study that demonstrates the usability and benefit of such a framework.",
issn="1433-2787",
doi="10.1007/s10009-014-0305-6",
url="http://dx.doi.org/10.1007/s10009-014-0305-6"
"


@Article"Syriani2015,
author="Syriani, Eugene
and Vangheluwe, Hans
and LaShomb, Brian",
title="T-Core: a framework for custom-built model transformation engines",
journal="Software "\&" Systems Modeling",
year="2015",
volume="14",
number="3",
pages="1215--1243",
abstract="A large number of model transformation languages and tools have emerged since the early 2000s. A transformation engineer is thus left with too many choices for the language he use to perform a specific transformation task. Furthermore, it is currently not possible to combine or reuse transformations implemented in different languages. We therefore propose T-Core, a framework where primitive transformation constructs can be combined to define and encapsulate reusable model transformation idioms. In this context, the transformation engineer is free to use existing transformation building blocks from an extensible library or define his own transformation units. The proposed primitive transformation operators are the result of deconstructing different existing transformation languages. Reconstructing these languages offers a common basis to compare their expressiveness, provides a framework for inter-operating them, and allows the transformation engineer to design transformations with the most appropriate constructs for the task at hand.",
issn="1619-1374",
doi="10.1007/s10270-013-0370-4",
url="http://dx.doi.org/10.1007/s10270-013-0370-4"
"


@Article"Scandariato2015,
author="Scandariato, Riccardo
and Wuyts, Kim
and Joosen, Wouter",
title="A descriptive study of Microsoft's threat modeling technique",
journal="Requirements Engineering",
year="2015",
volume="20",
number="2",
pages="163--180",
abstract="Microsoft's STRIDE is a popular threat modeling technique commonly used to discover the security weaknesses of a software system. In turn, discovered weaknesses are a major driver for incepting security requirements. Despite its successful adoption, to date no empirical study has been carried out to quantify the cost and effectiveness of STRIDE. The contribution of this paper is the evaluation of STRIDE via a descriptive study that involved 57 students in their last master year in computer science. The study addresses three research questions. First, it assesses how many valid threats per hour are produced on average. Second, it evaluates the correctness of the analysis results by looking at the average number of false positives, i.e., the incorrect threats. Finally, it determines the completeness of the analysis results by looking at the average number of false negatives, i.e., the overlooked threats.",
issn="1432-010X",
doi="10.1007/s00766-013-0195-2",
url="http://dx.doi.org/10.1007/s00766-013-0195-2"
"


@Article"Ali2010,
author="Ali, Raian
and Dalpiaz, Fabiano
and Giorgini, Paolo",
title="A goal-based framework for contextual requirements modeling and analysis",
journal="Requirements Engineering",
year="2010",
volume="15",
number="4",
pages="439--458",
abstract="Requirements engineering (RE) research often ignores or presumes a uniform nature of the context in which the system operates. This assumption is no longer valid in emerging computing paradigms, such as ambient, pervasive and ubiquitous computing, where it is essential to monitor and adapt to an inherently varying context. Besides influencing the software, context may influence stakeholders' goals and their choices to meet them. In this paper, we propose a goal-oriented RE modeling and reasoning framework for systems operating in varying contexts. We introduce contextual goal models to relate goals and contexts; context analysis to refine contexts and identify ways to verify them; reasoning techniques to derive requirements reflecting the context and users priorities at runtime; and finally, design time reasoning techniques to derive requirements for a system to be developed at minimum cost and valid in all considered contexts. We illustrate and evaluate our approach through a case study about a museum-guide mobile information system.",
issn="1432-010X",
doi="10.1007/s00766-010-0110-z",
url="http://dx.doi.org/10.1007/s00766-010-0110-z"
"


@Article"Valles-Barajas2011,
author="Valles-Barajas, Fernando",
title="A survey of UML applications in mechatronic systems",
journal="Innovations in Systems and Software Engineering",
year="2011",
volume="7",
number="1",
pages="43--51",
abstract="Mechatronic systems are composed of mechanical, electronic and software parts. Recently, software processes and modeling notations traditionally used in software engineering have been used in building mechatronic systems. One of the modeling notations used in software design is the Unified Modeling Language (UML), a visual modeling language. In this paper, an analysis of UML in the building of mechatronic systems is presented.",
issn="1614-5054",
doi="10.1007/s11334-011-0143-6",
url="http://dx.doi.org/10.1007/s11334-011-0143-6"
"


@Article"Ehrig2009,
author="Ehrig, Karsten
and K"\"u"ster, Jochen Malte
and Taentzer, Gabriele",
title="Generating instance models from meta models",
journal="Software "\&" Systems Modeling",
year="2009",
volume="8",
number="4",
pages="479--500",
abstract="Meta modeling is a wide-spread technique to define visual languages, with the UML being the most prominent one. Despite several advantages of meta modeling such as ease of use, the meta modeling approach has one disadvantage: it is not constructive, i.e., it does not offer a direct means of generating instances of the language. This disadvantage poses a severe limitation for certain applications. For example, when developing model transformations, it is desirable to have enough valid instance models available for large-scale testing. Producing such a large set by hand is tedious. In the related problem of compiler testing, a string grammar together with a simple generation algorithm is typically used to produce words of the language automatically. In this paper, we introduce instance-generating graph grammars for creating instances of meta models, thereby overcoming the main deficit of the meta modeling approach for defining languages.",
issn="1619-1374",
doi="10.1007/s10270-008-0095-y",
url="http://dx.doi.org/10.1007/s10270-008-0095-y"
"


@Article"Dotti2006,
author="Dotti, Fernando Lu"\'i"s
and Ribeiro, Leila
and Santos, Osmar Marchi dos
and Pasini, F"\'a"bio",
title="Verifying Object-based Graph Grammars",
journal="Software "\&" Systems Modeling",
year="2006",
volume="5",
number="3",
pages="289--311",
abstract="The development of concurrent and reactive systems is gaining importance since they are well-suited to modern computing platforms, such as the Internet. However, the development of correct concurrent and reactive systems is a non-trivial task. Object-based graph grammar (OBGG) is a visual formal language suitable for the specification of this class of systems. In previous work, a translation from OBGG to PROMELA (the input language of the SPIN model checker) was defined, enabling the verification of OBGG models using SPIN. In this paper we extend this approach in two different ways: (1) the approach for property specification is improved, enabling to prove properties not only about possible OBGG derivations, but also about the internal state of involved objects; (2) an approach is defined to interpret PROMELA races as OBGG derivations, generating graphical counter-examples for properties that are not true for a given OBGG model. Another contribution of this paper is (3) the definition of a method for model checking partial systems (isolated objects or a set of objects) using an assume-guarantee approach. A gas station system modeled with OBGGs is used to illustrate the contributions.",
issn="1619-1374",
doi="10.1007/s10270-006-0014-z",
url="http://dx.doi.org/10.1007/s10270-006-0014-z"
"


@Article"Ferrari2010,
author="Ferrari, Remo
and Miller, James A.
and Madhavji, Nazim H.",
title="A controlled experiment to assess the impact of system architectures on new system requirements",
journal="Requirements Engineering",
year="2010",
volume="15",
number="2",
pages="215--233",
abstract="While much research attention has been paid to transitioning from requirements to software architectures, relatively little attention has been paid to how new requirements are affected by an existing system architecture. Specifically, no scientific studies have been conducted on the ``characteristic'' differences between the newly elicited requirements gathered in the presence or absence of an existing software architecture. This paper describes an exploratory controlled study investigating such requirements characteristics. We identify a multitude of characteristics (e.g., end-user focus, technological focus, and importance) that were affected by the presence or absence of an SA, together with the extent of this effect. Furthermore, we identify the specific aspects of the architecture that had an impact on the characteristics. The study results have implications for RE process engineering, post-requirements analysis, requirements engineering tools, traceability management, and future empirical work in RE based on several emergent hypotheses resultant from this study.",
issn="1432-010X",
doi="10.1007/s00766-010-0099-3",
url="http://dx.doi.org/10.1007/s00766-010-0099-3"
"


@Article"Maoz2011,
author="Maoz, Shahar
and Harel, David",
title="On tracing reactive systems",
journal="Software "\&" Systems Modeling ",
year="2011",
volume="10",
number="4",
pages="447--468",
abstract="We present a rich and highly dynamic technique for analyzing, visualizing, and exploring the execution traces of reactive systems. The two inputs are a designer's inter-object scenario-based behavioral model, visually described using a UML2-compliant dialect of live sequence charts (LSC), and an execution trace of the system. Our method allows one to visualize, navigate through, and explore, the activation and progress of the scenarios as they ``come to life'' during execution. Thus, a concrete system's runtime is recorded and viewed through abstractions provided by behavioral models used for its design, tying the visualization and exploration of system execution traces to model-driven engineering. We support both event-based and real-time-based tracing, and use details-on-demand mechanisms, multi-scaling grids, and gradient coloring methods. Novel model exploration techniques include semantics-based navigation, filtering, and trace comparison. The ideas are implemented and tested in a prototype tool called the Tracer.",
issn="1619-1374",
doi="10.1007/s10270-010-0151-2",
url="http://dx.doi.org/10.1007/s10270-010-0151-2"
"


@Article"Zang2013,
author="Zang, Liang-Jun
and Cao, Cong
and Cao, Ya-Nan
and Wu, Yu-Ming
and CAO, Cun-Gen",
title="A Survey of Commonsense Knowledge Acquisition",
journal="Journal of Computer Science and Technology",
year="2013",
volume="28",
number="4",
pages="689--719",
abstract="Collecting massive commonsense knowledge (CSK) for commonsense reasoning has been a long time standing challenge within artificial intelligence research. Numerous methods and systems for acquiring CSK have been developed to overcome the knowledge acquisition bottleneck. Although some specific commonsense reasoning tasks have been presented to allow researchers to measure and compare the performance of their CSK systems, we compare them at a higher level from the following aspects: CSK acquisition task (what CSK is acquired from where), technique used (how can CSK be acquired), and CSK evaluation methods (how to evaluate the acquired CSK). In this survey, we first present a categorization of CSK acquisition systems and the great challenges in the field. Then, we review and compare the CSK acquisition systems in detail. Finally, we conclude the current progress in this field and explore some promising future research issues.",
issn="1860-4749",
doi="10.1007/s11390-013-1369-6",
url="http://dx.doi.org/10.1007/s11390-013-1369-6"
"


@Article"Gnesi2015,
author="Gnesi, Stefania
and Jarzabek, Stan",
title="Special section on the 17th International Software Product Line Conference",
journal="International Journal on Software Tools for Technology Transfer",
year="2015",
volume="17",
number="5",
pages="555--557",
abstract="Today, companies develop, maintain and deploy families of similar software products (e.g., games for different models of smartphones) rather than a single product. Software product lines engineering refers to software engineering methods, tools and techniques for creating a collection of similar software systems from a shared set of software assets using a common means of production. Software Product Line Conferences started in 1996, as the premier forum for practitioners, researchers and educators to present and discuss the most recent ideas, innovations, trends, experiences, and concerns in the area of software product lines, software product family engineering and, more recently, systems family engineering, managing families of software products as a whole rather than each family member individually. This special section stems from the 17th SPL Conference held in Tokyo, Japan, in August 2013. The contributions to this special section are further elaborations of the papers presented at the conference.",
issn="1433-2787",
doi="10.1007/s10009-015-0386-x",
url="http://dx.doi.org/10.1007/s10009-015-0386-x"
"


@Article"Mart�nez-Fern�ndez2014,
author="Mart"\'i"nez-Fern"\'a"ndez, Silverio
and Ayala, Claudia P.
and Franch, Xavier
and Marques, Helena Martins
and Ameller, David",
title="Towards guidelines for building a business case and gathering evidence of software reference architectures in industry",
journal="Journal of Software Engineering Research and Development",
year="2014",
volume="2",
number="1",
pages="7",
abstract="Software reference architectures are becoming widely adopted by organizations that need to support the design and maintenance of software applications of a shared domain. For organizations that plan to adopt this architecture-centric approach, it becomes fundamental to know the return on investment and to understand how software reference architectures are designed, maintained, and used. Unfortunately, there is little evidence-based support to help organizations with these challenges.",
issn="2195-1721",
doi="10.1186/s40411-014-0007-5",
url="http://dx.doi.org/10.1186/s40411-014-0007-5"
"


@Article"Jacobson2012,
author="Jacobson, Ivar
and Huang, Shihong
and Kajko-Mattsson, Mira
and McMahon, Paul
and Seymour, Ed",
title="Semat---Three Year Vision",
journal="Programming and Computer Software",
year="2012",
volume="38",
number="1",
pages="1--12",
abstract="The purpose of writing this Three Year Vision paper is threefold. Firstly, it briefly recaps the progress Semat has made thus far; secondly, it lays out the future directions for people working actively within the Semat community; thirdly, it provides the background for seeking funding support from agencies, such as the European Community and the like. Funding support is necessary to sustain the ongoing activities of Semat and its growth into a broader community effort, as most people working within Semat are volunteers. As such, the paper may be both too much and too little for the wider supporter base. However, we intend to make our work fully transparent, hence, we publish it widely. We seek feedback and comments from supporters and signatories in order to improve the vision. In this context, other companion papers are being written to better address the specific needs for the practitioners, the industry and the academia.",
issn="1608-3261",
doi="10.1134/S0361768812010021",
url="http://dx.doi.org/10.1134/S0361768812010021"
"


@Article"Soundarajan2008,
author="Soundarajan, Neelam
and Hallstrom, Jason O.
and Shu, Guoqiang
and Delibas, Adem",
title="Patterns: from system design to software testing",
journal="Innovations in Systems and Software Engineering",
year="2008",
volume="4",
number="1",
pages="71--85",
abstract="Design patterns are used extensively in the design of software systems. Patterns codify effective solutions for recurring design problems and allow software engineers to reuse these solutions, tailoring them appropriately to their particular applications, rather than reinventing them from scratch. In this paper, we consider the following question: How can system designers and implementers test whether their systems, as implemented, are faithful to the requirements of the patterns used in their design? A key consideration underlying our work is that the testing approach should enable us, in testing whether a particular pattern P has been correctly implemented in different systems designed using P, to reuse the common parts of this effort rather than having to do it from scratch for each system. Thus in the approach we present, corresponding to each pattern P, there is a set of pattern test case templates (PTCTs). A PTCT codifies a reusable test case structure designed to identify defects associated with applications of P in all systems designed using P. Next we present a process using which, given a system designed using P, the system tester can generate a test suite from the PTCTs for P that can be used to test the particular system for bugs in the implementation of P in that system. This allows the tester to tailor the PTCTs for P to the needs of the particular system by specifying a set of specialization rules that are designed to reflect the scenarios in which the defects codified in this set of PTCTs are likely to manifest themselves in the particular system. We illustrate the approach using the Observer pattern.",
issn="1614-5054",
doi="10.1007/s11334-007-0042-z",
url="http://dx.doi.org/10.1007/s11334-007-0042-z"
"


@Article"Barrag�nsMart�nez2008,
author="Barrag"\'a"ns Mart"\'i"nez, Ana Bel"\'e"n
and Pazos Arias, Jos"\'e" J.
and Fern"\'a"ndez Vilas, Ana
and Garc"\'i"a Duque, Jorge
and L"\'o"pez Nores, Mart"\'i"n
and D"\'i"az Redondo, Rebeca P.
and Blanco Fern"\'a"ndez, Yolanda",
title="Composing requirements specifications from multiple prioritized sources",
journal="Requirements Engineering",
year="2008",
volume="13",
number="3",
pages="187--206",
abstract="The formal methodology                   MultiSpec                 supports the evolution of software specifications gathered from multiple perspectives. A viewpoint-based approach is used to explicitly separate the descriptions provided by different stakeholders, and concentrate on identifying and resolving conflicts between them. The challenge addressed in this article consists in taking into account that some views may have greater degrees of relevance and, consequently, their opinion will have more importance when either obtaining the merged model or resolving the contradictions. To this end, we propose a priority-based approach, where such priority value is twofold. On the one hand, it considers external factors to the perspectives such as the importance assigned to each view by the analyst depending on who is specifying the view or the amount of stakeholders involved in that specification. On the other hand, this priority value also considers internal factors related to the quality of the views and, in order to be able to quantify this value,                   MultiSpec                 proposes two measures: coverage and density of each perspective which will be combined in a completeness value. The contributions of this approach will be clearly illustrated through a simple example.",
issn="1432-010X",
doi="10.1007/s00766-008-0064-6",
url="http://dx.doi.org/10.1007/s00766-008-0064-6"
"


@Article"St�rrle2013,
author="St"\"o"rrle, Harald",
title="Towards clone detection in UML domain models",
journal="Software "\&" Systems Modeling",
year="2013",
volume="12",
number="2",
pages="307--329",
abstract="Code clones (i.e., duplicate fragments of code) have been studied for long, and there is strong evidence that they are a major source of software faults. Anecdotal evidence suggests that this phenomenon occurs similarly in models, suggesting that model clones are as detrimental to model quality as they are to code quality. However, programming language code and visual models have significant differences that make it difficult to directly transfer notions and algorithms developed in the code clone arena to model clones. In this article, we develop and propose a definition of the notion of ``model clone'' based on the thorough analysis of practical scenarios. We propose a formal definition of model clones, specify a clone detection algorithm for UML domain models, and implement it prototypically. We investigate different similarity heuristics to be used in the algorithm, and report the performance of our approach. While we believe that our approach advances the state of the art significantly, it is restricted to UML models, its results leave room for improvements, and there is no validation by field studies.",
issn="1619-1374",
doi="10.1007/s10270-011-0217-9",
url="http://dx.doi.org/10.1007/s10270-011-0217-9"
"


@Article"Rodrigues2015,
author="Rodrigues, Taniro
and Delicato, Fl"\'a"via C.
and Batista, Thais
and Pires, Paulo F.
and Pirmez, Luci",
title="An approach based on the domain perspective to develop WSAN applications",
journal="Software "\&" Systems Modeling",
year="2015",
pages="1--29",
abstract="As wireless sensor and actuator networks (WSANs) can be used in many different domains, WSAN applications have to be built from two viewpoints: domain and network. These different viewpoints create a gap between the abstractions handled by the application developers, namely the domain and network experts. Furthermore, there is a coupling between the application logic and the underlying sensor platform, which results in platform-dependent projects and source codes difficult to maintain, modify, and reuse. Consequently, the process of developing an application becomes cumbersome. In this paper, we propose a model-driven architecture (MDA) approach for WSAN application development. Our approach aims to facilitate the task of the developers by: (1) enabling application design through high abstraction level models; (2) providing a specific methodology for developing WSAN applications; and (3) offering an MDA infrastructure composed of PIM, PSM, and transformation programs to support this process. Our approach allows the direct contribution of domain experts in the development of WSAN applications, without requiring specific knowledge of programming WSAN platforms. In addition, it allows network experts to focus on the specific characteristics of their area of expertise without the need of knowing each specific application domain.",
issn="1619-1374",
doi="10.1007/s10270-015-0498-5",
url="http://dx.doi.org/10.1007/s10270-015-0498-5"
"


@Article"Torres2012,
author="Torres, Victoria
and Giner, Pau
and Pelechano, Vicente",
title="Developing BP-driven web applications through the use of MDE techniques",
journal="Software "\&" Systems Modeling",
year="2012",
volume="11",
number="4",
pages="609--631",
abstract="Model driven engineering (MDE) is a suitable approach for performing the construction of software systems (in particular in the Web application domain). There are different types of Web applications depending on their purpose (i.e., document-centric, interactive, transactional, workflow/business process-based, collaborative, etc). This work focusses on business process-based Web applications in order to be able to understand business processes in a broad sense, from the lightweight business processes already addressed by existing proposals to long-running asynchronous processes. This work presents a MDE method for the construction of systems of this type. The method has been designed in two steps following the MDE principles. In the first step, the system is represented by means of models in a technology-independent manner. These models capture the different aspects of Web-based systems (these aspects refer to behaviour, structure, navigation, and presentation issues). In the second step, the model transformations (both model- to-model and model-to-text) are applied in order to obtain the final system in terms of a specific technology. In addition, a set of Eclipse-based tools has been developed to provide automation in the application of the proposed method in order to validate the proposal.",
issn="1619-1374",
doi="10.1007/s10270-010-0177-5",
url="http://dx.doi.org/10.1007/s10270-010-0177-5"
"


@Article"Andrade2010,
author="Andrade, Ermeson
and Maciel, Paulo
and Falc"\~a"o, Tiago
and Nogueira, Bruno
and Araujo, Carlos
and Callou, Gustavo",
title="Performance and energy consumption estimation for commercial off-the-shelf component system design",
journal="Innovations in Systems and Software Engineering",
year="2010",
volume="6",
number="1",
pages="107--114",
abstract="Nowadays, component-based embedded real- time systems have been used to improve the system development as well as to keep cost down through the reuse of embedded software applications. Besides, the use of semi-formal models has been widely adopted in the embedded real-time system component and system life cycle due to their friendly and intuitive notations. However, the ever more complex systems of today require modeling methods that allow early detection of potential problems in the initial phases of development. This paper presents the mapping process of UML state machine diagram into a time Petri net with energy constraints so as to estimate execution time and energy consumption in early phases of the embedded real-time component development life cycle. The estimates obtained from the model show that the proposed approach is indeed a good approximation to the respective measures obtained from the real hardware platform.",
issn="1614-5054",
doi="10.1007/s11334-009-0110-7",
url="http://dx.doi.org/10.1007/s11334-009-0110-7"
"


@Article"VanDerStraeten2007,
author="Van Der Straeten, Ragnhild
and Jonckers, Viviane
and Mens, Tom",
title="A formal approach to model refactoring and model refinement",
journal="Software "\&" Systems Modeling",
year="2007",
volume="6",
number="2",
pages="139--162",
abstract="Model-driven engineering is an emerging software engineering approach that relies on model transformation. Typical kinds of model transformations are model refinement and model refactoring. Whenever such a transformation is applied to a consistent model, we would like to know whether the consistency is preserved by the transformation. Therefore, in this article, we formally define and explore the relation between behaviour inheritance consistency of a refined model with respect to the original model, and behaviour preservation of a refactored model with respect to the original model. As it turns out, there is a strong similarity between these notions of behaviour consistency and behaviour preservation. To illustrate this claim, we formalised the behaviour specified by UML 2.0 sequence and protocol state machine diagrams. We show how the reasoning capabilities of description logics, a decidable fragment of first-order logic, can be used in a natural way to detect behaviour inconsistencies. These reasoning capabilities can be used in exactly the same way to detect behaviour preservation violations during model refactoring. A prototype plug-in in a UML CASE tool has been developed to validate our claims.",
issn="1619-1374",
doi="10.1007/s10270-006-0025-9",
url="http://dx.doi.org/10.1007/s10270-006-0025-9"
"


@Article"Nelson2007,
author="Nelson, Maria Augusta V.
and Alencar, Paulo S. C.
and Cowan, Donald D.",
title="Informal description and analysis of geographic requirements: an approach based on problems",
journal="Software "\&" Systems Modeling",
year="2007",
volume="6",
number="3",
pages="223--245",
abstract="Software requirements describe a problem in the real world that a software system is intended to solve. Describing requirements is challenging because usually too much attention is given to the final software product instead of concentrating on the problem itself and the real world. The area of geographic applications is no exception. Existing approaches to software development that are specific to the geographic area, for example, GIS tools, spatial databases, geographic query languages, and spatial data structures, are suitable for designing and implementing geographic applications and are, therefore, solution-oriented. We present a problem-oriented approach for requirements description of geographic applications. Most geographic applications are composed of well-known geographic subproblems. The proposed approach provides classes of common geographic subproblems that can be used to promote analysis and description of real-world problems. Each class of problems is presented as a problem frame showing domain properties, requirements and specifications. The problem frames discussed in this work are based on Jackson's general purpose problem frames and are tailored here for the geographic area. The approach is validated through a case study.",
issn="1619-1374",
doi="10.1007/s10270-006-0031-y",
url="http://dx.doi.org/10.1007/s10270-006-0031-y"
"


@Article"Gorse2006,
author="Gorse, Nicolas
and Logrippo, Luigi
and Sincennes, Jacques",
title="Detecting feature interaction in CPL",
journal="Software "\&" Systems Modeling",
year="2006",
volume="5",
number="2",
pages="121--134",
abstract="This article addresses the problem of detecting feature interactions in the area of telephony systems design. The proposed approach consists of two phases: filtering and testing. The filtering phase detects possible interactions by identifying incoherencies in a logic specification of the main elements of the features, consisting of preconditions, triggers, results and constraints. If incoherencies are identified, then an interaction is suspected, test cases corresponding to the suspected interaction are generated and testing is applied to see if the interaction actually exists. Two case studies, carried out on established benchmarks, show that this approach gives good results in practice.",
issn="1619-1374",
doi="10.1007/s10270-005-0101-6",
url="http://dx.doi.org/10.1007/s10270-005-0101-6"
"


@Article"Pakulin2007,
author="Pakulin, N. V.
and Khoroshilov, A. V.",
title="Development of formal models and conformance testing for systems with asynchronous interfaces and telecommunications protocols",
journal="Programming and Computer Software",
year="2007",
volume="33",
number="6",
pages="316--335",
abstract="There is a gap between the formal modeling and testing methods for modern protocols and asynchronous software systems: due to high complexity of such systems, attempts to include formal models in testing procedures fail. In this paper, we propose an approach to filling this gap based on a formalization of the behavior of systems with asynchronous interfaces using contract specifications followed by the use of these specifications to design adaptive test suites. This approach was used for testing various software systems including implementations of the IPv6 Internet protocols stack and implementations of the POSIX and Linux Standard Base software interfaces.",
issn="1608-3261",
doi="10.1134/S0361768807060035",
url="http://dx.doi.org/10.1134/S0361768807060035"
"


@Article"El-Attar2010,
author="El-Attar, Mohamed
and Miller, James",
title="Improving the quality of use case models using antipatterns",
journal="Software "\&" Systems Modeling",
year="2010",
volume="9",
number="2",
pages="141--160",
abstract="Use case (UC) modeling is a popular requirements modeling technique. While these models are simple to create and read; this simplicity is often misconceived, leading practitioners to believe that creating high quality models is straightforward. Therefore, many low quality models that are inconsistent, incorrect, contain premature restrictive design decision and contain ambiguous information are produced. To combat this problem of creating low quality UC models, this paper presents a new technique that utilizes antipatterns as a mechanism for remedying quality problems in UC models. The technique, supported by the tool ARBIUM, provides a framework for developers to define antipatterns. The feasibility of the approach is demonstrated by applying it to a real-world system. The results indicate that applying the technique improves the overall quality and clarity of UC models.",
issn="1619-1374",
doi="10.1007/s10270-009-0112-9",
url="http://dx.doi.org/10.1007/s10270-009-0112-9"
"


@Article"Blouin2015,
author="Blouin, Arnaud
and Combemale, Beno"\^i"t
and Baudry, Benoit
and Beaudoux, Olivier",
title="Kompren: modeling and generating model slicers",
journal="Software "\&" Systems Modeling",
year="2015",
volume="14",
number="1",
pages="321--337",
abstract="Among model comprehension tools, model slicers are tools that extract a subset of model elements, for a specific purpose. Model slicers provide a mechanism to isolate and focus on parts of the model, thereby improving the overall analysis process. However, existing slicers are dedicated to a specific modeling language. This is an issue when we observe that new domain specific modeling languages, for which we want slicing abilities, are created almost on a daily basis. This paper proposes the Kompren language to model and generate model slicers for any DSL (e.g. modeling for software development or for civil engineering) and for different purposes (e.g. monitoring and model comprehension). We detail the semantics of the Kompren language and of the model slicer generator. This provides a set of expected properties about the slices that are extracted by the different forms of the slicer. Then we illustrate these different forms of slicers on case studies from various domains.",
issn="1619-1374",
doi="10.1007/s10270-012-0300-x",
url="http://dx.doi.org/10.1007/s10270-012-0300-x"
"


@Article"Evermann2009,
author="Evermann, Joerg",
title="A UML and OWL description of Bunge's upper-level ontology model",
journal="Software "\&" Systems Modeling",
year="2009",
volume="8",
number="2",
pages="235--249",
abstract="A prominent high-level ontology is that proposed by Mario Bunge. While it has been extensively used for research in IS analysis and conceptual modelling, it has not been employed in the more formal settings of semantic web research. We claim that its specification in natural language is the key inhibitor to its wider use. Consequently, this paper offers a description of this ontology in open, standardized knowledge representation formats. The ontology is described both in UML and OWL in order to address needs of both semantic web and conceptual modelling communities.",
issn="1619-1374",
doi="10.1007/s10270-008-0082-3",
url="http://dx.doi.org/10.1007/s10270-008-0082-3"
"


@Article"Godefroid2004,
author="Godefroid, Patrice
and Khurshid, Sarfraz",
title="Exploring very large state spaces using genetic algorithms",
journal="International Journal on Software Tools for Technology Transfer",
year="2004",
volume="6",
number="2",
pages="117--127",
abstract="We present a novel framework for exploring very large state spaces of concurrent reactive systems. Our framework exploits application-independent heuristics using genetic algorithms to guide a state-space search toward error states. We have implemented this framework in conjunction with VeriSoft, a tool for exploring the state spaces of software applications composed of several concurrent processes executing arbitrary code. We present experimental results obtained with several examples of programs, including a C implementation of a public-key authentication protocol. We discuss heuristics and properties of state spaces that help a genetic search detect deadlocks and assertion violations. For finding errors in very large state spaces, our experiments show that a genetic search using simple heuristics can significantly outperform random and systematic searches. ",
issn="1433-2787",
doi="10.1007/s10009-004-0141-1",
url="http://dx.doi.org/10.1007/s10009-004-0141-1"
"


@Article"Yskout2014,
author="Yskout, Koen
and Scandariato, Riccardo
and Joosen, Wouter",
title="Change patterns",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="2",
pages="625--648",
abstract="Change, such as in the requirements or the assumptions of a system, has a far-reaching impact across several software artifacts. This paper argues that patterns of co-evolution (or change patterns) can be observed between intertwined pairs of artifacts, like the requirements specification and the architectural design. The paper introduces change patterns as a precise framework to systematically capture and handle change. The approach is based on model-driven engineering concepts and is accompanied by a tool-supported process. Changing trust assumptions are presented as an example of security-related evolution, and are used to illustrate the approach. The approach is empirically validated by means of a controlled experiment involving 12 subjects, and a case study involving an industrial partner.",
issn="1619-1374",
doi="10.1007/s10270-012-0276-6",
url="http://dx.doi.org/10.1007/s10270-012-0276-6"
"


@Article"Kusel2015,
author="Kusel, A.
and Sch"\"o"nb"\"o"ck, J.
and Wimmer, M.
and Kappel, G.
and Retschitzegger, W.
and Schwinger, W.",
title="Reuse in model-to-model transformation languages: are we there yet?",
journal="Software "\&" Systems Modeling",
year="2015",
volume="14",
number="2",
pages="537--572",
abstract="In the area of model-driven engineering, model transformations are proposed as the technique to systematically manipulate models. For increasing development productivity as well as quality of model transformations, reuse mechanisms are indispensable. Although numerous mechanisms have been proposed, no systematic comparison exists, making it unclear, which reuse mechanisms may be best employed in a certain situation. Thus, this paper provides an in-depth comparison of reuse mechanisms in model-to-model transformation languages and categorizes them along their intended scope of application. Finally, current barriers and facilitators to model transformation reuse are discussed.",
issn="1619-1374",
doi="10.1007/s10270-013-0343-7",
url="http://dx.doi.org/10.1007/s10270-013-0343-7"
"


@Article"Sabetzadeh2006,
author="Sabetzadeh, Mehrdad
and Easterbrook, Steve",
title="View merging in the presence of incompleteness and inconsistency",
journal="Requirements Engineering",
year="2006",
volume="11",
number="3",
pages="174--193",
abstract="View merging, also called view integration, is a key problem in conceptual modeling. Large models are often constructed and accessed by manipulating individual views, but it is important to be able to consolidate a set of views to gain a unified perspective, to understand interactions between views, or to perform various types of analysis. View merging is complicated by incompleteness and inconsistency: Stakeholders often have varying degrees of confidence about their statements. Their views capture different but overlapping aspects of a problem, and may have discrepancies over the terminology being used, the concepts being modeled, or how these concepts should be structured. Once views are merged, it is important to be able to trace the elements of the merged view back to their sources and to the merge assumptions related to them. In this paper, we present a framework for merging incomplete and inconsistent graph-based views. We introduce a formalism, called annotated graphs, with a built-in annotation scheme for modeling incompleteness and inconsistency. We show how structure-preserving maps can be employed to express the relationships between disparate views modeled as annotated graphs, and provide a general algorithm for merging views with arbitrary interconnections. We provide a systematic way to generate and represent the traceability information required for tracing the merged view elements back to their sources, and to the merge assumptions giving rise to the elements.",
issn="1432-010X",
doi="10.1007/s00766-006-0032-y",
url="http://dx.doi.org/10.1007/s00766-006-0032-y"
"


@Article"Sutcliffe2006,
author="Sutcliffe, Alistair
and Fickas, Stephen
and Sohlberg, McKay Moore",
title="PC-RE: a method for personal and contextual requirements engineering with some experience",
journal="Requirements Engineering",
year="2006",
volume="11",
number="3",
pages="157--173",
abstract="A method for requirements analysis is proposed that accounts for individual and personal goals, and the effect of time and context on personal requirements. First a framework to analyse the issues inherent in requirements that change over time and location is proposed. The implications of the framework on system architecture are considered as three implementation pathways: functional specifications, development of customisable features and automatic adaptation by the system. These pathways imply the need to analyse system architecture requirements. A scenario-based analysis method is described for specifying requirements goals and their potential change. The method addresses goal setting for measurement and monitoring, and conflict resolution when requirements at different layers (group, individual) and from different sources (personal, advice from an external authority) conflict. The method links requirements analysis to design by modelling alternative solution pathways. Different implementation pathways have cost--benefit implications for stakeholders, so cost--benefit analysis techniques are proposed to assess trade-offs between goals and implementation strategies. The use of the framework is illustrated with two case studies in assistive technology domains: e-mail and a personalised navigation system. The first case study illustrates personal requirements to help cognitively disabled users communicate via e-mail, while the second addresses personal and mobile requirements to help disabled users make journeys on their own, assisted by a mobile PDA guide. In both case studies the experience from requirements analysis to implementation, requirements monitoring, and requirements evolution is reported.",
issn="1432-010X",
doi="10.1007/s00766-006-0030-0",
url="http://dx.doi.org/10.1007/s00766-006-0030-0"
"


@Article"Castillos2011,
author="Castillos, Kalou Cabrera
and Dadeau, Fr"\'e"d"\'e"ric
and Julliand, Jacques",
title="Scenario-based testing from UML/OCL behavioral models",
journal="International Journal on Software Tools for Technology Transfer",
year="2011",
volume="13",
number="5",
pages="431--448",
abstract="We present in this article a way to produce test suites applied to the POSIX mini-challenge based on a behavioral model of a file system manager written in UML/OCL. We illustrate the limitations of a fully automated test generation approach, which justifies the use of test scenarios as a complement to a functional testing approach. Scenarios are expressed through regular expressions describing sequences of operations, possibly punctuated by intermediate states that have to be reached by the execution of the model. Scenarios are unfolded into extended sequences of operations that are played on the model using symbolic animation techniques. We experimented our approach by testing the conformance of two different file systems w.r.t. the POSIX standard: a recent Linux distribution and a customized Java implementation of POSIX used to evaluate the relevance of our approach and its complementarity with a structural test generation approach.",
issn="1433-2787",
doi="10.1007/s10009-011-0189-7",
url="http://dx.doi.org/10.1007/s10009-011-0189-7"
"


@Article"Bagnara2006,
author="Bagnara, Roberto
and Hill, Patricia M.
and Zaffanella, Enea",
title="Widening operators for powerset domains",
journal="International Journal on Software Tools for Technology Transfer",
year="2006",
volume="8",
number="4",
pages="449--466",
abstract="The finite powerset construction upgrades an abstract domain by allowing for the representation of finite disjunctions of its elements. While most of the operations on the finite powerset abstract domain are easily obtained by ``lifting'' the corresponding operations on the base-level domain, the problem of endowing finite powersets with a provably correct widening operator is still open. In this paper we define three generic widening methodologies for the finite powerset abstract domain. The widenings are obtained by lifting any widening operator defined on the base-level abstract domain and are parametric with respect to the specification of a few additional operators that allow all the flexibility required to tune the complexity/precision trade-off. As far as we know, this is the first time that the problem of deriving non-trivial, provably correct widening operators in a domain refinement is tackled successfully. We illustrate the proposed techniques by instantiating our widening methodologies on powersets of convex polyhedra, a domain for which no non-trivial widening operator was previously known.",
issn="1433-2787",
doi="10.1007/s10009-005-0215-8",
url="http://dx.doi.org/10.1007/s10009-005-0215-8"
"


@Article"Degrandsart2014,
author="Degrandsart, Sylvain
and Demeyer, Serge
and Van den Bergh, Jan
and Mens, Tom",
title="A transformation-based approach to context-aware modelling",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="1",
pages="191--208",
abstract="Context-aware computing is a paradigm for governing the numerous mobile devices surrounding us. In this computing paradigm, software applications continuously and dynamically adapt to different ``contexts'' implying different software configurations of such devices. Unfortunately, modelling a context-aware application (CAA) for all possible contexts is only feasible in the simplest of cases. Hence, tool support verifying certain properties is required. In this article, we introduce the CAA model, in which context adaptations are specified explicitly as model transformations. By mapping this model to graphs and graph transformations, we can exploit graph transformation techniques such as critical pair analysis to find contexts for which the resulting application model is ambiguous. We validate our approach by means of an example of a mobile city guide, demonstrating that we can identify subtle context interactions that might go unnoticed otherwise.",
issn="1619-1374",
doi="10.1007/s10270-012-0239-y",
url="http://dx.doi.org/10.1007/s10270-012-0239-y"
"


@Article"Fernandes2012,
author="Fernandes, Jo"\~a"o M.
and Dori, Dov",
title="Model-based approaches and frameworks for embedded software systems",
journal="Innovations in Systems and Software Engineering",
year="2012",
volume="8",
number="1",
pages="1--2",
issn="1614-5054",
doi="10.1007/s11334-011-0176-x",
url="http://dx.doi.org/10.1007/s11334-011-0176-x"
"


@Article"Rendon2010,
author="Rendon, Gloria
and Ger, Mao-Feng
and Kantorovitz, Ruth
and Natarajan, Shreedhar
and Tilson, Jeffrey
and Jakobsson, Eric",
title="Understanding the ``Horizontal Dimension'' of Molecular Evolution to Annotate, Classify, and Discover Proteins with Functional Domains",
journal="Journal of Computer Science and Technology",
year="2010",
volume="25",
number="1",
pages="82--94",
abstract="Protein evolution proceeds by two distinct processes: 1) individual mutation and selection for adaptive mutations and 2) rearrangement of entire domains within proteins into novel combinations, producing new protein families that combine functional properties in ways that previously did not exist. Domain rearrangement poses a challenge to sequence alignment-based search methods, such as BLAST, in predicting homology since the methodology implicitly assumes that related proteins primarily differ from each other by individual mutations. Moreover, there is ample evidence that the evolutionary process has used (and continues to use) domains as building blocks, therefore, it seems fit to utilize computational, domain-based methods to reconstruct that process. A challenge and opportunity for computational biology is how to use knowledge of evolutionary domain recombination to characterize families of proteins whose evolutionary history includes such recombination, to discover novel proteins, and to infer protein-protein interactions. In this paper we review techniques and databases that exploit our growing knowledge of ``horizontal'' protein evolution, and suggest possible areas of future development. We illustrate the power of the domain-based methods and the possible directions of future development by a case history in progress aiming at facilitating a particular approach to understanding microbial pathogenicity.",
issn="1860-4749",
doi="10.1007/s11390-010-9307-3",
url="http://dx.doi.org/10.1007/s11390-010-9307-3"
"


@Article"Masci2015,
author="Masci, Paolo
and Ruk"\v"s"""\."e""nas, Rimvydas
and Oladimeji, Patrick
and Cauchi, Abigail
and Gimblett, Andy
and Li, Yunqiu
and Curzon, Paul
and Thimbleby, Harold",
title="The benefits of formalising design guidelines: a case study on the predictability of drug infusion pumps",
journal="Innovations in Systems and Software Engineering",
year="2015",
volume="11",
number="2",
pages="73--93",
abstract="A demonstration is presented of how automated reasoning tools can be used to check the predictability of a user interface. Predictability concerns the ability of a user to determine the outcomes of their actions reliably. It is especially important in situations such as a hospital ward where medical devices are assumed to be reliable devices by their expert users (clinicians) who are frequently interrupted and need to quickly and accurately continue a task. There are several forms of predictability. A definition is considered where information is only inferred from the current perceptible output of the system. In this definition, the user is not required to remember the history of actions that led to the current state. Higher-order logic is used to specify predictability, and the Symbolic Analysis Laboratory is used to automatically verify predictability on real interactive number entry systems of two commercial drug infusion pumps---devices used in the healthcare domain to deliver fluids (e.g., medications, nutrients) into a patient's body in controlled amounts. Areas of unpredictability are precisely identified with the analysis. Verified solutions that make an unpredictable system predictable are presented through design modifications and verified user strategies that mitigate against the identified issues.",
issn="1614-5054",
doi="10.1007/s11334-013-0200-4",
url="http://dx.doi.org/10.1007/s11334-013-0200-4"
"


@Article"Marrone2014,
author="Marrone, Stefano
and Flammini, Francesco
and Mazzocca, Nicola
and Nardone, Roberto
and Vittorini, Valeria",
title="Towards Model-Driven V"\&"V assessment of railway control systems",
journal="International Journal on Software Tools for Technology Transfer",
year="2014",
volume="16",
number="6",
pages="669--683",
abstract="Verification and Validation (V"\&"V) activities aiming at certifying railway controllers are among the most critical and time-consuming in system development life cycle. As such, they would greatly benefit from novel approaches enabling both automation and traceability for assessment purposes. While several formal and Model-Based approaches have been proposed in the scientific literature, some of which are successfully employed in industrial settings, we are still far from an integrated and unified methodology which allows guiding design choices, minimizing the chances of failures/non-compliances, and considerably reducing the overall assessment effort. To address these issues, this paper describes a Model-Driven Engineering approach which is very promising to tackle the aforementioned challenges. In fact, the usage of appropriate Unified Modeling Language profiles featuring system analysis and test case specification capabilities, together with tool chains for model transformations and analysis, seems a viable way to allow end-users to concentrate on high-level holistic models and specification of non-functional requirements (i.e., dependability) and support the automation of the V"\&"V process. We show, through a case study belonging to the railway signalling domain, how the approach is effective in supporting activities like system testing and availability evaluation.",
issn="1433-2787",
doi="10.1007/s10009-014-0320-7",
url="http://dx.doi.org/10.1007/s10009-014-0320-7"
"


@Article"Qadeer2012,
author="Qadeer, Shaz
and Tasiran, Serdar",
title="Runtime verification of concurrency-specific correctness criteria",
journal="International Journal on Software Tools for Technology Transfer",
year="2012",
volume="14",
number="3",
pages="291--305",
abstract="We give an overview of correctness criteria specific to concurrent shared-memory programs and runtime verification techniques for verifying these criteria. We cover a spectrum of criteria, from ones focusing on low-level thread interference such as races to higher-level ones such as linearizability. We contrast these criteria in the context of runtime verification. We present the key ideas underlying the runtime verification techniques for these criteria and summarize the state of the art. Finally, we discuss the issue of coverage for runtime verification for concurrency and present techniques that improve the set of covered thread interleavings.",
issn="1433-2787",
doi="10.1007/s10009-011-0210-1",
url="http://dx.doi.org/10.1007/s10009-011-0210-1"
"


@Article"Garc�a-Barriocanal2012,
author="Garc"\'i"a-Barriocanal, Elena
and Sicilia, Miguel-Angel
and S"\'a"nchez-Alonso, Salvador",
title="Social Network-Aware Interfaces as Facilitators of Innovation",
journal="Journal of Computer Science and Technology",
year="2012",
volume="27",
number="6",
pages="1211--1221",
abstract="Achieving continuous innovation in organizations requires a balance between exploiting yet acquired knowledge and exploring new knowledge. In addition to having the adequate resources, change and innovation capabilities require specific management support and organizational structures. Recent research has pointed out the importance of social network structure and of the activity of agents that work across domains or disciplines in the innovation-oriented behaviour of organizations. As a consequence, information systems should ideally be able to support the analysis, development and management of such social structure for the benefit of organizational objectives. Current social network interfaces provide an established mental model to workers that can be hypothesized to be adequate for supporting activities that foster innovative behaviour. That behaviour is facilitated through exposing the activities of other workers across organizational structures. This paper reports on the design of a user interface specifically targeted to manage the social aspects of innovation based on some aspects of Hargadon's model of innovation and knowledge brokering. The emergent nature of interactions in social network sites is used as the metaphor to foster situated cognition. The interface design assessment is described and some metrics for innovative behaviour that could be derived for such an interface are sketched.",
issn="1860-4749",
doi="10.1007/s11390-012-1297-x",
url="http://dx.doi.org/10.1007/s11390-012-1297-x"
"


@Article"Sun2013,
author="Sun, Guo-Dao
and Wu, Ying-Cai
and Liang, Rong-Hua
and Liu, Shi-Xia",
title="A Survey of Visual Analytics Techniques and Applications: State-of-the-Art Research and Future Challenges",
journal="Journal of Computer Science and Technology",
year="2013",
volume="28",
number="5",
pages="852--867",
abstract="Visual analytics employs interactive visualizations to integrate users' knowledge and inference capability into numerical/algorithmic data analysis processes. It is an active research field that has applications in many sectors, such as security, finance, and business. The growing popularity of visual analytics in recent years creates the need for a broad survey that reviews and assesses the recent developments in the field. This report reviews and classifies recent work into a set of application categories including space and time, multivariate, text, graph and network, and other applications. More importantly, this report presents analytics space, inspired by design space, which relates each application category to the key steps in visual analytics, including visual mapping, model-based analysis, and user interactions. We explore and discuss the analytics space to add the current understanding and better understand research trends in the field.",
issn="1860-4749",
doi="10.1007/s11390-013-1383-8",
url="http://dx.doi.org/10.1007/s11390-013-1383-8"
"


@Article"Wang2008,
author="Wang, Miao-Miao
and Cao, Jian-Nong
and Li, Jing
and Dasi, Sajal K.",
title="Middleware for Wireless Sensor Networks: A Survey",
journal="Journal of Computer Science and Technology",
year="2008",
volume="23",
number="3",
pages="305--326",
abstract="Wireless Sensor Networks (WSNs) have found more and more applications in a variety of pervasive computing environments. However, how to support the development, maintenance, deployment and execution of applications over WSNs remains to be a nontrivial and challenging task, mainly because of the gap between the high level requirements from pervasive computing applications and the underlying operation of WSNs. Middleware for WSN can help bridge the gap and remove impediments. In recent years, research has been carried out on WSN middleware from different aspects and for different purposes. In this paper, we provide a comprehensive review of the existing work on WSN middleware, seeking for a better understanding of the current issues and future directions in this field. We propose a reference framework to analyze the functionalities of WSN middleware in terms of the system abstractions and the services provided. We review the approaches and techniques for implementing the services. On the basis of the analysis and by using a feature tree, we provide taxonomy of the features of WSN middleware and their relationships, and use the taxonomy to classify and evaluate existing work. We also discuss open problems in this important area of research.",
issn="1860-4749",
doi="10.1007/s11390-008-9135-x",
url="http://dx.doi.org/10.1007/s11390-008-9135-x"
"


@Article"Lima2015,
author="Lima, Lucas
and Miyazawa, Alvaro
and Cavalcanti, Ana
and Corn"\'e"lio, M"\'a"rcio
and Iyoda, Juliano
and Sampaio, Augusto
and Hains, Ralph
and Larkham, Adrian
and Lewis, Vaughan",
title="An integrated semantics for reasoning about SysML design models using refinement",
journal="Software "\&" Systems Modeling",
year="2015",
pages="1--28",
abstract="SysML is a variant of UML for systems design. Several formalisations of SysML�(and UML) are available. Our work is distinctive in two ways:�a semantics for refinement and for a representative collection of elements from the UML4SysML profile�(blocks, state machines, activities, and interactions) used in combination. We provide a means to analyse and refine design models specified using SysML. This facilitates the discovery of problems earlier in the system development lifecycle, reducing time, and costs of production. Here, we describe our semantics, which is defined using a state-rich process algebra and implemented in a tool for automatic generation of formal models. We also show how the semantics can be used for refinement-based analysis and development. Our case study is a leadership-election protocol, a critical component of an industrial application. Our major contribution is a framework for reasoning using refinement about systems specified by collections of SysML diagrams.",
issn="1619-1374",
doi="10.1007/s10270-015-0492-y",
url="http://dx.doi.org/10.1007/s10270-015-0492-y"
"


@Article"Breu2007,
author="Breu, Ruth
and Popp, Gerhard
and Alam, Muhammad",
title="Model based development of access policies",
journal="International Journal on Software Tools for Technology Transfer",
year="2007",
volume="9",
number="5",
pages="457--470",
abstract="In this paper we present a novel approach for the specification of user rights in the context of an object oriented use case driven development process. Basically, we extend the specification of methods by a permission section describing the right of some actor to call the method of an object. Our approach is both role based and context based while allowing for permissions to be specified at a fine-grained data-dependent level. We use first-order logic with a built-in notion of objects and classes (provided with an algebraic semantics) as our syntactic and semantic framework. In the second part of the paper, we demonstrate the application of this approach in a model-based context to generate permissions in distributed peer-to-peer networks.",
issn="1433-2787",
doi="10.1007/s10009-007-0045-y",
url="http://dx.doi.org/10.1007/s10009-007-0045-y"
"


@Article"Mahr2009,
author="Mahr, Bernd",
title="Information science and the logic of models",
journal="Software "\&" Systems Modeling",
year="2009",
volume="8",
number="3",
pages="365--383",
issn="1619-1374",
doi="10.1007/s10270-009-0119-2",
url="http://dx.doi.org/10.1007/s10270-009-0119-2"
"


@Article"Rago2013,
author="Rago, Alejandro
and Marcos, Claudia
and Diaz-Pace, J. Andr"\'e"s",
title="Uncovering quality-attribute concerns in use case specifications via early aspect mining",
journal="Requirements Engineering",
year="2013",
volume="18",
number="1",
pages="67--84",
abstract="Quality-attribute requirements describe constraints on the development and behavior of a software system, and their satisfaction is key for the success of a software project. Detecting and analyzing quality attributes in early development stages provides insights for system design, reduces risks, and ultimately improves the developers' understanding of the system. A common problem, however, is that quality-attribute information tends to be understated in requirements specifications and scattered across several documents. Thus, making the quality attributes first-class citizens becomes usually a time-consuming task for analysts. Recent developments have made it possible to mine concerns semi-automatically from textual documents. Leveraging on these ideas, we present a semi-automated approach to identify latent quality attributes that works in two stages. First, a mining tool extracts early aspects from use cases, and then these aspects are processed to derive candidate quality attributes. This derivation is based on an ontology of quality-attribute scenarios. We have built a prototype tool called QAMiner to implement our approach. The evaluation of this tool in two case studies from the literature has shown interesting results. As main contribution, we argue that our approach can help analysts to skim requirements documents and quickly produce a list of potential quality attributes for the system.",
issn="1432-010X",
doi="10.1007/s00766-011-0142-z",
url="http://dx.doi.org/10.1007/s00766-011-0142-z"
"


@Article"Mili2006,
author="Mili, Ali
and Sheldon, Frederick
and Jilani, Lamia Labed
and Vinokurov, Alex
and Thomasian, Alex
and Ayed, Rahma Ben",
title="Modeling security as a dependability attribute: a refinement-based approach",
journal="Innovations in Systems and Software Engineering",
year="2006",
volume="2",
number="1",
pages="39--48",
abstract="As distributed, networked computing systems become the dominant computing platform in a growing range of applications, they increase opportunities for security violations by opening hitherto unknown vulnerabilities. Also, as systems take on more critical functions, they increase the stakes of security by acting as custodians of assets that have great economic or social value. Finally, as perpetrators grow increasingly sophisticated, they increase the threats on system security. Combined, these premises place system security at the forefront of engineering concerns. In this paper, we introduce and discuss a refinement-based model for one dimension of system security, namely survivability.",
issn="1614-5054",
doi="10.1007/s11334-006-0023-7",
url="http://dx.doi.org/10.1007/s11334-006-0023-7"
"


@Article"Yang2015,
author="Yang, Lili
and Prasanna, Raj
and King, Malcolm",
title="GDIA: Eliciting information requirements in emergency first response",
journal="Requirements Engineering",
year="2015",
volume="20",
number="4",
pages="345--362",
abstract="Good information is vital for first responders in an emergency. However, although information systems can provide vast amounts of data, the information requirements of emergency first responders in complex, dynamic, ad hoc, and stressful environments cannot be systematically captured by existing requirement engineering approaches. This paper outlines the unique features of emergency response operations first and then drives an emergency-response-specific method to suit those features. The method is named as goal-directed information analysis (GDIA), which is based on but easier to use than an approach, goal-directed task analysis. We argue that goals are implicit and thus difficult to be captured from first responders because of the features of the emergency operations. GDIA starts from scenarios and has seven clearly defined and repeatable steps, including task analysis, which then leads to a simpler and more accurate analysis of the goal structure, before the rest of the hierarchy, including decisions and information requirements, is completed. A case study is presented using this GDIA approach to retrieve information requirements for four types of key fire fighters responding to a fire in a high-risk building environment. This work led to several real applications in the emergency service area, indicating the success of the approach.",
issn="1432-010X",
doi="10.1007/s00766-014-0202-2",
url="http://dx.doi.org/10.1007/s00766-014-0202-2"
"


@Article"Frehse2008,
author="Frehse, Goran",
title="PHAVer: algorithmic verification of hybrid systems past HyTech",
journal="International Journal on Software Tools for Technology Transfer",
year="2008",
volume="10",
number="3",
pages="263--279",
abstract="In 1995, HyTech broke new ground as a potentially powerful tool for verifying hybrid systems. But due to practical and systematic limitations it is only applicable to relatively simple systems. We address the main problems of HyTech with PHAVer, a new tool for the exact verification of safety properties of hybrid systems with piecewise constant bounds on the derivatives, so-called linear hybrid automata. Affine dynamics are handled by on-the-fly overapproximation and partitioning of the state space based on user-provided constraints and the dynamics of the system. PHAVer features exact arithmetic in a robust implementation that, based on the Parma Polyhedra Library, supports arbitrarily large numbers. To force termination and manage the complexity of the polyhedral computations, we propose methods to conservatively limit the number of bits and constraints of polyhedra. Experimental results for a navigation benchmark and a tunnel diode circuit demonstrate the effectiveness of the approach.",
issn="1433-2787",
doi="10.1007/s10009-007-0062-x",
url="http://dx.doi.org/10.1007/s10009-007-0062-x"
"


@Article"Iacob2014,
author="Iacob, M. E.
and Meertens, L. O.
and Jonkers, H.
and Quartel, D. A. C.
and Nieuwenhuis, L. J. M.
and van Sinderen, M. J.",
title="From enterprise architecture to business models and back",
journal="Software "\&" Systems Modeling",
year="2014",
volume="13",
number="3",
pages="1059--1083",
abstract="In this study, we argue that important IT change processes affecting an organization's enterprise architecture are also mirrored by a change in the organization's business model. An analysis of the business model may establish whether the architecture change has value for the business. Therefore, in order to facilitate such analyses, we propose an approach to relate enterprise models specified in ArchiMate to business models, modeled using Osterwalder's Business Model Canvas. Our approach is accompanied by a method that supports business model-driven migration from a baseline architecture to a target architecture and is demonstrated by means of a case study.",
issn="1619-1374",
doi="10.1007/s10270-012-0304-6",
url="http://dx.doi.org/10.1007/s10270-012-0304-6"
"


@Article"Guo2014,
author="Guo, Hai-Feng
and Subramaniam, Mahadevan",
title="Model-based test generation using extended symbolic grammars",
journal="International Journal on Software Tools for Technology Transfer",
year="2014",
volume="16",
number="4",
pages="437--455",
abstract="A novel, model-based test case generation approach for validating reactive systems, especially those supporting richly structured data inputs and/or interactions, is presented. Given an executable system model and an extended symbolic grammar specifying plausible system inputs, the approach performs a model-based simulation to (i) ensure the consistency of the model with respect to the specified inputs, and (ii) generate corresponding test cases for validating the system. The model-based simulation produces a state transition diagram (STD) automatically justifying the model runtime behaviors within the test case coverage. The STD can further be transformed to produce an evolved symbolic grammar, which can then be used to incrementally generate a refined set of test cases. As a case study, we present a live sequence chart (LSC) model-based test generator, named LCT in short, for LSC simulation and consistency testing. The evolved symbolic grammar produced by the simulator can either be used to generate practical test cases for software testing, or be further refined by applying our model-based test generation approach again with additional test coverage criteria. We further show that LSCs can also be used to specify and test certain temporal system properties during the model simulation. Their satisfaction, reflected in the STD, can either be served as a directive for selective test generation, or a basis for further temporal property model checking.",
issn="1433-2787",
doi="10.1007/s10009-014-0316-3",
url="http://dx.doi.org/10.1007/s10009-014-0316-3"
"


@Article"Hall�2015,
author="Hall"\'e", Sylvain
and Vallet, Jason
and Tremblay-Lessard, Rapha"\"e"l",
title="On piggyback runtime monitoring of object-oriented programs",
journal="International Journal on Software Tools for Technology Transfer",
year="2015",
volume="17",
number="2",
pages="125--142",
abstract="A runtime monitor enforcing a constraint on sequences of method calls on an object involves the implementation of an independent piece of code called a monitor. This monitor intercepts relevant events in the program and must keep track of the state of the sequence by updating an appropriate state machine. The present paper stems from the observation that an object's member fields must already contain an encoding of that state machine and that a monitor essentially duplicates operations that the object performs internally. Rather than maintaining a state machine in parallel, the paper puts forward the concept of ``piggyback'' runtime monitoring, where the monitor relies as much as possible on the object's own state variables to perform its task. Experiments on real-world benchmarks show that this approach greatly simplifies the monitoring process and drastically reduces the incurred runtime overhead compared to classical solutions.",
issn="1433-2787",
doi="10.1007/s10009-014-0326-1",
url="http://dx.doi.org/10.1007/s10009-014-0326-1"
"


@Article"Lim2011,
author="Lim, Junghee
and Lal, Akash
and Reps, Thomas",
title="Symbolic analysis via semantic reinterpretation",
journal="International Journal on Software Tools for Technology Transfer",
year="2011",
volume="13",
number="1",
pages="61--87",
abstract="The paper presents a novel technique to create implementations of the basic primitives used in symbolic program analysis: forward symbolic evaluation, weakest liberal precondition, and symbolic composition. We used the technique to create a system in which, for the cost of writing just one specification---an interpreter for the programming language of interest---one obtains automatically generated, mutually-consistent implementations of all three symbolic-analysis primitives. This can be carried out even for languages with pointers and address arithmetic. Our implementation has been used to generate symbolic-analysis primitives for the x86 and PowerPC instruction sets.",
issn="1433-2787",
doi="10.1007/s10009-010-0158-6",
url="http://dx.doi.org/10.1007/s10009-010-0158-6"
"


@Article"Houben2013,
author="Houben, Fred
and Igna, Georgeta
and Vaandrager, Frits",
title="Modeling task systems using parameterized partial orders",
journal="International Journal on Software Tools for Technology Transfer",
year="2013",
volume="15",
number="3",
pages="269--286",
abstract="Inspired by work on model-based design of printers, the notion of a parametrized partial order (PPO) has recently been introduced. PPOs are a simple extension of partial orders, expressive enough to compactly represent large task graphs with finite repetitive behavior. We present a translation of a subclass of PPOs to timed automata and prove that the transition system induced by the Uppaal models is isomorphic to the configuration structure of the original PPO. Moreover, we introduce real-time task systems (RTTSs), a general model for real-time embedded systems that we have used to describe the data paths of realistic printer designs. In an RTTS, tasks are represented as PPOs and the pace of a task instance may vary, depending on the resources that are allocated to it. We describe a translation of a subclass of RTTSs to Uppaal, and establish, for an even smaller subclass, bisimulation equivalence between the timed configuration semantics of an RTTS and the transition system induced by the corresponding Uppaal translation. Lastly, we report on a series of experiments which demonstrates that the resulting Uppaal models are more tractable than handcrafted models of the same systems used in earlier case studies.",
issn="1433-2787",
doi="10.1007/s10009-012-0264-8",
url="http://dx.doi.org/10.1007/s10009-012-0264-8"
"


@Article"Maiden2005,
author="Maiden, N. A. M.
and Manning, S.
and Jones, S.
and Greenwood, J.",
title="Generating requirements from systems models using patterns: a case study",
journal="Requirements Engineering",
year="2005",
volume="10",
number="4",
pages="276--288",
abstract="Academic research has produced many model-based specification and analysis techniques, however, most organisations continue to document requirements as textual statements. To help bridge this gap between academic research and requirements practice, this paper reports an extension to the RESCUE process in which patterns for generating requirements statements from i* system models were manually applied to i* models developed for a complex air traffic control system. The paper reports the results of this application and describes them with examples, the benefits of the approach to the project, and ongoing research to implement these patterns in the REDEPEND modelling tool to make requirements engineers more productive. We review similar work on requirements modelling and expression, and compare our work to it to demonstrate the proposed advance in the state of the art. Finally the paper discusses future uses of requirements generation from model patterns in RESCUE.",
issn="1432-010X",
doi="10.1007/s00766-005-0010-9",
url="http://dx.doi.org/10.1007/s00766-005-0010-9"
"


@Article"Bouajjani2012,
author="Bouajjani, Ahmed
and Habermehl, Peter
and Rogalewicz, Adam
and Vojnar, Tom"\'a""\v"s""",
title="Abstract regular (tree) model checking",
journal="International Journal on Software Tools for Technology Transfer",
year="2012",
volume="14",
number="2",
pages="167--191",
abstract="Regular model checking is a generic technique for verification of infinite-state and/or parametrised systems which uses finite word automata or finite tree automata to finitely represent potentially infinite sets of reachable configurations of the systems being verified. The problems addressed by regular model checking are typically undecidable. In order to facilitate termination in as many cases as possible, acceleration is needed in the incremental computation of the set of reachable configurations in regular model checking. In this work, we describe how various incrementally refinable abstractions on finite (word and tree) automata can be used for this purpose. Moreover, the use of abstraction does not only increase chances of the technique to terminate, but it also significantly reduces the problem of an explosion in the number of states of the automata that are generated by regular model checking. We illustrate the efficiency of abstract regular (tree) model checking in verification of simple systems with various sources of infinity such as unbounded counters, queues, stacks, and parameters. We then show how abstract regular tree model checking can be used for verification of programs manipulating tree-like dynamic data structures. Even more complex data structures can be handled using a suitable tree-like encoding.",
issn="1433-2787",
doi="10.1007/s10009-011-0205-y",
url="http://dx.doi.org/10.1007/s10009-011-0205-y"
"


@Article"Batra2007,
author="Batra, Dinesh",
title="Cognitive complexity in data modeling: causes and recommendations",
journal="Requirements Engineering",
year="2007",
volume="12",
number="4",
pages="231--244",
abstract="Data modeling is a complex task for novice designers. This paper conducts a systematic study of cognitive complexity to reveal important factors pertaining to data modeling. Four major sources of complexity principles are identified: problem solving principles, design principles, information overload, and systems theory. The factors that lead to complexity are listed in each category. Each factor is then applied to the context of data modeling to evaluate if it affects data modeling complexity. Redundant factors from different sources are ignored, and closely linked factors are merged. The factors are then integrated to come up with a comprehensive list of factors. The factors that cannot largely be controlled are dropped from further analysis. The remaining factors are employed to develop a semantic differential scale for assessing cognitive complexity. The paper concludes with implications and recommendations on how to address cognitive complexity caused by data modeling.",
issn="1432-010X",
doi="10.1007/s00766-006-0040-y",
url="http://dx.doi.org/10.1007/s00766-006-0040-y"
"


@Article"M�ry2015,
author="M"\'e"ry, Dominique
and Poppleton, Michael",
title="Towards an integrated formal method for verification of liveness properties in distributed systems: with application to population protocols",
journal="Software "\&" Systems Modeling",
year="2015",
pages="1--33",
abstract="State-based formal methods [e.g. Event-B/RODIN (Abrial in Modeling in Event-B---system and software engineering. Cambridge University Press, Cambridge, 2010; Abrial et al. in Int J Softw Tools Technol Transf (STTT) 12(6):447--466, 2010)] for critical system development and verification are now well established, with track records including tool support and industrial applications. The focus of proof-based verification, in particular, is on safety properties. Liveness properties, which guarantee eventual, or converging computations of some requirements, are less well dealt with. Inductive reasoning about liveness is not explicitly supported. Liveness proofs are often complex and expensive, requiring high-skill levels on the part of the verification engineer. Fairness-based temporal logic approaches have been proposed to address this, e.g. TLA Lamport (ACM Trans Program Lang Syst 16(3):872--923, 1994) and that of Manna and Pnueli (Temporal verification of reactive systems---safety. Springer, New York, 1995). We contribute to this technology need by proposing a fairness-based method integrating temporal and first-order logic, proof and tools for modelling and verification of safety and liveness properties. The method is based on an integration of Event-B and TLA. Building on our previous work (M"\'e"ry and Poppleton in Integrated formal methods, 10th international conference, IFM 2013, Turku, Finland, pp 208--222, 2013. doi:                10.1007/978-3-642-38613-8"\_"15                              ), we present the method via three example population protocols Angluin et al. (Distrib Comput 18(4):235--253, 2006). These were proposed as a theoretical framework for computability reasoning about Wireless Sensor Network and Mobile Ad-Hoc Network algorithms. Our examples present typical liveness and convergence requirements. We prove convergence results for the examples by integrated modelling and proof with Event-B/RODIN and TLA. We exploit existing proof rules, define and apply three new proof rules; soundness proofs are also provided. During the process we observe certain repeating patterns in the proofs. These are easily identified and reused because of the explicit nature of the reasoning.",
issn="1619-1374",
doi="10.1007/s10270-015-0504-y",
url="http://dx.doi.org/10.1007/s10270-015-0504-y"
"


@Article"Milhau2011,
author="Milhau, J.
and Idani, A.
and Laleau, R.
and Labiadh, M. A.
and Ledru, Y.
and Frappier, M.",
title="Combining UML, ASTD and B for the formal specification of an access control filter",
journal="Innovations in Systems and Software Engineering",
year="2011",
volume="7",
number="4",
pages="303--313",
abstract="Combination of formal and semi-formal methods is more and more required to produce specifications that can be, on the one hand, understood and thus validated by both designers and users and, on the other hand, precise enough to be verified by formal methods. This motivates our aim to use these complementary paradigms in order to deal with security aspects of information systems. This paper presents a methodology to specify access control policies starting with a set of graphical diagrams: UML for the functional model, SecureUML for static access control and ASTD for dynamic access control. These diagrams are then translated into a set of B machines. Finally, we present the formal specification of an access control filter that coordinates the different kinds of access control rules and the specification of functional operations. The goal of such B specifications is to rigorously check the access control policy of an information system taking advantage of tools from the B method.",
issn="1614-5054",
doi="10.1007/s11334-011-0166-z",
url="http://dx.doi.org/10.1007/s11334-011-0166-z"
"


@Article"Brucker2005,
author="Brucker, Achim D.
and Wolff, Burkhart",
title="A verification approach to applied system security",
journal="International Journal on Software Tools for Technology Transfer",
year="2005",
volume="7",
number="3",
pages="233--247",
abstract="We present a method for the security analysis of realistic models over off-the-shelf systems and their configuration by formal, machine-checked proofs. The presentation follows a large case study based on a formal security analysis of a CVS-Server architecture.",
issn="1433-2787",
doi="10.1007/s10009-004-0176-3",
url="http://dx.doi.org/10.1007/s10009-004-0176-3"
"


@Article"Andrikopoulos2013,
author="Andrikopoulos, Vasilios
and Binz, Tobias
and Leymann, Frank
and Strauch, Steve",
title="How to adapt applications for the Cloud environment",
journal="Computing",
year="2013",
volume="95",
number="6",
pages="493--535",
abstract="The migration of existing applications to the Cloud requires adapting them to a new computing paradigm. Existing works have focused on migrating the whole application stack by means of virtualization and deployment on the Cloud, delegating the required adaptation effort to the level of resource management. With the proliferation of Cloud services allowing for more flexibility and better control over the application migration, the migration of individual application layers, or even individual architectural components to the Cloud, becomes possible. Towards this goal, in this work we focus on the challenges and solutions for each layer when migrating different parts of the application to the Cloud. We categorize different migration types and identify the potential impact and adaptation needs for each of these types on the application layers based on an exhaustive survey of the State of the Art. We also investigate various cross-cutting concerns that need to be considered for the migration of the application, and position them with respect to the identified migration types. Finally, we present some of the open research issues in the field and position our future work targeting these research questions.",
issn="1436-5057",
doi="10.1007/s00607-012-0248-2",
url="http://dx.doi.org/10.1007/s00607-012-0248-2"
"


@Article"Atkinson2008,
author="Atkinson, Colin
and K"\"u"hne, Thomas",
title="Reducing accidental complexity in domain models",
journal="Software "\&" Systems Modeling",
year="2008",
volume="7",
number="3",
pages="345--359",
abstract="A fundamental principle in engineering, including software engineering, is to minimize the amount of accidental complexity which is introduced into engineering solutions due to mismatches between a problem and the technology used to represent the problem. As model-driven development moves to the center stage of software engineering, it is particularly important that this principle be applied to the technologies used to create and manipulate models, especially models that are intended to be free of solution decisions. At present, however, there is a significant mismatch between the ``two level'' modeling paradigm used to construct mainstream domain models and the conceptual information such models are required to represent---a mismatch that makes such models more complex than they need be. In this paper, we identify the precise nature of the mismatch, discuss a number of more or less satisfactory workarounds, and show how it can be avoided.",
issn="1619-1374",
doi="10.1007/s10270-007-0061-0",
url="http://dx.doi.org/10.1007/s10270-007-0061-0"
"


@Article"ElKateb2015,
author="El Kateb, Donia
and Zannone, Nicola
and Moawad, Assaad
and Caire, Patrice
and Nain, Gr"\'e"gory
and Mouelhi, Tejeddine
and Le Traon, Yves",
title="Conviviality-driven access control policy",
journal="Requirements Engineering",
year="2015",
volume="20",
number="4",
pages="363--382",
abstract="Nowadays many organizations experience security incidents due to unauthorized access to information. To reduce the risk of such incidents, security policies are often employed to regulate access to information. Such policies, however, are often too restrictive, and users do not have the rights necessary to perform assigned duties. As a consequence, access control mechanisms are perceived by users as a barrier and thus bypassed, making the system insecure. In this paper, we draw a bridge between the social concept of conviviality and access control. Conviviality has been introduced as a social science concept for ambient intelligence and multi-agent systems to highlight soft qualitative requirements like user-friendliness of systems. To bridge the gap between conviviality and security, we propose a methodological framework for updating and adapting access control policies based on conviviality recommendations. Our methodology integrates and extends existing techniques to assist system designers in the derivation of access control policies from socio-technical requirements of the system, while taking into account the conviviality of the system. We illustrate our framework using the Ambient Assisted Living use case from the HotCity of Luxembourg.",
issn="1432-010X",
doi="10.1007/s00766-014-0204-0",
url="http://dx.doi.org/10.1007/s00766-014-0204-0"
"


@Article"Bodik2013,
author="Bodik, Rastislav
and Jobstmann, Barbara",
title="Algorithmic program synthesis: introduction",
journal="International Journal on Software Tools for Technology Transfer",
year="2013",
volume="15",
number="5",
pages="397--411",
abstract="Program synthesis is a process of producing an executable program from a specification. Algorithmic synthesis produces the program automatically, without an intervention from an expert. While classical compilation falls under the definition of algorithmic program synthesis, with the source program being the specification, the synthesis literature is typically concerned with producing programs that cannot be (easily) obtained with the deterministic transformations of a compiler. To this end, synthesis algorithms often perform a search, either in a space of candidate programs or in a space of transformations that might be composed to transform the specification into a desired program. In this introduction to the special journal issue, we survey the history of algorithmic program synthesis and introduce the contributed articles. We divide the field into reactive synthesis, which is concerned with automata-theoretic techniques for controllers that handle an infinite stream of requests, and functional synthesis, which produces programs consuming finite input. Contributed articles are divided analogously. We also provide pointers to synthesis work outside these categories and list many applications of synthesis.",
issn="1433-2787",
doi="10.1007/s10009-013-0287-9",
url="http://dx.doi.org/10.1007/s10009-013-0287-9"
"


@Article"Abdelzad2015,
author="Abdelzad, Vahdat
and Lethbridge, Timothy C.",
title="Promoting traits into model-driven development",
journal="Software "\&" Systems Modeling",
year="2015",
pages="1--21",
abstract="Traits, as sets of behaviors, can provide a good mechanism for reusability. However, they are limited in important ways and are not present in widely used programming and modeling languages and hence are not readily available for use by mainstream developers. In this paper, we add UML associations and other modeling concepts to traits and apply them to Java and C++ through model-driven development. We also extend traits with required interfaces so dependencies at the semantics level become part of their usage, rather than simple syntactic capture. All this is accomplished in Umple, a textual modeling language based upon UML that allows adding programming constructs to the model. We applied the work to two case studies. The results show that we can promote traits to the modeling level along with the improvement in flexibility and reusability.",
issn="1619-1374",
doi="10.1007/s10270-015-0505-x",
url="http://dx.doi.org/10.1007/s10270-015-0505-x"
"


@Article"Pesic2007,
author="Pesic, Maja
and van der Aalst, Wil M. P.",
title="Modelling work distribution mechanisms using Colored Petri Nets",
journal="International Journal on Software Tools for Technology Transfer",
year="2007",
volume="9",
number="3",
pages="327--352",
abstract="Workflow management systems support business processes and are driven by their models. These models cover different perspectives including the control-flow, resource, and data perspectives. This paper focuses on the resource perspective, i.e., the way the system distributes work based on the structure of the organization and capabilities/qualifications of people. Contemporary workflow management systems offer a wide variety of mechanisms to support the resource perspective. Because the resource perspective is essential for the applicability of such systems, it is important to better understand the mechanisms and their interactions. Our goal is not to evaluate and compare what different systems do, but to understand how they do it. We use Colored Petri Nets (CPNs) to model work distribution mechanisms. First, we provide a basic model that can be seen as a reference model of existing workflow management systems. This model is then extended for three specific systems (Staffware, FileNet, and FLOWer). Moreover, we show how more advanced work distribution mechanisms, referred to as resource patterns, can be modelled and analyzed.",
issn="1433-2787",
doi="10.1007/s10009-007-0036-z",
url="http://dx.doi.org/10.1007/s10009-007-0036-z"
"


@Article"Rose2015,
author="Rose, Jeremy",
title="Improving software management: the industry model, the knowledge model, the network model",
journal="Innovations in Systems and Software Engineering",
year="2015",
volume="11",
number="1",
pages="9--23",
abstract="Thinking about improving the management of software development in software firms has been dominated by one approach: the capability maturity model (CMM) devised and administered at Carnegie Mellon University. Although widely known and used, there are a number of well-understood difficulties and limitations with this approach. This article examines, through the lens of modern management theory, the governing assumptions about management and organizational improvement behind the CMM approach. It characterizes this assumption set as the industry model of software management improvement. We take a dialectic approach to propose antithetical assumptions and a configuration approach to weave these different assumptions into alternative assumption platforms: the knowledge model and the network model. These two models, we suggest, might be better foundations for some types of software managements in a world responding to globalization and rapid technology change. If these assumption platforms were used to underpin improvements in software management, we ask, what kinds of approaches would they lead to?",
issn="1614-5054",
doi="10.1007/s11334-014-0240-4",
url="http://dx.doi.org/10.1007/s11334-014-0240-4"
"


@Article"Bera2014,
author="Bera, Palash
and Evermann, Joerg",
title="Guidelines for using UML association classes and their effect on domain understanding in requirements engineering",
journal="Requirements Engineering",
year="2014",
volume="19",
number="1",
pages="63--80",
abstract="The analysis and description of the application domain are important parts of the requirements engineering process. Domain descriptions are frequently represented as models in the de-facto standard unified modeling language (UML). Recent research has specified the semantics of various UML language elements for domain modeling, based on ontological considerations. In this paper, we empirically examine ontological modeling guidelines for the UML association construct, which plays a central role in UML class diagrams. Using an experimental study, we find that some, but not all, of the proposed guidelines lead to better application domain models. We use a process-tracing study to investigate in more detail the effects of ontological guidelines. The combined results indicate that ontological guidelines can improve the usefulness of UML class diagrams for describing the application domain, and thus have the potential to improve downstream system development activities and ultimately affect the successful information systems implementation.",
issn="1432-010X",
doi="10.1007/s00766-012-0159-y",
url="http://dx.doi.org/10.1007/s00766-012-0159-y"
"


@Article"SantosFran�a2015,
author="Santos Fran"\c"c""a, Juliana Baptista dos
and Netto, Joanne Manh"\~a"es
and do E.�S.�Carvalho, Juliana
and Santoro, Fl"\'a"via Maria
and Bai"\~a"o, Fernanda Araujo
and Pimentel, Mariano",
title="KIPO: the knowledge-intensive process ontology",
journal="Software "\&" Systems Modeling",
year="2015",
volume="14",
number="3",
pages="1127--1157",
abstract="A business process is a sequence of activities that aims at creating products or services, granting value to the customer, and is generally represented by a business process model. Business process models play an important role in bridging the gap between the business domain and the information technology, increasing the weight of business modeling as first step of software development. However, the traditional way of representing a process is not suitable for the so-called Knowledge-Intensive Processes (KIP). This type of process comprises sequences of activities based on intensive acquisition, sharing, storage and (re)use of knowledge, so that the amount of value added to the organization depends on the actor knowledge. Current research in the literature points to the lack of approaches to make this kind of process explicit and strategies for handling information that is necessary for their understanding and support. The goal of this paper is to present KIPO---a knowledge-intensive process ontology, which encompasses a clear and semantically rich definition of KIPs, and to discuss the results of a case study to evaluate KIPO with regard to its applicability and capability of making all relevant knowledge embedded in a KIP explicit.",
issn="1619-1374",
doi="10.1007/s10270-014-0397-1",
url="http://dx.doi.org/10.1007/s10270-014-0397-1"
"


@Article"Kassab2015,
author="Kassab, Mohamad
and Kilicay-Ergin, Nil",
title="Applying analytical hierarchy process to system quality requirements prioritization",
journal="Innovations in Systems and Software Engineering",
year="2015",
volume="11",
number="4",
pages="303--312",
abstract="The order in which design decisions or tactics are incorporated within a system architecture has a significant impact on how well quality requirements are addressed in the architecture solution. Quality attributes are most often correlated; attempts to achieve one quality attribute can help or hinder the achievement of another quality relevant for the system. Thus, prioritization of quality requirements and design tactics to address these quality requirements is a useful guide for system architects. Conventional techniques of quality attribute prioritization are qualitative in nature and trade-off among design tactics are not addressed during prioritization. In this paper, analytic hierarchy process (AHP) technique is proposed to quantitatively rank design decisions and tactics while at the same time taking into consideration the interrelationships between system quality requirements and design tactics and principles. The approach is demonstrated on remote monitoring system for medical patients. The approach facilitates an objective ranking of tactics and design principles and eliminates inconsistencies between business and technical stakeholder valuation.",
issn="1614-5054",
doi="10.1007/s11334-015-0260-8",
url="http://dx.doi.org/10.1007/s11334-015-0260-8"
"


@Article"Furfaro2009,
author="Furfaro, Angelo
and Nigro, Libero",
title="A development methodology for embedded systems based on RT-DEVS",
journal="Innovations in Systems and Software Engineering",
year="2009",
volume="5",
number="2",
pages="117--127",
abstract="This work is concerned with modelling, analysis and implementation of embedded control systems using RT-DEVS, i.e. a specialization of classic discrete event system specification (DEVS) for real-time. RT-DEVS favours model continuity, i.e. the possibility of using the same model for property analysis (by simulation or model checking) and for real time execution. Special case tools are reported in the literature for RT-DEVS model analysis and design. In this work, temporal analysis of a model exploits a translation in Uppaal timed automata for exhaustive verification. For large models a simulator was realized in Java which directly stems from RT-DEVS operational semantics. The same concerns are at the basis of a real-time executive. The paper describes the proposed RT-DEVS development methodology and clarifies its implementation status. The approach is demonstrated by applying it to an embedded system example which is analyzed through model checking and implemented in Java. Finally, research directions which deserve further work are indicated.",
issn="1614-5054",
doi="10.1007/s11334-009-0085-4",
url="http://dx.doi.org/10.1007/s11334-009-0085-4"
"


@Article"Fidge2006,
author="Fidge, C. J.",
title="Formal change impact analyses for emulated control software",
journal="International Journal on Software Tools for Technology Transfer",
year="2006",
volume="8",
number="4",
pages="321--335",
abstract="Processor emulators are a software tool for allowing legacy computer programs to be executed on a modern processor. In the past emulators have been used in trivial applications such as maintenance of video games. Now, however, processor emulation is being applied to safety-critical control systems, including military avionics. These applications demand utmost guarantees of correctness, but no verification techniques exist for proving that an emulated system preserves the original system's functional and timing properties. Here we show how this can be done by combining concepts previously used for reasoning about real-time program compilation, coupled with an understanding of the new and old software architectures. In particular, we show how both the old and new systems can be given a common semantics, thus allowing their behaviours to be compared directly.",
issn="1433-2787",
doi="10.1007/s10009-004-0174-5",
url="http://dx.doi.org/10.1007/s10009-004-0174-5"
"


@Article"Jalali2015,
author="Jalali, Amin
and Ouyang, Chun
and Wohed, Petia
and Johannesson, Paul",
title="Supporting aspect orientation in business process management",
journal="Software "\&" Systems Modeling",
year="2015",
pages="1--23",
abstract="Coping with complexity is an important issue in both research and industry. One strategy to deal with complexity is separation of concerns, which can be addressed using aspect-oriented paradigm. Despite being well researched in programming, this paradigm is still in a preliminary stage in the area of business process management (BPM). While some efforts have been made to introduce aspect orientation in business process modelling, there is no holistic approach with a formal underlying foundation to support aspect-oriented business process design and enactment, and this gap restricts aspect-oriented paradigm from being practically deployed in the area of BPM. Therefore, this paper proposes a sound systematic approach which builds on a formal syntax for modelling aspect-oriented business processes and a Petri Net-based operational semantics for enacting these processes. The approach enables the implementation of software system artefacts as a proof of concept to support design and enactment of aspect-oriented business processes in practice. The approach is demonstrated using a banking case study, where processes are modelled using a concrete notation that conforms to the proposed formal syntax and then executed in a state-of-the-art BPM system where the implemented artefacts are deployed.",
issn="1619-1374",
doi="10.1007/s10270-015-0496-7",
url="http://dx.doi.org/10.1007/s10270-015-0496-7"
"


@Article"Kolp2006,
author="Kolp, Manuel
and Giorgini, Paolo
and Mylopoulos, John",
title="Multi-Agent Architectures as Organizational Structures",
journal="Autonomous Agents and Multi-Agent Systems",
year="2006",
volume="13",
number="1",
pages="3--25",
abstract="A Multi-Agent System (hereafter MAS) is an organization of coordinated autonomous agents that interact in order to achieve common goals. Considering real world organizations as an metaphor, this paper proposes architectural styles for MAS which adopt concepts from organizational theories. The styles are modeled in i*/Tropos, using the notions of actor, goal and actor dependency and are intended to capture needs/wants, delegations and obligations. The proposed architectural styles are evaluated with respect to a set of software quality attributes, such as predictability and adaptability. In addition, we report on a comparative study of organizational and conventional software architectures using a mobile robot control example from the Software Engineering literature. The research reported here was conducted within the scope of the Tropos project, whose objective is to develop a comprehensive agent-oriented software development methodology.",
issn="1573-7454",
doi="10.1007/s10458-006-5717-6",
url="http://dx.doi.org/10.1007/s10458-006-5717-6"
"


@Article"Razavi2014,
author="Razavi, Niloofar
and Farzan, Azadeh
and McIlraith, Sheila A.",
title="Generating effective tests for concurrent programs via AI automated planning techniques",
journal="International Journal on Software Tools for Technology Transfer",
year="2014",
volume="16",
number="1",
pages="49--65",
abstract="Testing concurrent programs is a challenging problem due to interleaving explosion: even for a fixed set of inputs, there is a huge number of concurrent runs that need to be tested to account for scheduler behavior. Testing all possible schedules is not practical. Consequently, most effective testing algorithms only test a select subset of runs. For example, limiting testing to runs that contain data races or atomicity violations has been shown to capture a large proportion of concurrency bugs. In this paper we present a general approach to concurrent program testing that is based on techniques from artificial intelligence (AI) automated planning. We propose a framework for predicting concurrent program runs that violate a collection of generic correctness specifications for concurrent programs, namely runs that contain data races, atomicity violations, or null-pointer dereferences. Our prediction is based on observing an arbitrary run of the program, and using information collected from this run to model the behavior of the program, and to predict new runs that contain bugs with one of the above noted violation patterns. We characterize the problem of predicting such new runs as an AI sequential planning problem with the temporally extended goal of achieving a particular violation pattern. In contrast to many state-of-the-art approaches, in our approach feasibility of the predicted runs is guaranteed and, therefore, all generated runs are fully usable for testing. Moreover, our planning-based approach has the merit that it can easily accommodate a variety of violation patterns which serve as the selection criteria for guiding search in the state space of concurrent runs. This is achieved by simply modifying the planning goal. We have implemented our approach using state-of-the-art AI planning techniques and tested it within the Penelope concurrent program testing framework [35]. Nevertheless, the approach is general and is amenable to a variety of program testing frameworks. Our experiments with a benchmark suite showed that our approach is very fast and highly effective, finding all known bugs.",
issn="1433-2787",
doi="10.1007/s10009-013-0277-y",
url="http://dx.doi.org/10.1007/s10009-013-0277-y"
"


@Article"Moyano2013,
author="Moyano, Francisco
and Fernandez-Gago, Carmen
and Lopez, Javier",
title="A framework for enabling trust requirements in social cloud applications",
journal="Requirements Engineering",
year="2013",
volume="18",
number="4",
pages="321--341",
abstract="Cloud applications entail the provision of a huge amount of heterogeneous, geographically distributed resources managed and shared by many different stakeholders who often do not know each other beforehand. This raises numerous security concerns that, if not addressed carefully, might hinder the adoption of this promising computational model. Appropriately dealing with these threats gains special relevance in the social cloud context, where computational resources are provided by the users themselves. We argue that taking trust and reputation requirements into account can leverage security in these scenarios by incorporating the notions of trust relationships and reputation into them. For this reason, we propose a development framework onto which developers can implement trust-aware social cloud applications. Developers can also adapt the framework in order to accommodate their application-specific needs.",
issn="1432-010X",
doi="10.1007/s00766-013-0171-x",
url="http://dx.doi.org/10.1007/s00766-013-0171-x"
"


@Article"Gupta2013,
author="Gupta, Ashutosh Kumar
and Majumdar, Rupak
and Rybalchenko, Andrey",
title="From tests to proofs",
journal="International Journal on Software Tools for Technology Transfer",
year="2013",
volume="15",
number="4",
pages="291--303",
abstract="We describe the design and implementation of an automatic invariant generator for imperative programs. While automatic invariant generation through constraint solving has been extensively studied from a theoretical viewpoint as a classical means of program verification, in practice existing tools do not scale even to moderately sized programs. This is because the constraints that need to be solved even for small programs are already too difficult for the underlying (non-linear) constraint solving engines. To overcome this obstacle, we propose to strengthen static constraint generation with information obtained from static abstract interpretation and dynamic execution of the program. The strengthening comes in the form of additional linear constraints that trigger a series of simplifications in the solver, and make solving more scalable. We demonstrate the practical applicability of the approach by an experimental evaluation on a collection of challenging benchmark programs and comparisons with related tools based on abstract interpretation and software model checking.",
issn="1433-2787",
doi="10.1007/s10009-012-0267-5",
url="http://dx.doi.org/10.1007/s10009-012-0267-5"
"


@Article"SilvaOuriques2015,
author="Silva Ouriques, Jo"\~a"o Felipe
and Cartaxo, Emanuela Gadelha
and Lima Machado, Patr"\'i"cia Duarte",
title="Revealing influence of model structure and test case profile on the prioritization of test cases in the context of model-based testing",
journal="Journal of Software Engineering Research and Development",
year="2015",
volume="3",
number="1",
pages="1",
abstract="Test case prioritization techniques aim at defining an order of test cases that favor the achievement of a goal during test execution, such as revealing failures as earlier as possible. A number of techniques have already been proposed and investigated in the literature and experimental results have discussed whether a technique is more successful than others. However, in the context of model-based testing, only a few attempts have been made towards either proposing or experimenting test case prioritization techniques. Moreover, a number of factors that may influence on the results obtained still need to be investigated before more general conclusions can be reached.",
issn="2195-1721",
doi="10.1186/s40411-014-0015-5",
url="http://dx.doi.org/10.1186/s40411-014-0015-5"
"


@Article"King2014,
author="King, Alistair
and Huffaker, Bradley
and Dainotti, Alberto
and Claffy, k. c.",
title="A coordinated view of the temporal evolution of large-scale Internet events",
journal="Computing",
year="2014",
volume="96",
number="1",
pages="53--65",
abstract="We present a method to visualize large-scale Internet events, such as a large region losing connectivity, or a stealth probe of the entire IPv4 address space. We apply a well-known technique in information visualization---multiple coordinated views---to Internet-specific data. We animate these coordinated views to study the temporal evolution of an event along different dimensions, including geographic spread, topological (address space) coverage, and traffic impact. We explain the techniques we used to create the visualization, and using two recent case studies we describe how this capability to simultaneously view multiple dimensions of events enabled greater insight into their properties.",
issn="1436-5057",
doi="10.1007/s00607-013-0288-2",
url="http://dx.doi.org/10.1007/s00607-013-0288-2"
"


@Article"AlDallal2012,
author="Al Dallal, Jehad
and Saleh, Kassem A.",
title="Synthesizing Distributed Protocol Specifications from a UML State Machine Modeled Service Specification",
journal="Journal of Computer Science and Technology",
year="2012",
volume="27",
number="6",
pages="1150--1168",
abstract="The object-oriented paradigm is widely applied in designing and implementing communication systems. Unified Modeling Language (UML) is a standard language used to model the design of object-oriented systems. A protocol state machine is a UML adopted diagram that is widely used in designing communication protocols. It has two key attractive advantages over traditional finite state machines: modeling concurrency and modeling nested hierarchical states. In a distributed communication system, each entity of the system has its own protocol that defines when and how the entity exchanges messages with other communicating entities in the system. The order of the exchanged messages must conform to the overall service specifications of the system. In object-oriented systems, both the service and the protocol specifications are modeled in UML protocol state machines. Protocol specification synthesis methods have to be applied to automatically derive the protocol specification from the service specification. Otherwise, a time-consuming process of design, analysis, and error detection and correction has to be applied iteratively until the design of the protocol becomes error-free and consistent with the service specification. Several synthesis methods are proposed in the literature for models other than UML protocol state machines, and therefore, because of the unique features of the protocol state machines, these methods are inapplicable to services modeled in UML protocol state machines. In this paper, we propose a synthesis method that automatically synthesizes the protocol specification of distributed protocol entities from the service specification, given that both types of specifications are modeled in UML protocol state machines. Our method is based on the latest UML version (UML2.3), and it is proven to synthesize protocol specifications that are syntactically and semantically correct. As an example application, the synthesis method is used to derive the protocol specification of the H.323 standard used in Internet calls.",
issn="1860-4749",
doi="10.1007/s11390-012-1293-1",
url="http://dx.doi.org/10.1007/s11390-012-1293-1"
"


@Article"David2012,
author="David, Alexandre
and Larsen, Kim. G.
and Legay, Axel
and M"\o"ller, Mikael H.
and Nyman, Ulrik
and Ravn, Anders P.
and Skou, Arne
and W"\k"a""sowski, Andrzej",
title="Compositional verification of real-time systems using Ecdar                  ",
journal="International Journal on Software Tools for Technology Transfer",
year="2012",
volume="14",
number="6",
pages="703--720",
abstract="We present a specification theory for timed systems implemented in the Ecdar tool. We illustrate the operations of the specification theory on a running example, showing the models and verification checks. To demonstrate the power of the compositional verification, we perform an in depth case study of a leader election protocol; Modeling it in Ecdar as Timed input/output automata Specifications and performing both monolithic and compositional verification of two interesting properties on it. We compare the execution time of the compositional to the classical verification showing a huge difference in favor of compositional verification.",
issn="1433-2787",
doi="10.1007/s10009-012-0237-y",
url="http://dx.doi.org/10.1007/s10009-012-0237-y"
"


@Article"Laviron2011,
author="Laviron, Vincent
and Logozzo, Francesco",
title="SubPolyhedra: a family of numerical abstract domains for the (more) scalable inference of linear inequalities",
journal="International Journal on Software Tools for Technology Transfer",
year="2011",
volume="13",
number="6",
pages="585--601",
abstract="We introduce SubPolyhedra (SubPoly), a new family of numerical abstract domains to infer and propagate linear inequalities. The key insight is that the reduced product of linear equalities and intervals produces powerful yet scalable analyses. Abstract domains in SubPoly are as expressive as Polyhedra, but they drop some of the deductive power to achieve scalability. The cost/precision ratio of abstract domains in the SubPoly family can be fine-tuned according to the precision one wants to retain at join points, and the algorithm used to infer the tighter bounds on intervals. We implemented SubPoly on the top of                                                                           "\$""\$""\"""\"""\backslash"tt Clousot"\"""\"""\$""\$"                , a generic abstract interpreter for                                                                           "\$""\$""\"""\"""\backslash"tt .Net."\backslash",Clousot"\"""\"""\$""\$"                 with SubPoly analyzes very large and complex code bases in few minutes. SubPoly can efficiently capture linear inequalities among hundreds of variables, a result well beyond the state-of-the-art implementations of Polyhedra.",
issn="1433-2787",
doi="10.1007/s10009-011-0199-5",
url="http://dx.doi.org/10.1007/s10009-011-0199-5"
"


@Article"Welch2008,
author="Welch, James
and Faitelson, David
and Davies, Jim",
title="Automatic maintenance of association invariants",
journal="Software "\&" Systems Modeling",
year="2008",
volume="7",
number="3",
pages="287--301",
abstract="Many approaches to software specification and design make use of invariants: constraints whose truth is preserved under operations on a system or component. Object modelling involves the definition of association invariants: constraints upon the sets of links corresponding to particular associations, most often concerning type, multiplicity, or symmetry. This paper shows how the definitions of operations may be extended to take account of association invariants, so that they may be properly considered when the operations are implemented. It introduces a formal, object-based modelling notation in which the process of extension and implementation, and thus the maintenance of association invariants, can be automated, making it easier to produce correct implementations of an object-oriented design.",
issn="1619-1374",
doi="10.1007/s10270-008-0085-0",
url="http://dx.doi.org/10.1007/s10270-008-0085-0"
"


@Article"Pike2013,
author="Pike, Lee
and Wegmann, Nis
and Niller, Sebastian
and Goodloe, Alwyn",
title="Copilot: monitoring embedded systems",
journal="Innovations in Systems and Software Engineering",
year="2013",
volume="9",
number="4",
pages="235--255",
abstract="Runtime verification (RV) is a natural fit for ultra-critical systems that require correct software behavior. Due to the low reliability of commodity hardware and the adversity of operational environments, it is common in ultra-critical systems to replicate processing units (and their hosted software) and incorporate fault-tolerant algorithms to compare the outputs, even if the software is considered to be fault-free. In this paper, we investigate the use of software monitoring in distributed fault-tolerant systems and the implementation of fault-tolerance mechanisms using RV techniques. We describe the Copilot language and compiler that generates monitors for distributed real-time systems, and we discuss two case-studies in which Copilot-generated monitors were used to detect onboard software and hardware faults and monitor air-ground data link messaging protocols.",
issn="1614-5054",
doi="10.1007/s11334-013-0223-x",
url="http://dx.doi.org/10.1007/s11334-013-0223-x"
"


@Article"Oliveira2005,
author="Oliveira, Marcel
and Cavalcanti, Ana
and Woodcock, Jim",
title="Formal development of industrial-scale systems in Circus",
journal="Innovations in Systems and Software Engineering",
year="2005",
volume="1",
number="2",
pages="125--146",
abstract="Circus is a new notation that may be used to specify both data and behavioural aspects of a system, and has an associated refinement calculus. In this work, we present rules to translate Circus programs to Java programs that use JCSP, a library that implements Communicating Sequential Processes constructs. These rules can be used as a complement to the Circus algebraic refinement technique, or as a guideline for implementation. They are a link between the results on refinement in the context of Circus and a practical programming language in current use. The rules can also be used as the basis for a tool that mechanises the translation. Although a few case studies are already available in the literature, the industrial fire control system, whose refinement and implementation is discussed in this paper, is, as far as we know, the largest case study on the Circus refinement strategy.",
issn="1614-5054",
doi="10.1007/s11334-005-0014-0",
url="http://dx.doi.org/10.1007/s11334-005-0014-0"
"


@Article"Zhou2014,
author="Zhou, Yu
and Ma, Xiaoxing
and Gall, Harald",
title="A middleware platform for the dynamic evolution of distributed component-based systems",
journal="Computing",
year="2014",
volume="96",
number="8",
pages="725--747",
abstract="In this paper, we present a middleware platform that supports the dynamic evolution of distributed component-based systems. It leverages the concept of ontologies to model the context of a system and an intrinsic mechanism is integrated to causally connect the dynamic architecture specification to the running system implementation. The ontological modeling covers both the environmental and the architectural knowledge using semantic data modeling. The intrinsic mechanism can automatically derive a run-time polymorphic architecture object to coordinate the involved components. The ontology based contextual representation and the polymorphic architecture-driven dynamic evolution are the two underpinnings of the platform. A scenario application---including the two primitive evolution actions---with the performance analysis is discussed to illustrate the feasibility.",
issn="1436-5057",
doi="10.1007/s00607-014-0396-7",
url="http://dx.doi.org/10.1007/s00607-014-0396-7"
"


@Article"Tuyls2006,
author="Tuyls, Karl
and Hoen, Pieter Jan 'T
and Vanschoenwinkel, Bram",
title="An Evolutionary Dynamical Analysis of Multi-Agent Learning in Iterated Games",
journal="Autonomous Agents and Multi-Agent Systems",
year="2006",
volume="12",
number="1",
pages="115--153",
abstract="In this paper, we investigate Reinforcement learning (RL) in multi-agent systems (MAS) from an evolutionary dynamical perspective. Typical for a MAS is that the environment is not stationary and the Markov property is not valid. This requires agents to be adaptive. RL is a natural approach to model the learning of individual agents. These Learning algorithms are however known to be sensitive to the correct choice of parameter settings for single agent systems. This issue is more prevalent in the MAS case due to the changing interactions amongst the agents. It is largely an open question for a developer of MAS of how to design the individual agents such that, through learning, the agents as a collective arrive at good solutions. We will show that modeling RL in MAS, by taking an evolutionary game theoretic point of view, is a new and potentially successful way to guide learning agents to the most suitable solution for their task at hand. We show how evolutionary dynamics (ED) from Evolutionary Game Theory can help the developer of a MAS in good choices of parameter settings of the used RL algorithms. The ED essentially predict the equilibriums outcomes of the MAS where the agents use individual RL algorithms. More specifically, we show how the ED predict the learning trajectories of Q-Learners for iterated games. Moreover, we apply our results to (an extension of) the COllective INtelligence framework (COIN). COIN is a proved engineering approach for learning of cooperative tasks in MASs. The utilities of the agents are re-engineered to contribute to the global utility. We show how the improved results for MAS RL in COIN, and a developed extension, are predicted by the ED.",
issn="1573-7454",
doi="10.1007/s10458-005-3783-9",
url="http://dx.doi.org/10.1007/s10458-005-3783-9"
"


@Article"Siau2009,
author="Siau, Keng
and Tian, Yuhong",
title="A semiotic analysis of unified modeling language graphical notations",
journal="Requirements Engineering",
year="2009",
volume="14",
number="1",
pages="15--26",
abstract="Unified modeling language (UML) is the standard modeling language for object-oriented system development. Despite its status as a standard, UML has a fuzzy formal specification and a weak theoretical foundation. Semiotics, the study of signs, provides a good theoretical foundation for UML research because graphical notations (or visual signs) of UML are subjected to the principles of signs. In our research, we use semiotics to study the effectiveness of graphical notations in UML. We hypothesized that the use of iconic signs as UML graphical notations leads to representation that is more accurately interpreted and that arouses fewer connotations than the use of symbolic signs. An open-ended survey was used to test these hypotheses. The results support our propositions that iconic UML graphical notations are more accurately interpreted by subjects and that the number of connotations is lower for iconic UML graphical notations than for symbolic UML graphical notations. The results have both theoretical and practical significance. This study illustrates the usefulness of using semiotics as a theoretical underpinning in analyzing, evaluating, and comparing graphical notations for modeling constructs. The results of this research also suggest ways and means of enhancing the graphical notations of UML modeling constructs.",
issn="1432-010X",
doi="10.1007/s00766-008-0071-7",
url="http://dx.doi.org/10.1007/s00766-008-0071-7"
"


@Article"Hakula2013,
author="Hakula, Harri
and Tuominen, Tomi",
title="Mathematica implementation of the high order finite element method applied to eigenproblems",
journal="Computing",
year="2013",
volume="95",
number="1",
pages="277--301",
abstract="In this paper an                                                                           "\$""\$"hp"\$""\$"                -FEM implementation on Mathematica is discussed. FEM-implementations on higher-level programming platforms are useful for prototyping new algorithms and ideas, but also serve as testing ground for interesting programming techniques. Here, an                                                                           "\$""\$"hp"\$""\$"                -adaptive algorithm for eigenproblems, and the use of precomputed data and generation of highly graded                                                                           "\$""\$"hp"\$""\$"                -meshes, are examples of the former and latter, respectively. The performance of the code is evaluated in relation to a suite of benchmark problems for the Laplacian and thin solids in elasticity.",
issn="1436-5057",
doi="10.1007/s00607-012-0262-4",
url="http://dx.doi.org/10.1007/s00607-012-0262-4"
"


@Article"Rox2013,
author="Rox, Jonas
and Ernst, Rolf",
title="Compositional performance analysis with improved analysis techniques for obtaining viable end-to-end latencies in distributed embedded systems",
journal="International Journal on Software Tools for Technology Transfer",
year="2013",
volume="15",
number="3",
pages="171--187",
abstract="To give worst case guarantees for the timing behavior of complex distributed embedded real-time systems, e.g. end-to-end latencies, different compositional approaches for system-level performance analysis have been developed which exhibit great flexibility and scalability. While these approaches are in theory able to handle arbitrary complex systems, the system-level results can easily become very pessimistic with increasing numbers of components. In this article, the basic principles of compositional system-level analysis are explained and its inherent strengths and weaknesses are elaborated. Furthermore, we present improved analysis techniques from existing research which can greatly reduce the pessimism of the system-level analysis results. Two techniques will be discussed in detail: the exploitation of a system's communication infrastructure by usage of composition and decomposition operators and the exploitation of information w.r.t. the correlation of event processing. These techniques help to make system-level analysis not only applicable, but also a highly useful technique in the integration phase of embedded system design.",
issn="1433-2787",
doi="10.1007/s10009-012-0260-z",
url="http://dx.doi.org/10.1007/s10009-012-0260-z"
"


@Article"Mich2005,
author="Mich, Luisa
and Anesi, Cinzia
and Berry, Daniel M.",
title="Applying a pragmatics-based creativity-fostering technique to requirements elicitation",
journal="Requirements Engineering",
year="2005",
volume="10",
number="4",
pages="262--275",
abstract="This paper proposes the application to requirements elicitation of an innovative creativity fostering technique based on a model of the pragmatics of communication, the Elementary Pragmatic Model (EPM). The EPM has been used to define a creative process, called EPMcreate (EPM Creative Requirements Engineering TEchnique) that consists of sixteen steps. In each step, the problem is analyzed according to one elementary behavior identified by the EPM. Each behavior suggests that the analyst look at the problem from a different combination of users' viewpoints. The feasibility and effectiveness of the technique in requirements elicitation was demonstrated by experiments on two projects with very different characteristics. Each experiment compared the performances of two analysis teams, one of which used EPMcreate and the other of which used brainstorming. The results of both experiments highlights the higher effectiveness of EPMcreate. Additional data from the experiments are examined for other insights into how and why EPMcreate is effective.",
issn="1432-010X",
doi="10.1007/s00766-005-0008-3",
url="http://dx.doi.org/10.1007/s00766-005-0008-3"
"


@Article"Byna2009,
author="Byna, Surendra
and Chen, Yong
and Sun, Xian-He",
title="Taxonomy of Data Prefetching for Multicore Processors",
journal="Journal of Computer Science and Technology",
year="2009",
volume="24",
number="3",
pages="405--417",
abstract="Data prefetching is an effective data access latency hiding technique to mask the CPU stall caused by cache misses and to bridge the performance gap between processor and memory. With hardware and/or software support, data prefetching brings data closer to a processor before it is actually needed. Many prefetching techniques have been developed for single-core processors. Recent developments in processor technology have brought multicore processors into mainstream. While some of the single-core prefetching techniques are directly applicable to multicore processors, numerous novel strategies have been proposed in the past few years to take advantage of multiple cores. This paper aims to provide a comprehensive review of the state-of-the-art prefetching techniques, and proposes a taxonomy that classifies various design concerns in developing a prefetching strategy, especially for multicore processors. We compare various existing methods through analysis as well.",
issn="1860-4749",
doi="10.1007/s11390-009-9233-4",
url="http://dx.doi.org/10.1007/s11390-009-9233-4"
"


@Article"Dalpiaz2013,
author="Dalpiaz, Fabiano
and Giorgini, Paolo
and Mylopoulos, John",
title="Adaptive socio-technical systems: a requirements-based approach",
journal="Requirements Engineering",
year="2013",
volume="18",
number="1",
pages="1--24",
abstract="A socio-technical system (STS) consists of an interplay of humans, organizations, and technical systems. STSs are heterogeneous, dynamic, unpredictable, and weakly controllable. Their operational environment changes unexpectedly, actors join and leave the system at will, actors fail to meet their objectives and under-perform, and dependencies on other actors are violated. To deal with such situations, we propose an architecture for STSs that makes an STS self-reconfigurable, i.e., capable of switching autonomously from one configuration to a better one. Our architecture performs a Monitor-Diagnose-Reconcile-Compensate cycle: it monitors actor behaviors and context changes, diagnoses failures and under-performance by checking whether monitored behavior is compliant with actors goals, finds a possible way to address the problem, and enacts compensation actions to reconcile actual and desired behavior. Compensation actions take into account the autonomy of participants in an STS, which cannot be controlled. Our architecture is requirements driven: we use extended Tropos goal models to diagnose failures as well as to identify alternative strategies to meet requirements. After presenting our conceptual architecture and the algorithms, it is founded upon; we describe a prototype implementation applied to a case study concerning smart-homes. We also provide experimental results that suggest that our architecture scales well as the size of the STS grows.",
issn="1432-010X",
doi="10.1007/s00766-011-0132-1",
url="http://dx.doi.org/10.1007/s00766-011-0132-1"
"


@Article"Ma2013,
author="Ma, Yu-Xin
and Xu, Jia-Yi
and Peng, Di-Chao
and Zhang, Ting
and Jin, Cheng-Zhe
and Qu, Hua-Min
and Chen, Wei
and Peng, Qun-Sheng",
title="A Visual Analysis Approach for Community Detection of Multi-Context Mobile Social Networks",
journal="Journal of Computer Science and Technology",
year="2013",
volume="28",
number="5",
pages="797--809",
abstract="The problem of detecting community structures of a social network has been extensively studied over recent years, but most existing methods solely rely on the network structure and neglect the context information of the social relations. The main reason is that a context-rich network offers too much flexibility and complexity for automatic or manual modulation of the multifaceted context in the analysis process. We address the challenging problem of incorporating context information into the community analysis with a novel visual analysis mechanism. Our approach consists of two stages: interactive discovery of salient context, and iterative context-guided community detection. Central to the analysis process is a context relevance model (CRM) that visually characterizes the influence of a given set of contexts on the variation of the detected communities, and discloses the community structure in specific context configurations. The extracted relevance is used to drive an iterative visual reasoning process, in which the community structures are progressively discovered. We introduce a suite of visual representations to encode the community structures, the context as well as the CRM. In particular, we propose an enhanced parallel coordinates representation to depict the context and community structures, which allows for interactive data exploration and community investigation. Case studies on several datasets demonstrate the efficiency and accuracy of our approach.",
issn="1860-4749",
doi="10.1007/s11390-013-1378-5",
url="http://dx.doi.org/10.1007/s11390-013-1378-5"
"


@Article"Affeldt2013,
author="Affeldt, Reynald",
title="On construction of a library of formally verified low-level arithmetic functions",
journal="Innovations in Systems and Software Engineering",
year="2013",
volume="9",
number="2",
pages="59--77",
abstract="Arithmetic functions are used in many important computer programs such as computer algebra systems and cryptographic software. The latter are critical applications whose correct implementation deserves to be formally guaranteed. They are also computation-intensive applications, so that programmers often resort to low-level assembly code to implement arithmetic functions. We propose an approach for the construction of a library of formally verified low-level arithmetic functions. To build our library, we first introduce a formalization of data structures for signed multi-precision arithmetic in low-level programs. We use this formalization to verify the implementation of several primitive arithmetic functions using Separation logic, an extension of Hoare logic to deal with pointers. Since this direct style of formal verification leads to technically involved specifications, we also propose for larger functions to show a formal simulation relation between pseudo-code and assembly. This style of verification is illustrated with a concrete implementation of the binary extended gcd algorithm.",
issn="1614-5054",
doi="10.1007/s11334-013-0195-x",
url="http://dx.doi.org/10.1007/s11334-013-0195-x"
"


@Article"Kim2014,
author="Kim, Kibum
and Ren, Xiangshi",
title="Assisting Visually Impaired People to Acquire Targets on a Large Wall-Mounted Display",
journal="Journal of Computer Science and Technology",
year="2014",
volume="29",
number="5",
pages="825--836",
abstract="Large displays have become ubiquitous in our everyday lives, but these displays are designed for sighted people. This paper addresses the need for visually impaired people to access targets on large wall-mounted displays. We developed an assistive interface which exploits mid-air gesture input and haptic feedback, and examined its potential for pointing and steering tasks in human computer interaction (HCI). In two experiments, blind and blindfolded users performed target acquisition tasks using mid-air gestures and two different kinds of feedback (i.e., haptic feedback and audio feedback). Our results show that participants perform faster in Fitts' law pointing tasks using the haptic feedback interface rather than the audio feedback interface. Furthermore, a regression analysis between movement time (MT) and the index of difficulty (ID) demonstrates that the Fitts' law model and the steering law model are both effective for the evaluation of assistive interfaces for the blind. Our work and findings will serve as an initial step to assist visually impaired people to easily access required information on large public displays using haptic interfaces.",
issn="1860-4749",
doi="10.1007/s11390-014-1471-4",
url="http://dx.doi.org/10.1007/s11390-014-1471-4"
"


@Article"Tsay2013,
author="Tsay, Yih-Kuen
and Tsai, Ming-Hsien
and Chang, Jinn-Shu
and Chang, Yi-Wen
and Liu, Chi-Shiang",
title="B"\"u"chi Store: an open repository of                                                                   "\$""\$""\backslash"omega "\$""\$"              -automata",
journal="International Journal on Software Tools for Technology Transfer",
year="2013",
volume="15",
number="2",
pages="109--123",
abstract="We introduce B"\"u"chi Store, an open repository of B"\"u"chi automata and other types of                                                                           "\$""\$""\backslash"omega "\$""\$"                -automata for model-checking practice, research, and education. The repository contains B"\"u"chi automata and their complements for common specification patterns and numerous temporal formulae. These automata are made as small as possible by various construction techniques in view of the fact that smaller automata are easier to understand and often help in speeding up the model-checking process. The repository is open, allowing the user to add automata that define new languages or are smaller than existing equivalent ones. Such a collection of B"\"u"chi automata is also useful as a benchmark for evaluating translation or complementation algorithms and as examples for studying B"\"u"chi automata and temporal logic. These apply analogously for other types of                                                                           "\$""\$""\backslash"omega "\$""\$"                -automata, including deterministic B"\"u"chi and deterministic parity automata, which are also collected in the repository. In particular, the use of smaller deterministic parity automata as an intermediary helps reduce the complexity of automatic synthesis of reactive systems from temporal specifications. ",
issn="1433-2787",
doi="10.1007/s10009-012-0268-4",
url="http://dx.doi.org/10.1007/s10009-012-0268-4"
"


@Article"D�Argenio2015,
author="D'Argenio, Pedro
and Legay, Axel
and Sedwards, Sean
and Traonouez, Louis-Marie",
title="Smart sampling for lightweight verification of Markov decision processes",
journal="International Journal on Software Tools for Technology Transfer",
year="2015",
volume="17",
number="4",
pages="469--484",
abstract="Markov decision processes (MDP) are useful to model optimisation problems in concurrent systems. To verify MDPs with efficient Monte Carlo techniques requires that their nondeterminism be resolved by a scheduler. Recent work has introduced the elements of lightweight techniques to sample directly from scheduler space, but finding optimal schedulers by simple sampling may be inefficient. Here we describe ``smart'' sampling algorithms that can make substantial improvements in performance.",
issn="1433-2787",
doi="10.1007/s10009-015-0383-0",
url="http://dx.doi.org/10.1007/s10009-015-0383-0"
"


@Article"Mora2012,
author="Mora, Antonio M.
and Fern"\'a"ndez-Ares, Antonio
and Merelo, Juan J.
and Garc"\'i"a-S"\'a"nchez, Pablo
and Fernandes, Carlos M.",
title="Effect of Noisy Fitness in Real-Time Strategy Games Player Behaviour Optimisation Using Evolutionary Algorithms",
journal="Journal of Computer Science and Technology",
year="2012",
volume="27",
number="5",
pages="1007--1023",
abstract="This paper investigates the performance and the results of an evolutionary algorithm (EA) specifically designed for evolving the decision engine of a program (which, in this context, is called bot) that plays Planet Wars. This game, which was chosen for the Google Artificial Intelligence Challenge in 2010, requires the bot to deal with multiple target planets, while achieving a certain degree of adaptability in order to defeat different opponents in different scenarios. The decision engine of the bot is initially based on a set of rules that have been defined after an empirical study, and a genetic algorithm (GA) is used for tuning the set of constants, weights and probabilities that those rules include, and therefore, the general behaviour of the bot. Then, the bot is supplied with the evolved decision engine and the results obtained when competing with other bots (a bot offered by Google as a sparring partner, and a scripted bot with a pre-established behaviour) are thoroughly analysed. The evaluation of the candidate solutions is based on the result of non-deterministic battles (and environmental interactions) against other bots, whose outcome depends on random draws as well as on the opponents' actions. Therefore, the proposed GA is dealing with a noisy fitness function. After analysing the effects of the noisy fitness, we conclude that tackling randomness via repeated combats and reevaluations reduces this effect and makes the GA a highly valuable approach for solving this problem.",
issn="1860-4749",
doi="10.1007/s11390-012-1281-5",
url="http://dx.doi.org/10.1007/s11390-012-1281-5"
"


@Article"Wang2010,
author="Wang, Shou-Xin
and Zhang, Li
and Wang, Shuai
and Qiu, Xiang",
title="A Cloud-Based Trust Model for Evaluating Quality of Web Services",
journal="Journal of Computer Science and Technology",
year="2010",
volume="25",
number="6",
pages="1130--1142",
abstract="Because trust is regarded as an essential secured relationship within a distributed network environment, selecting services over the Internet from the viewpoint of trust has been a major trend. Current research about trust model and evaluation in the context of Web services does not rationally and accurately reflect some essential characteristics of trust such as subjective uncertainty and dynamism. In this paper, we analyze some important characteristics of trust, and some key factors that affect the trust relation in the Web service environment. Accordingly, we propose a trust model based on Cloud Model theory to describe the subjective uncertainty of trust factors. A time-related backward cloud generation algorithm is given to express the dynamism of trust. Furthermore, according to the trust model and algorithm, a formalized calculation approach is provided to evaluate the trust degree of services requestors in providers. Our experiment shows that the evaluation of trust degree can effectively support trust-decisions and provide a helpful exploitation for selecting services based on the viewpoint of trust.",
issn="1860-4749",
doi="10.1007/s11390-010-9394-1",
url="http://dx.doi.org/10.1007/s11390-010-9394-1"
"


@Article"Liu2010,
author="Liu, Yunhao
and Yang, Zheng
and Wang, Xiaoping
and Jian, Lirong",
title="Location, Localization, and Localizability",
journal="Journal of Computer Science and Technology",
year="2010",
volume="25",
number="2",
pages="274--297",
abstract="Location-aware technology spawns numerous unforeseen pervasive applications in a wide range of living, production, commence, and public services. This article provides an overview of the location, localization, and localizability issues of wireless ad-hoc and sensor networks. Making data geographically meaningful, location information is essential for many applications, and it deeply aids a number of network functions, such as network routing, topology control, coverage, boundary detection, clustering, etc. We investigate a large body of existing localization approaches with focuses on error control and network localizability, the two rising aspects that attract significant research interests in recent years. Error control aims to alleviate the negative impact of noisy ranging measurement and the error accumulation effect during cooperative localization process. Network localizability provides theoretical analysis on the performance of localization approaches, providing guidance on network configuration and adjustment. We emphasize the basic principles of localization to understand the state-of-the-art and to address directions of future research in the new and largely open areas of location-aware technologies.",
issn="1860-4749",
doi="10.1007/s11390-010-9324-2",
url="http://dx.doi.org/10.1007/s11390-010-9324-2"
"


@Article"Li2009,
author="Li, Hui-Qian
and Xia, Fen
and Zeng, Daniel
and Wang, Fei-Yue
and Mao, Wen-Ji",
title="Exploring Social Annotations with the Application to Web Page Recommendation",
journal="Journal of Computer Science and Technology",
year="2009",
volume="24",
number="6",
pages="1028",
abstract="Collaborative social annotation systems allow users to record and share their original keywords or tag attachments to Web resources such as Web pages, photos, or videos. These annotations are a method for organizing and labeling information. They have the potential to help users navigate the Web and locate the needed resources. However, since annotations are posted by users under no central control, there exist problems such as spam and synonymous annotations. To efficiently use annotation information to facilitate knowledge discovery from the Web, it is advantageous if we organize social annotations from semantic perspective and embed them into algorithms for knowledge discovery. This inspires the Web page recommendation with annotations, in which users and Web pages are clustered so that semantically similar items can be related. In this paper we propose four graphic models which cluster users, Web pages and annotations and recommend Web pages for given users by assigning items to the right cluster first. The algorithms are then compared to the classical collaborative filtering recommendation method on a real-world data set. Our result indicates that the graphic models provide better recommendation performance and are robust to fit for the real applications.",
issn="1860-4749",
doi="10.1007/s11390-009-9292-6",
url="http://dx.doi.org/10.1007/s11390-009-9292-6"
"


